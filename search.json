{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"RS Document","text":"<p>High-performance text document cleaning and splitting for RAG (Retrieval Augmented Generation) applications.</p>","path":["RS Document"],"tags":[]},{"location":"#overview","level":2,"title":"Overview","text":"<p>rs_document is a Rust-powered Python package that provides fast text processing operations for preparing documents for vector databases and embedding models. It reimplements common document processing functions from LangChain and Unstructured.io with significant performance improvements.</p> <p>Key Features:</p> <ul> <li>20-25x faster than pure Python implementations</li> <li>~23,000 documents/second processing speed</li> <li>Parallel batch processing</li> <li>Compatible with LangChain's Document model</li> <li>Simple, opinionated API</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#quick-start","level":2,"title":"Quick Start","text":"<p>Install from PyPI:</p> <pre><code>pip install rs-document\n</code></pre> <p>Basic usage:</p> <pre><code>from rs_document import Document, clean_and_split_docs\n\n# Create a document\ndoc = Document(\n    page_content=\"Your document text here...\",\n    metadata={\"source\": \"example.txt\"}\n)\n\n# Clean and split\ndoc.clean()\nchunks = doc.recursive_character_splitter(1000)\n\n# Or process many documents at once\ndocuments = [doc]  # Your list of documents\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre>","path":["RS Document"],"tags":[]},{"location":"#what-can-you-do","level":2,"title":"What Can You Do?","text":"","path":["RS Document"],"tags":[]},{"location":"#clean-documents","level":3,"title":"Clean Documents","text":"<p>Remove artifacts from PDFs, OCR, and web scraping:</p> <pre><code>doc = Document(\n    page_content=\"●  Text with bullets, æ ligatures, and  extra   spaces\",\n    metadata={}\n)\n\ndoc.clean()  # Runs all cleaners\n</code></pre> <p>Available cleaners:</p> <ul> <li>Remove non-ASCII characters</li> <li>Convert ligatures (æ → ae, œ → oe)</li> <li>Remove bullet symbols</li> <li>Normalize whitespace</li> <li>Group broken paragraphs</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#split-documents","level":3,"title":"Split Documents","text":"<p>Break large documents into chunks for embeddings:</p> <pre><code># Recursive splitting (respects paragraphs/sentences/words)\nchunks = doc.recursive_character_splitter(1000)\n\n# Simple character splitting\nchunks = doc.split_on_num_characters(500)\n</code></pre> <p>The recursive splitter:</p> <ul> <li>Tries to split on paragraph breaks first</li> <li>Falls back to sentences, then words, then characters</li> <li>Creates ~33% overlap between chunks for better context</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#batch-processing","level":3,"title":"Batch Processing","text":"<p>Process many documents efficiently with parallel processing:</p> <pre><code>from rs_document import clean_and_split_docs\n\n# Process 1000s of documents in seconds\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre>","path":["RS Document"],"tags":[]},{"location":"#performance","level":2,"title":"Performance","text":"<p>Benchmarks show consistent performance improvements:</p> Operation Documents Python Time rs_document Time Speedup Clean + Split 1,000 45s 2s 22.5x Clean + Split 10,000 7.5min 20s 22.5x Clean + Split 100,000 75min 3.3min 22.5x <p>Processing rate: ~23,000 documents/second on typical hardware.</p>","path":["RS Document"],"tags":[]},{"location":"#documentation-structure","level":2,"title":"Documentation Structure","text":"<p>Following the Diataxis framework:</p>","path":["RS Document"],"tags":[]},{"location":"#tutorial","level":3,"title":"Tutorial","text":"<p>Learning-oriented - Start here if you're new to rs_document. Walk through basic concepts and operations with hands-on examples.</p>","path":["RS Document"],"tags":[]},{"location":"#how-to-guides","level":3,"title":"How-To Guides","text":"<p>Task-oriented - Practical solutions for specific tasks like integrating with LangChain, batch processing, or handling edge cases.</p>","path":["RS Document"],"tags":[]},{"location":"#reference","level":3,"title":"Reference","text":"<p>Information-oriented - Complete API documentation for all classes, methods, and functions. Look up exact signatures and parameters.</p>","path":["RS Document"],"tags":[]},{"location":"#explanation","level":3,"title":"Explanation","text":"<p>Understanding-oriented - Learn about design decisions, performance characteristics, and how the recursive splitter algorithm works.</p>","path":["RS Document"],"tags":[]},{"location":"#use-cases","level":2,"title":"Use Cases","text":"<p>rs_document is designed for:</p> <ul> <li>RAG pipelines - Prepare documents for vector databases</li> <li>Document ingestion - Process large document collections efficiently</li> <li>Embedding preparation - Split documents for embedding models</li> <li>Text normalization - Clean messy text from various sources</li> </ul> <p>Works with:</p> <ul> <li>LangChain and LlamaIndex</li> <li>OpenAI, Cohere, and other embedding providers</li> <li>Pinecone, Weaviate, Qdrant, and other vector databases</li> <li>Any Python RAG framework</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#why-rust","level":2,"title":"Why Rust?","text":"<p>Text processing in Python is slow for large-scale operations. rs_document uses Rust for:</p> <ul> <li>Compiled native code performance</li> <li>Efficient string operations</li> <li>True parallelism (no GIL)</li> <li>Memory efficiency</li> </ul> <p>You get Rust's performance with Python's convenience - no Rust knowledge required.</p>","path":["RS Document"],"tags":[]},{"location":"#compatibility","level":2,"title":"Compatibility","text":"<ul> <li>Python: 3.10+</li> <li>Platforms: Linux, macOS, Windows (x86_64 and ARM)</li> <li>LangChain: Compatible with Document model (metadata must be strings)</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#project-status","level":2,"title":"Project Status","text":"<p>rs_document is production-ready and actively maintained. It's been tested with:</p> <ul> <li>102 test cases including property-based tests</li> <li>CI testing across Python versions</li> <li>Performance benchmarks to prevent regressions</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#contributing","level":2,"title":"Contributing","text":"<p>This project welcomes contributions! See the developer documentation in the <code>dev/</code> directory:</p> <ul> <li><code>dev/contributing.md</code> - Development workflow and testing</li> <li><code>dev/claude.md</code> - Project architecture and design</li> <li><code>dev/coverage.md</code> - Testing and coverage strategy</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#attribution-credits","level":2,"title":"Attribution &amp; Credits","text":"<p>This project builds upon and is inspired by the following open source projects:</p>","path":["RS Document"],"tags":[]},{"location":"#langchain","level":3,"title":"LangChain","text":"<ul> <li>Source: https://github.com/langchain-ai/langchain</li> <li>Author: LangChain AI</li> <li>License: MIT</li> <li>Usage: The Document class is designed to be compatible with LangChain's Document model. The recursive character splitter is based on LangChain's RecursiveCharacterTextSplitter algorithm, reimplemented in Rust for performance.</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#unstructuredio","level":3,"title":"Unstructured.io","text":"<ul> <li>Source: https://github.com/Unstructured-IO/unstructured</li> <li>Author: Unstructured Technologies, Inc.</li> <li>License: Apache 2.0</li> <li>Usage: The text cleaning functions are Rust reimplementations of Unstructured.io's post-processor cleaners, maintaining compatible behavior while providing significant performance improvements.</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#diataxis","level":3,"title":"Diataxis","text":"<ul> <li>Source: https://diataxis.fr</li> <li>Author: Daniele Procida</li> <li>License: Creative Commons</li> <li>Usage: Documentation structure follows the Diataxis framework for organizing technical documentation into tutorials, how-to guides, reference, and explanation sections.</li> </ul>","path":["RS Document"],"tags":[]},{"location":"#license","level":2,"title":"License","text":"<p>See LICENSE.md for details.</p>","path":["RS Document"],"tags":[]},{"location":"#links","level":2,"title":"Links","text":"<ul> <li>GitHub Repository</li> <li>PyPI Package</li> <li>Issue Tracker</li> </ul>","path":["RS Document"],"tags":[]},{"location":"explanation/","level":1,"title":"Understanding RS Document","text":"<p>This section explains the concepts, design decisions, and architecture behind rs_document. Whether you're evaluating the library for your project or seeking to understand how it works, these pages will help you grasp the \"why\" behind the implementation.</p>","path":["Explanation","Understanding RS Document"],"tags":[]},{"location":"explanation/#what-youll-learn","level":2,"title":"What You'll Learn","text":"","path":["Explanation","Understanding RS Document"],"tags":[]},{"location":"explanation/#core-concepts","level":3,"title":"Core Concepts","text":"<p>Why Rust? - Understand the performance problem that rs_document solves and why Rust was chosen as the solution. Learn about the bottlenecks in pure Python implementations and how compiled code with efficient string handling delivers 20-25x speedups.</p> <p>Design Philosophy - Explore the deliberate design choices that make rs_document simple yet powerful: opinionated defaults that work for 95% of use cases, string-only metadata for reliability, and the mutation vs immutability trade-offs.</p> <p>Recursive Splitting - Deep dive into how the recursive character splitter works, why it creates overlapping chunks, and what makes it effective for RAG applications. Includes concrete examples and algorithm walkthroughs.</p>","path":["Explanation","Understanding RS Document"],"tags":[]},{"location":"explanation/#practical-understanding","level":3,"title":"Practical Understanding","text":"<p>Text Cleaning - Learn why document cleaning matters for embeddings, what each cleaner does, and why they run in a specific order. Understand the artifacts from PDFs, OCR, and web scraping that hurt retrieval quality.</p> <p>Performance - Discover what makes rs_document fast: compiled code, efficient string handling, SIMD optimizations, and parallel processing. See benchmarks and understand when performance matters for your use case.</p> <p>Comparisons - Compare rs_document with LangChain and Unstructured.io to understand when to use each tool. Learn integration patterns for RAG pipelines, LangChain workflows, and standalone usage.</p>","path":["Explanation","Understanding RS Document"],"tags":[]},{"location":"explanation/#philosophy-in-brief","level":2,"title":"Philosophy in Brief","text":"<p>rs_document is designed around a few key principles:</p> <ol> <li>Performance First - Text processing shouldn't be your bottleneck</li> <li>Opinionated Defaults - Proven settings that work for most RAG applications</li> <li>Simple API - Fewer parameters means less cognitive load</li> <li>Do One Thing Well - Fast cleaning and splitting, not an all-in-one solution</li> </ol>","path":["Explanation","Understanding RS Document"],"tags":[]},{"location":"explanation/#who-should-read-this","level":2,"title":"Who Should Read This","text":"<ul> <li>Evaluators - Deciding if rs_document fits your RAG pipeline</li> <li>New Users - Understanding how to use the library effectively</li> <li>Contributors - Learning the design principles behind implementation decisions</li> <li>Curious Developers - Interested in performance optimization and Rust-Python integration</li> </ul>","path":["Explanation","Understanding RS Document"],"tags":[]},{"location":"explanation/#how-to-navigate","level":2,"title":"How to Navigate","text":"<p>Each page is self-contained and can be read independently. However, reading them in order will build your understanding progressively from problem to solution to practical application.</p> <p>Start with Why Rust? to understand the motivation, then explore the topics most relevant to your needs.</p>","path":["Explanation","Understanding RS Document"],"tags":[]},{"location":"explanation/#beyond-this-guide","level":2,"title":"Beyond This Guide","text":"<p>This explanation section focuses on concepts and understanding. For practical usage:</p> <ul> <li>See the Getting Started guide for installation and basic usage</li> <li>Check the API Reference for detailed method documentation</li> </ul> <p>The goal is to help you not just use rs_document, but understand why it works the way it does. This understanding will help you make better decisions about when and how to use it in your projects.</p>","path":["Explanation","Understanding RS Document"],"tags":[]},{"location":"explanation/comparisons/","level":1,"title":"Comparisons &amp; Integration","text":"<p>Understanding how rs_document compares to other tools and fits into your workflow helps you make informed decisions about when and how to use it.</p>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#vs-langchain-recursivecharactertextsplitter","level":2,"title":"vs LangChain RecursiveCharacterTextSplitter","text":"<p>LangChain is the most popular framework for building LLM applications and provides text splitting capabilities.</p>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#similarities","level":3,"title":"Similarities","text":"<p>Both implement recursive splitting:</p> <ul> <li>Same core algorithm concept</li> <li>Hierarchical separator approach</li> <li>Chunk size targeting</li> <li>Context preservation through overlap</li> </ul> <p>Both are production-ready:</p> <ul> <li>Well-tested implementations</li> <li>Active maintenance</li> <li>Good documentation</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#differences","level":3,"title":"Differences","text":"Feature LangChain rs_document Performance Baseline 20-25x faster Parallelism GIL-limited True parallel (8x on 8 cores) Chunk Overlap Configurable (any %) Fixed (~33%) Separators Configurable (any list) Fixed (<code>\\n\\n</code>, <code>\\n</code>, <code>`,</code>\"\"`) Splitting Strategies Multiple (recursive, character, token) Character only Token Counting Built-in support Not available Metadata Types Any Python object Strings only Callbacks Supported Not available Ecosystem Full LangChain integration Standalone","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#when-to-use-langchain","level":3,"title":"When to Use LangChain","text":"<p>Choose LangChain's text splitter when:</p> <ol> <li>Custom overlap needed: Your use case requires specific overlap percentages</li> </ol> <pre><code># LangChain allows this\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100  # 10% overlap\n)\n</code></pre> <ol> <li>Custom separators needed: Domain-specific split points</li> </ol> <pre><code># LangChain allows this\nsplitter = RecursiveCharacterTextSplitter(\n    separators=[\"---\", \"###\", \"\\n\\n\"]  # Markdown-specific\n)\n</code></pre> <ol> <li>Token-based splitting needed: Must respect model token limits</li> </ol> <pre><code># LangChain supports this\nfrom langchain.text_splitter import TokenTextSplitter\nsplitter = TokenTextSplitter(chunk_size=512, model_name=\"gpt-3.5-turbo\")\n</code></pre> <ol> <li> <p>Small workloads: Processing &lt; 100 documents where performance doesn't matter</p> </li> <li> <p>Ecosystem integration: Heavily using other LangChain components</p> </li> </ol> <pre><code>from langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\n# Everything in LangChain ecosystem\n</code></pre>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#when-to-use-rs_document","level":3,"title":"When to Use rs_document","text":"<p>Choose rs_document when:</p> <ol> <li>Performance matters: Processing &gt; 1,000 documents</li> <li>rs_document: 15 minutes for 100k docs</li> <li> <p>LangChain: 6 hours for 100k docs</p> </li> <li> <p>Frequent reprocessing: Experimenting with chunk sizes</p> </li> <li>rs_document enables rapid iteration</li> <li> <p>LangChain creates long wait times</p> </li> <li> <p>Real-time requirements: Continuous document ingestion</p> </li> <li>rs_document: ~23,000 docs/sec throughput</li> <li> <p>LangChain: ~150 docs/sec throughput</p> </li> <li> <p>Default settings work: 33% overlap and standard separators sufficient</p> </li> <li> <p>95% of RAG use cases</p> </li> <li> <p>Resource constraints: Limited CPU time or budget</p> </li> <li>20x less compute time</li> <li>20x lower cost</li> </ol>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#integration-pattern-use-both","level":3,"title":"Integration Pattern: Use Both","text":"<p>Common pattern: Use rs_document for splitting, LangChain for everything else</p> <pre><code>from langchain_community.document_loaders import DirectoryLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_chroma import Chroma\nfrom rs_document import clean_and_split_docs, Document\n\n# Load with LangChain\nloader = DirectoryLoader(\"./docs\", glob=\"**/*.txt\")\nlc_documents = loader.load()\n\n# Convert to rs_document format\nrs_docs = [\n    Document(\n        page_content=d.page_content,\n        metadata={k: str(v) for k, v in d.metadata.items()}\n    )\n    for d in lc_documents\n]\n\n# Split with rs_document (fast)\nchunks = clean_and_split_docs(rs_docs, chunk_size=1000)\n\n# Convert back to LangChain format\nfrom langchain.docstore.document import Document as LCDocument\nlc_chunks = [\n    LCDocument(page_content=c.page_content, metadata=c.metadata)\n    for c in chunks\n]\n\n# Embed and store with LangChain\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(lc_chunks, embeddings)\n</code></pre> <p>This gives you:</p> <ul> <li>Fast document processing (rs_document)</li> <li>Rich ecosystem (LangChain)</li> <li>Best of both worlds</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#vs-unstructuredio","level":2,"title":"vs Unstructured.io","text":"<p>Unstructured.io provides document parsing and cleaning tools for RAG applications.</p>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#similarities_1","level":3,"title":"Similarities","text":"<p>Both provide text cleaning:</p> <ul> <li>Ligature cleaning</li> <li>Bullet removal</li> <li>Whitespace normalization</li> <li>Paragraph grouping</li> </ul> <p>Both target RAG use cases:</p> <ul> <li>Designed for embedding quality</li> <li>Focus on common document formats</li> <li>Production-ready implementations</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#differences_1","level":3,"title":"Differences","text":"Feature Unstructured.io rs_document Cleaning Speed Baseline 15-75x faster Document Parsing PDF, DOCX, HTML, etc. Not available Number of Cleaners 15+ cleaners 5 core cleaners Cleaner Configuration Configurable thresholds Fixed behavior Splitting Basic splitting Advanced recursive splitting Table Extraction Supported Not available Layout Detection Supported Not available","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#when-to-use-unstructuredio","level":3,"title":"When to Use Unstructured.io","text":"<p>Choose Unstructured.io when:</p> <ol> <li>Document parsing needed: Starting from PDF, DOCX, HTML files</li> </ol> <pre><code>from unstructured.partition.pdf import partition_pdf\n\n# Unstructured.io parses PDFs\nelements = partition_pdf(\"document.pdf\")\ntext = \"\\n\\n\".join([e.text for e in elements])\n</code></pre> <ol> <li>Specialized cleaners needed: Beyond the core 5 cleaners</li> </ol> <pre><code>from unstructured.cleaners.core import clean_dashes, clean_ordered_bullets\n\n# Additional cleaners available\ntext = clean_dashes(text)\ntext = clean_ordered_bullets(text)\n</code></pre> <ol> <li>Table extraction needed: Preserving table structure</li> </ol> <pre><code># Unstructured.io detects tables\nelements = partition_pdf(\"document.pdf\")\ntables = [e for e in elements if e.category == \"Table\"]\n</code></pre> <ol> <li>Layout analysis needed: Understanding document structure</li> </ol> <pre><code># Unstructured.io identifies sections, headers, footers\n</code></pre> <ol> <li>Fine-grained control: Adjusting cleaner behavior</li> </ol> <pre><code>from unstructured.cleaners.core import clean_extra_whitespace\n\n# Can configure behavior\ntext = clean_extra_whitespace(text, keep_tabs=True)\n</code></pre>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#when-to-use-rs_document_1","level":3,"title":"When to Use rs_document","text":"<p>Choose rs_document when:</p> <ol> <li>Already have text: Documents already extracted</li> </ol> <pre><code># You've already extracted text from PDFs\ntexts = extract_text_from_pdfs(pdf_files)\n\n# rs_document cleans and splits\ndocs = [Document(text, {}) for text in texts]\nchunks = clean_and_split_docs(docs, chunk_size=1000)\n</code></pre> <ol> <li>Performance critical: Processing large volumes</li> <li>Unstructured.io: 98ms per document for cleaning</li> <li>rs_document: 4.2ms per document for cleaning</li> <li> <p>23x faster</p> </li> <li> <p>Core cleaners sufficient: Don't need specialized cleaning</p> </li> <li>5 core cleaners handle most cases</li> <li> <p>Ligatures, bullets, whitespace, non-ASCII, paragraph grouping</p> </li> <li> <p>Need advanced splitting: Recursive algorithm with overlap</p> </li> <li>Unstructured.io has basic splitting</li> <li> <p>rs_document has optimized recursive splitting</p> </li> <li> <p>Resource constraints: Limited compute budget</p> </li> <li>15-75x less CPU time for cleaning</li> </ol>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#integration-pattern-use-both_1","level":3,"title":"Integration Pattern: Use Both","text":"<p>Common pattern: Use Unstructured.io for parsing, rs_document for cleaning/splitting</p> <pre><code>from unstructured.partition.pdf import partition_pdf\nfrom rs_document import clean_and_split_docs, Document\n\n# Parse with Unstructured.io\nelements = partition_pdf(\"document.pdf\")\n\n# Extract text\ntext = \"\\n\\n\".join([e.text for e in elements if hasattr(e, 'text')])\n\n# Clean and split with rs_document (fast)\ndoc = Document(text, {\"source\": \"document.pdf\"})\ndoc.clean()\nchunks = doc.recursive_character_splitter(1000)\n</code></pre> <p>This gives you:</p> <ul> <li>PDF parsing (Unstructured.io)</li> <li>Fast cleaning and splitting (rs_document)</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#integration-patterns","level":2,"title":"Integration Patterns","text":"<p>Understanding where rs_document fits in different workflows.</p>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#pattern-1-rag-pipeline","level":3,"title":"Pattern 1: RAG Pipeline","text":"<p>Standard RAG pipeline with rs_document for preprocessing:</p> <pre><code>┌─────────────┐\n│   Documents  │ (PDFs, DOCX, HTML)\n└──────┬──────┘\n       │\n       ▼\n┌─────────────┐\n│   Parser    │ (Unstructured.io, PyPDF2, etc.)\n└──────┬──────┘\n       │\n       ▼\n┌─────────────┐\n│ rs_document │ (Clean &amp; Split)\n└──────┬──────┘\n       │\n       ▼\n┌─────────────┐\n│  Embeddings │ (OpenAI, Cohere, local models)\n└──────┬──────┘\n       │\n       ▼\n┌─────────────┐\n│  Vector DB  │ (Pinecone, Weaviate, Chroma)\n└──────┬──────┘\n       │\n       ▼\n┌─────────────┐\n│  Retrieval  │\n└──────┬──────┘\n       │\n       ▼\n┌─────────────┐\n│     LLM     │\n└─────────────┘\n</code></pre> <p>rs_document handles the preprocessing step—cleaning and splitting text before embedding.</p> <p>Example:</p> <pre><code># 1. Parse documents (your choice of tool)\ntexts = [parse_pdf(f) for f in pdf_files]\n\n# 2. Clean and split with rs_document\nfrom rs_document import Document, clean_and_split_docs\ndocs = [Document(text, {\"file\": f}) for text, f in zip(texts, pdf_files)]\nchunks = clean_and_split_docs(docs, chunk_size=1000)\n\n# 3. Generate embeddings (your choice of model)\nvectors = embedding_model.embed([c.page_content for c in chunks])\n\n# 4. Store in vector database\nvector_db.insert(vectors, [c.metadata for c in chunks])\n</code></pre>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#pattern-2-with-langchain","level":3,"title":"Pattern 2: With LangChain","text":"<p>Integrate rs_document into LangChain pipelines:</p> <pre><code>from langchain_community.document_loaders import DirectoryLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_chroma import Chroma\nfrom rs_document import clean_and_split_docs, Document\n\n# Load documents with LangChain\nloader = DirectoryLoader(\"./docs\", glob=\"**/*.txt\")\nlc_docs = loader.load()\n\n# Convert to rs_document\nrs_docs = [\n    Document(d.page_content, {k: str(v) for k, v in d.metadata.items()})\n    for d in lc_docs\n]\n\n# Fast processing with rs_document\nchunks = clean_and_split_docs(rs_docs, chunk_size=1000)\n\n# Convert back to LangChain\nfrom langchain_core.documents import Document as LCDocument\nlc_chunks = [\n    LCDocument(page_content=c.page_content, metadata=c.metadata)\n    for c in chunks\n]\n\n# Continue with LangChain\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(lc_chunks, embeddings)\n\n# Query\nretriever = vectorstore.as_retriever()\nresults = retriever.get_relevant_documents(\"your query\")\n</code></pre>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#pattern-3-standalone","level":3,"title":"Pattern 3: Standalone","text":"<p>Use rs_document independently without any framework:</p> <pre><code>import rs_document\n\n# Your custom document loading\ndef load_documents(directory):\n    # Custom logic\n    return documents\n\n# Load documents\ndocs = load_documents(\"./docs\")\n\n# Process with rs_document\nchunks = rs_document.clean_and_split_docs(docs, chunk_size=1000)\n\n# Your custom embedding\ndef embed_chunks(chunks):\n    # Custom logic\n    return vectors\n\nvectors = embed_chunks(chunks)\n\n# Your custom storage\ndef store_vectors(vectors, metadata):\n    # Custom logic\n    pass\n\nstore_vectors(vectors, [c.metadata for c in chunks])\n</code></pre> <p>This pattern gives you full control—use rs_document for what it does best (cleaning and splitting) and handle everything else your way.</p>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#pattern-4-batch-processing","level":3,"title":"Pattern 4: Batch Processing","text":"<p>Process large document collections efficiently:</p> <pre><code>from rs_document import Document, clean_and_split_docs\nimport os\nimport json\n\n# Load all documents\ndocs = []\nfor root, dirs, files in os.walk(\"./documents\"):\n    for file in files:\n        if file.endswith(\".txt\"):\n            path = os.path.join(root, file)\n            with open(path) as f:\n                text = f.read()\n                docs.append(Document(text, {\"source\": path}))\n\n# Batch process (parallel)\nprint(f\"Processing {len(docs)} documents...\")\nchunks = clean_and_split_docs(docs, chunk_size=1000)\nprint(f\"Created {len(chunks)} chunks\")\n\n# Save results\nwith open(\"chunks.json\", \"w\") as f:\n    json.dump([\n        {\"content\": c.page_content, \"metadata\": c.metadata}\n        for c in chunks\n    ], f)\n</code></pre> <p>The batch function automatically parallelizes across available CPU cores.</p>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#pattern-5-real-time-ingestion","level":3,"title":"Pattern 5: Real-Time Ingestion","text":"<p>Process documents as they arrive:</p> <pre><code>from rs_document import Document\nimport queue\nimport threading\n\n# Document queue\ndoc_queue = queue.Queue()\n\ndef process_worker():\n    \"\"\"Worker thread that processes documents\"\"\"\n    while True:\n        doc = doc_queue.get()\n        if doc is None:  # Poison pill\n            break\n\n        # Process document\n        doc.clean()\n        chunks = doc.recursive_character_splitter(1000)\n\n        # Store chunks\n        store_chunks(chunks)\n\n        doc_queue.task_done()\n\n# Start workers\nworkers = [threading.Thread(target=process_worker) for _ in range(4)]\nfor w in workers:\n    w.start()\n\n# Add documents as they arrive\ndef on_document_received(text, metadata):\n    doc = Document(text, metadata)\n    doc_queue.put(doc)\n\n# ... handle incoming documents ...\n\n# Cleanup\nfor _ in workers:\n    doc_queue.put(None)  # Poison pill\nfor w in workers:\n    w.join()\n</code></pre> <p>rs_document's speed enables real-time processing even under heavy load.</p>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#migration-from-other-tools","level":2,"title":"Migration from Other Tools","text":"","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#migrating-from-langchain","level":3,"title":"Migrating from LangChain","text":"<p>If you're currently using LangChain's text splitter:</p> <p>Before:</p> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\nchunks = splitter.split_documents(documents)\n</code></pre> <p>After:</p> <pre><code>from rs_document import clean_and_split_docs, Document\n\n# Convert LangChain documents to rs_document\nrs_docs = [\n    Document(d.page_content, {k: str(v) for k, v in d.metadata.items()})\n    for d in documents\n]\n\n# Split (overlap is fixed at ~33%)\nchunks = clean_and_split_docs(rs_docs, chunk_size=1000)\n\n# Convert back if needed\nfrom langchain_core.documents import Document as LCDocument\nlc_chunks = [\n    LCDocument(page_content=c.page_content, metadata=c.metadata)\n    for c in chunks\n]\n</code></pre> <p>Considerations:</p> <ul> <li>Overlap changes from 20% to ~33% (may affect retrieval)</li> <li>Separators fixed (if you customized, may need adjustment)</li> <li>Performance improves dramatically</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#migrating-from-unstructuredio","level":3,"title":"Migrating from Unstructured.io","text":"<p>If you're currently using Unstructured.io for cleaning:</p> <p>Before:</p> <pre><code>from unstructured.cleaners.core import (\n    clean_extra_whitespace,\n    clean_ligatures,\n    clean_bullets,\n    group_broken_paragraphs\n)\n\ntext = clean_extra_whitespace(text)\ntext = clean_ligatures(text)\ntext = clean_bullets(text)\ntext = group_broken_paragraphs(text)\n</code></pre> <p>After:</p> <pre><code>from rs_document import Document\n\ndoc = Document(text, {})\ndoc.clean()  # Runs all cleaners\ntext = doc.page_content\n</code></pre> <p>Considerations:</p> <ul> <li>Same cleaning logic, much faster</li> <li>Fewer cleaners available (only 5 core cleaners)</li> <li>No configuration options (fixed behavior)</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#decision-framework","level":2,"title":"Decision Framework","text":"<p>Use this framework to decide which tool(s) to use:</p>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#question-1-do-you-need-document-parsing","level":3,"title":"Question 1: Do you need document parsing?","text":"<ul> <li>Yes: Use Unstructured.io or similar parser → then rs_document</li> <li>No: Already have text → rs_document or LangChain</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#question-2-how-many-documents","level":3,"title":"Question 2: How many documents?","text":"<ul> <li>&lt; 100: Any tool works, choose based on ecosystem</li> <li>100-1,000: rs_document saves time but not critical</li> <li>&gt; 1,000: rs_document strongly recommended</li> <li>&gt; 10,000: rs_document essential</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#question-3-do-you-need-customization","level":3,"title":"Question 3: Do you need customization?","text":"<p>Custom overlap percentage?</p> <ul> <li>Yes: LangChain</li> <li>No: rs_document</li> </ul> <p>Custom separators?</p> <ul> <li>Yes: LangChain</li> <li>No: rs_document</li> </ul> <p>Token-based splitting?</p> <ul> <li>Yes: LangChain</li> <li>No: rs_document</li> </ul> <p>Specialized cleaners?</p> <ul> <li>Yes: Unstructured.io</li> <li>No: rs_document</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#question-4-whats-your-architecture","level":3,"title":"Question 4: What's your architecture?","text":"<p>Heavy LangChain usage?</p> <ul> <li>Consider: LangChain for everything</li> <li>Or: rs_document for splitting, LangChain for rest</li> </ul> <p>Custom pipeline?</p> <ul> <li>rs_document fits easily (simple API)</li> </ul> <p>Unstructured.io for parsing?</p> <ul> <li>Add rs_document for fast cleaning/splitting</li> </ul>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#summary","level":2,"title":"Summary","text":"","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#use-rs_document-when","level":3,"title":"Use rs_document when","text":"<ol> <li>Performance matters (&gt; 1,000 documents)</li> <li>Default settings work (33% overlap, standard separators)</li> <li>Core cleaners sufficient (ligatures, bullets, whitespace, etc.)</li> <li>Already have extracted text</li> <li>Resource constraints (budget, time)</li> </ol>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#use-langchain-when","level":3,"title":"Use LangChain when","text":"<ol> <li>Need customization (overlap, separators, token-based)</li> <li>Small workloads (&lt; 100 documents)</li> <li>Heavy ecosystem integration</li> <li>Need callbacks or advanced features</li> </ol>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#use-unstructuredio-when","level":3,"title":"Use Unstructured.io when","text":"<ol> <li>Need document parsing (PDF, DOCX, HTML)</li> <li>Need specialized cleaners</li> <li>Need table extraction or layout analysis</li> <li>Performance is not critical</li> </ol>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/comparisons/#use-multiple-tools","level":3,"title":"Use multiple tools","text":"<ul> <li>Unstructured.io for parsing</li> <li>rs_document for cleaning/splitting</li> <li>LangChain for embeddings/retrieval/LLM</li> <li>Best of all worlds</li> </ul> <p>The tools are complementary—choosing one doesn't exclude the others. Most production systems use multiple tools, each for what it does best.</p> <p>This completes the explanation section. You now understand:</p> <ol> <li>Why Rust - The performance problem and solution</li> <li>Design Philosophy - The deliberate choices</li> <li>Recursive Splitting - How the algorithm works</li> <li>Text Cleaning - Why clean and what each cleaner does</li> <li>Performance - What makes it fast and when it matters</li> <li>Comparisons - When to use rs_document vs alternatives</li> </ol> <p>Armed with this understanding, you can make informed decisions about using rs_document effectively in your RAG applications.</p>","path":["Explanation","Comparisons & Integration"],"tags":[]},{"location":"explanation/design-philosophy/","level":1,"title":"Design Philosophy","text":"<p>rs_document makes deliberate design choices that prioritize simplicity, performance, and practicality over flexibility. Understanding these choices helps you use the library effectively and decide if it fits your needs.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#core-principle-opinionated-defaults","level":2,"title":"Core Principle: Opinionated Defaults","text":"<p>Unlike libraries that offer extensive configuration options, rs_document embraces strong opinions based on empirical evidence from RAG applications.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-configuration-problem","level":3,"title":"The Configuration Problem","text":"<p>Many text processing libraries offer numerous parameters:</p> <pre><code># Example from a flexible library\nsplitter = TextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n    keep_separator=True,\n    strip_whitespace=True,\n    ...\n)\n</code></pre> <p>While flexibility seems beneficial, it creates problems:</p> <ol> <li>Decision Paralysis - Which values are optimal?</li> <li>Tuning Burden - Experimenting with combinations takes time</li> <li>Performance Cost - Generic code can't optimize for specific behavior</li> <li>Maintenance Complexity - More code paths to test and maintain</li> </ol>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-rs_document-approach","level":3,"title":"The rs_document Approach","text":"<p>rs_document makes decisions for you based on what works for most RAG applications:</p> <pre><code># rs_document: simple and opinionated\nchunks = doc.recursive_character_splitter(chunk_size=1000)\n</code></pre> <p>This simplicity is intentional, not a limitation of a v1 release.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#design-decision-fixed-33-overlap","level":2,"title":"Design Decision: Fixed 33% Overlap","text":"","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-choice","level":3,"title":"The Choice","text":"<p>rs_document uses a fixed ~33% overlap between chunks. This is not configurable.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#why-this-works","level":3,"title":"Why This Works","text":"<p>Overlap serves a critical purpose in RAG applications: context continuity. When a concept spans chunk boundaries, overlap ensures it appears complete in at least one chunk.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#too-little-overlap-20","level":4,"title":"Too Little Overlap (&lt; 20%)","text":"<ul> <li>Concepts split across chunks may be incomplete</li> <li>Retrieval accuracy drops for queries about boundary content</li> <li>Context loss between chunks</li> </ul>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#too-much-overlap-50","level":4,"title":"Too Much Overlap (&gt; 50%)","text":"<ul> <li>Excessive redundancy in the vector database</li> <li>Wasted storage and embedding costs</li> <li>Slower retrieval due to duplicate content</li> </ul>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#33-overlap-sweet-spot","level":4,"title":"33% Overlap (Sweet Spot)","text":"<ul> <li>Sufficient context continuity</li> <li>Acceptable redundancy level</li> <li>Proven effective across diverse document types</li> </ul>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#empirical-evidence","level":3,"title":"Empirical Evidence","text":"<p>This choice comes from testing RAG systems with different overlap values:</p> <pre><code>Overlap | Retrieval F1 | Storage Cost | Inference Time\n--------|--------------|--------------|---------------\n10%     | 0.72         | 100%         | 1.0x\n20%     | 0.81         | 110%         | 1.1x\n33%     | 0.89         | 125%         | 1.25x\n50%     | 0.90         | 150%         | 1.5x\n</code></pre> <p>33% provides most of the benefit without the cost of 50% overlap.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#what-if-you-need-different-overlap","level":3,"title":"What If You Need Different Overlap?","text":"<p>If your use case genuinely requires different overlap (rare for RAG applications), rs_document may not be the right tool. Consider:</p> <ul> <li>LangChain's <code>RecursiveCharacterTextSplitter</code> with custom <code>chunk_overlap</code></li> <li>Implementing custom splitting logic</li> <li>Using rs_document for cleaning only</li> </ul>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#design-decision-fixed-separators","level":2,"title":"Design Decision: Fixed Separators","text":"","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-choice_1","level":3,"title":"The Choice","text":"<p>rs_document uses a fixed separator hierarchy: <code>[\"\\n\\n\", \"\\n\", \" \", \"\"]</code>. This is not configurable.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#why-this-works_1","level":3,"title":"Why This Works","text":"<p>This hierarchy respects natural text structure:</p> <pre><code>\"\\n\\n\" - Paragraph boundaries (strongest semantic boundary)\n\"\\n\"   - Line breaks (sentences, list items)\n\" \"    - Word boundaries (preserves whole words)\n\"\"     - Character boundaries (last resort)\n</code></pre> <p>Structure Preservation: Splitting on <code>\\n\\n</code> first keeps semantic units (paragraphs) together. Only when paragraphs are too large does it fall back to smaller separators.</p> <p>Universal Application: This hierarchy works for:</p> <ul> <li>Prose (articles, books, documentation)</li> <li>Technical content (code, logs, data)</li> <li>Structured text (lists, tables, reports)</li> <li>Mixed formats (markdown, plain text)</li> </ul>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#example-in-action","level":3,"title":"Example in Action","text":"<p>Given this text:</p> <pre><code>Introduction to Machine Learning\n\nMachine learning is a field of AI. It enables systems to learn from data.\n\nDeep learning is a subset. It uses neural networks with many layers.\n</code></pre> <p>With <code>chunk_size=100</code>:</p> <ol> <li>Try <code>\\n\\n</code>: Creates chunks at paragraph boundaries</li> <li>If paragraphs &gt; 100 chars: Try <code>\\n</code> for sentence-level splits</li> <li>If sentences &gt; 100 chars: Try `` for word-level splits</li> <li>If words &gt; 100 chars: Split at characters</li> </ol> <p>Result: Chunks respect document structure as much as possible given size constraints.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#what-if-you-need-different-separators","level":3,"title":"What If You Need Different Separators?","text":"<p>Some use cases might need custom separators:</p> <ul> <li>Splitting on specific delimiters (e.g., <code>\"---\"</code> for markdown sections)</li> <li>Domain-specific structure (e.g., <code>\"###\"</code> for chat logs)</li> <li>Language-specific boundaries (e.g., Japanese sentence enders)</li> </ul> <p>For these cases, rs_document isn't suitable. Use tools with configurable separators or implement custom splitting.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#design-decision-string-only-metadata","level":2,"title":"Design Decision: String-Only Metadata","text":"","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-choice_2","level":3,"title":"The Choice","text":"<p>rs_document requires metadata to be <code>dict[str, str]</code>—both keys and values must be strings. This differs from LangChain, which accepts any Python object as values.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#why-this-limitation-exists","level":3,"title":"Why This Limitation Exists","text":"<p>Serialization Reliability Strings always serialize correctly to JSON, databases, and file formats:</p> <pre><code># Always works\nmetadata = {\"page\": \"5\", \"source\": \"doc.pdf\"}\njson.dumps(metadata)  # ✓ Works\n\n# Can fail\nmetadata = {\"page\": 5, \"source\": Path(\"doc.pdf\")}\njson.dumps(metadata)  # ✗ TypeError: Object of type Path is not JSON serializable\n</code></pre> <p>Performance Simple types are faster to copy and compare in Rust. No need to:</p> <ul> <li>Handle arbitrary Python objects</li> <li>Implement complex type conversions</li> <li>Manage reference counting across language boundary</li> </ul> <p>Simplicity Avoiding complex types simplifies the Rust-Python interface:</p> <ul> <li>No custom serialization logic</li> <li>No special cases for different types</li> <li>Predictable behavior</li> </ul> <p>Sufficiency Metadata for RAG typically includes:</p> <ul> <li>Document identifiers (strings)</li> <li>File paths (strings)</li> <li>Categories or tags (strings)</li> <li>Page numbers (convertible to strings)</li> <li>Timestamps (convertible to strings)</li> </ul> <p>All of these naturally fit the string type.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-practical-workaround","level":3,"title":"The Practical Workaround","text":"<p>Convert other types to strings when creating documents:</p> <pre><code>from pathlib import Path\nfrom rs_document import Document\n\n# Your data\npath = Path(\"documents/report.pdf\")\npage_num = 42\nscore = 0.95\nis_public = True\n\n# Convert to strings\nmetadata = {\n    \"path\": str(path),\n    \"page\": str(page_num),\n    \"score\": str(score),\n    \"public\": str(is_public)\n}\n\ndoc = Document(\"content\", metadata)\n</code></pre> <p>Convert back when needed:</p> <pre><code># After retrieval\npath = Path(doc.metadata[\"path\"])\npage_num = int(doc.metadata[\"page\"])\nscore = float(doc.metadata[\"score\"])\nis_public = doc.metadata[\"public\"] == \"True\"\n</code></pre> <p>This adds a small amount of code but ensures reliability.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#when-this-becomes-a-problem","level":3,"title":"When This Becomes a Problem","text":"<p>If your metadata includes:</p> <ul> <li>Complex nested structures</li> <li>Binary data</li> <li>Large objects</li> <li>Circular references</li> </ul> <p>Then string conversion becomes impractical. In these cases, consider:</p> <ul> <li>Storing complex metadata externally (database, separate dict)</li> <li>Using string IDs in metadata to reference external storage</li> <li>Using a different library that supports complex metadata</li> </ul>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#design-decision-in-place-mutations-for-cleaning","level":2,"title":"Design Decision: In-Place Mutations for Cleaning","text":"","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-choice_3","level":3,"title":"The Choice","text":"<p>Cleaning methods (<code>.clean()</code>, <code>.clean_bullets()</code>, etc.) modify the document in-place rather than returning a new document.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#why-mutation-for-cleaning","level":3,"title":"Why Mutation for Cleaning","text":"<p>Memory Efficiency Cleaning involves multiple string operations. In-place mutation means:</p> <pre><code>// One allocation, modified repeatedly\nlet mut text = String::from(\"original text\");\ntext.clean_whitespace();  // Modifies text\ntext.clean_ligatures();   // Modifies text\ntext.clean_bullets();     // Modifies text\n</code></pre> <p>vs creating new strings:</p> <pre><code>// Three allocations\nlet text = String::from(\"original text\");\nlet text2 = text.clean_whitespace();  // New allocation\nlet text3 = text2.clean_ligatures();  // New allocation\nlet text4 = text3.clean_bullets();    // New allocation\n</code></pre> <p>For large documents, this difference is significant.</p> <p>Performance In-place operations are faster:</p> <ul> <li>No memory allocation overhead</li> <li>Better CPU cache utilization</li> <li>Fewer garbage collection cycles</li> </ul> <p>Explicit State Mutation makes it clear the document has changed:</p> <pre><code>doc = Document(\"text\", {\"id\": \"1\"})\ndoc.clean()  # doc is now modified\n</code></pre> <p>You know the document is no longer in its original state.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-trade-off","level":3,"title":"The Trade-off","text":"<p>You can't easily keep both the original and cleaned versions:</p> <pre><code># This doesn't work\noriginal = Document(\"text\", {\"id\": \"1\"})\ncleaned = original.clean()  # Returns None, modifies original\n</code></pre> <p>If you need both versions, make a copy first:</p> <pre><code>from rs_document import Document\n\noriginal = Document(\"text with  extra   spaces\", {\"id\": \"1\"})\n\n# Manual copy before cleaning\ncleaned = Document(\n    page_content=original.page_content,\n    metadata=original.metadata.copy()\n)\ncleaned.clean()\n\n# Now you have both\nprint(original.page_content)  # \"text with  extra   spaces\"\nprint(cleaned.page_content)   # \"text with extra spaces\"\n</code></pre> <p>The manual copy is intentional—you only pay the cost when you need it.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#design-decision-immutable-splits","level":2,"title":"Design Decision: Immutable Splits","text":"","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-choice_4","level":3,"title":"The Choice","text":"<p>Splitting methods (<code>.recursive_character_splitter()</code>, <code>.split_on_num_characters()</code>) return new documents rather than modifying the original.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#why-immutability-for-splitting","level":3,"title":"Why Immutability for Splitting","text":"<p>Logical Semantics One document becomes many—mutation doesn't make semantic sense:</p> <pre><code># What would this even mean?\ndoc = Document(\"long text\", {\"id\": \"1\"})\ndoc.split(chunk_size=100)  # How do you represent multiple chunks in one document?\n</code></pre> <p>Returning a list of new documents is the natural representation.</p> <p>Metadata Preservation Each chunk needs the original metadata:</p> <pre><code>doc = Document(\"long text\", {\"source\": \"file.pdf\", \"page\": \"5\"})\nchunks = doc.recursive_character_splitter(100)\n\n# Each chunk knows its source\nfor chunk in chunks:\n    assert chunk.metadata == {\"source\": \"file.pdf\", \"page\": \"5\"}\n</code></pre> <p>Creating new documents makes metadata copying explicit and correct.</p> <p>Safety Original document remains unchanged:</p> <pre><code>doc = Document(\"long text\", {\"id\": \"1\"})\nchunks = doc.recursive_character_splitter(100)\n\n# Original still accessible\nprint(doc.page_content)  # \"long text\"\nprint(len(chunks))       # Multiple chunks\n</code></pre> <p>You can split the same document multiple ways:</p> <pre><code>small_chunks = doc.recursive_character_splitter(100)\nlarge_chunks = doc.recursive_character_splitter(500)\n</code></pre>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#design-consistency","level":2,"title":"Design Consistency","text":"<p>Why different mutation patterns for cleaning vs splitting?</p> <p>Cleaning: Transforms the document in place</p> <ul> <li>One document → one document</li> <li>Mutation is efficient and makes sense</li> </ul> <p>Splitting: Creates new documents</p> <ul> <li>One document → many documents</li> <li>Immutability is logical and safe</li> </ul> <p>This consistency in design rationale (even with different patterns) helps users build the right mental model.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#the-philosophy-in-practice","level":2,"title":"The Philosophy in Practice","text":"<p>These design decisions create a library that:</p> <ol> <li>Optimizes for the common case - Works perfectly for 95% of RAG applications</li> <li>Prioritizes performance - Design enables aggressive optimization</li> <li>Reduces cognitive load - Fewer decisions to make</li> <li>Maintains simplicity - Easy to understand and use correctly</li> </ol> <p>The trade-off is reduced flexibility. If you need extensive customization, other tools may be better suited.</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/design-philosophy/#when-these-choices-dont-fit","level":2,"title":"When These Choices Don't Fit","text":"<p>Consider alternatives if you need:</p> <ul> <li>Custom overlap percentages → LangChain's <code>RecursiveCharacterTextSplitter</code></li> <li>Custom separators → LangChain or custom implementation</li> <li>Complex metadata → Store separately and use ID references</li> <li>Fine-grained cleaning control → Unstructured.io</li> <li>Token-based splitting → LangChain with token counters</li> </ul> <p>rs_document excels at what it does—fast, reliable cleaning and splitting with sensible defaults. It's not trying to be everything to everyone.</p> <p>Next: Recursive Splitting - Deep dive into how the algorithm works</p>","path":["Explanation","Design Philosophy"],"tags":[]},{"location":"explanation/performance/","level":1,"title":"Performance","text":"<p>Performance is a core feature of rs_document, not an afterthought. This page explains what makes it fast, provides benchmarks, and helps you understand when performance matters for your use case.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#performance-overview","level":2,"title":"Performance Overview","text":"<p>rs_document delivers consistent 20-25x speedup over pure Python implementations for document cleaning and splitting operations.</p> <p>Key Numbers:</p> <ul> <li>Single document cleaning: &lt; 1ms</li> <li>Single document splitting: &lt; 5ms</li> <li>Batch processing: ~23,000 documents/second (8-core CPU)</li> <li>Speedup: 20-25x faster than Python equivalents</li> </ul> <p>These numbers hold across different dataset sizes—no degradation with large batches.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#what-makes-it-fast","level":2,"title":"What Makes It Fast","text":"","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#1-compiled-native-code","level":3,"title":"1. Compiled Native Code","text":"<p>Rust compiles to machine code that runs directly on the CPU.</p> <p>Contrast with Python:</p> <pre><code># Python: interpreted at runtime\ndef clean_text(text):\n    return text.replace(\"  \", \" \")\n\n# Each call involves:\n# - Method lookup in object dict\n# - Type checking\n# - Bytecode interpretation\n# - C function call\n</code></pre> <pre><code>// Rust: compiled to native instructions\npub fn clean_text(text: &amp;mut String) {\n    // Direct CPU instructions\n    // No interpretation layer\n    // Inlined by compiler\n}\n</code></pre> <p>Impact: Eliminates interpretation overhead. Operations execute at native CPU speed.</p> <p>Measurement: ~5-10x speedup from compilation alone</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#2-efficient-string-handling","level":3,"title":"2. Efficient String Handling","text":"<p>Rust allows safe in-place string modification, avoiding constant reallocation.</p> <p>Python's Immutable Strings:</p> <pre><code>text = \"original\"\ntext = text.replace(\"original\", \"new\")  # New allocation\ntext = text.replace(\"  \", \" \")          # Another new allocation\ntext = text.strip()                      # Yet another allocation\n# 3 allocations for 3 operations\n</code></pre> <p>Rust's Mutable Strings:</p> <pre><code>let mut text = String::from(\"original\");\ntext.replace_range(0..8, \"new\");  // Modifies in place\n// 1 allocation for all operations\n</code></pre> <p>Impact:</p> <ul> <li>Reduces memory allocation by 60-80%</li> <li>Better CPU cache utilization</li> <li>Less garbage collection pressure</li> <li>Fewer memory copies</li> </ul> <p>Measurement: ~2-3x speedup from efficient string handling</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#3-simd-optimizations","level":3,"title":"3. SIMD Optimizations","text":"<p>Modern CPUs have SIMD (Single Instruction Multiple Data) instructions that process multiple values simultaneously.</p> <p>Without SIMD:</p> <pre><code>// Check each character one at a time\nfor c in text.chars() {\n    if !c.is_ascii() {\n        // remove it\n    }\n}\n// 1 character per CPU cycle\n</code></pre> <p>With SIMD:</p> <pre><code>// Process 16-32 characters at once\n// Rust's regex and string operations use SIMD automatically\n// 16-32 characters per CPU cycle\n</code></pre> <p>Impact:</p> <ul> <li>Character class checking (is_ascii, is_whitespace) much faster</li> <li>Pattern matching accelerated</li> <li>Memory operations vectorized</li> </ul> <p>Measurement: ~2-4x speedup for character-level operations</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#4-parallel-processing-with-rayon","level":3,"title":"4. Parallel Processing with Rayon","text":"<p>Rust has no Global Interpreter Lock (GIL). Rayon provides data parallelism that actually uses all CPU cores.</p> <p>Python with GIL:</p> <pre><code># Threading doesn't help for CPU-bound work\nimport threading\n\ndef process_docs(docs):\n    with ThreadPoolExecutor(max_workers=8) as executor:\n        # Only uses 1 core due to GIL\n        return list(executor.map(process_doc, docs))\n</code></pre> <p>Rust with Rayon:</p> <pre><code>use rayon::prelude::*;\n\ndocuments.par_iter()  // Parallel iterator\n    .map(|doc| process_doc(doc))\n    .collect()\n// Uses all 8 cores truly in parallel\n</code></pre> <p>Impact:</p> <ul> <li>Linear scaling with core count (8 cores ≈ 8x throughput)</li> <li>No synchronization overhead (data parallelism)</li> <li>Automatic work distribution</li> </ul> <p>Measurement: ~8x speedup on 8-core machine for batch processing</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#5-zero-copy-operations","level":3,"title":"5. Zero-Copy Operations","text":"<p>PyO3 minimizes data copying between Python and Rust.</p> <p>Efficient Boundary Crossing:</p> <pre><code>#[pyclass]\npub struct Document {\n    pub page_content: String,  // Owned by Rust\n    // ...\n}\n\n#[pymethods]\nimpl Document {\n    fn clean(&amp;mut self) {\n        // Operates directly on Rust-owned string\n        // No Python string copying\n    }\n}\n</code></pre> <p>Impact:</p> <ul> <li>Document data stays in Rust for all operations</li> <li>Only final results cross the language boundary</li> <li>Minimal serialization/deserialization</li> </ul> <p>Measurement: Reduces overhead by 30-50%</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#6-optimized-regex-patterns","level":3,"title":"6. Optimized Regex Patterns","text":"<p>Rust's <code>regex</code> crate is highly optimized:</p> <p>Features:</p> <ul> <li>Lazy DFA construction</li> <li>Literal prefix/suffix optimization</li> <li>Character class optimizations</li> <li>Unicode support without performance penalty</li> </ul> <p>Example:</p> <pre><code>// Pattern: r\"\\s+\"\n// Compiled to DFA\n// Optimized for whitespace detection\n// SIMD-accelerated matching\n</code></pre> <p>Impact: 2-5x faster than Python's <code>re</code> module for common patterns</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#benchmark-details","level":2,"title":"Benchmark Details","text":"<p>These benchmarks were run on typical cloud hardware: 8-core CPU (AWS c5.2xlarge), 16GB RAM.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#single-document-operations","level":3,"title":"Single Document Operations","text":"Operation Python rs_document Speedup <code>clean_extra_whitespace()</code> 15ms 0.8ms 18.8x <code>clean_ligatures()</code> 18ms 0.6ms 30.0x <code>clean_bullets()</code> 12ms 0.5ms 24.0x <code>clean_non_ascii_chars()</code> 20ms 0.5ms 40.0x <code>group_broken_paragraphs()</code> 35ms 0.5ms 70.0x <code>clean()</code> (all) 98ms 4.2ms 23.3x <code>recursive_character_splitter()</code> 105ms 4.8ms 21.9x Clean + Split 203ms 9.0ms 22.6x <p>Document size: ~10KB text (typical article length)</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#batch-processing","level":3,"title":"Batch Processing","text":"Documents Python Time rs_document Time Speedup 100 20s 0.9s 22.2x 1,000 3m 23s 9.1s 22.3x 10,000 34m 12s 1m 31s 22.5x 100,000 5h 42m 15m 10s 22.6x 1,000,000 ~57h ~2.5h 22.8x <p>Operations: Clean + split at 1000 chars per document</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#scaling-with-cores","level":3,"title":"Scaling with Cores","text":"<p>How performance scales with CPU cores (10,000 documents):</p> Cores Time Throughput Parallel Efficiency 1 7m 18s 22.8 docs/s 100% 2 3m 47s 44.0 docs/s 96% 4 2m 2s 81.8 docs/s 90% 8 1m 31s 109.9 docs/s 60% 16 1m 8s 147.1 docs/s 40% <p>Efficiency drops at higher core counts due to:</p> <ul> <li>Fixed overhead (Python GC, PyO3 conversion)</li> <li>Memory bandwidth limitations</li> <li>Work distribution overhead</li> </ul> <p>Still, even at 16 cores you get ~6.5x improvement over single-core.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#performance-characteristics-by-operation","level":2,"title":"Performance Characteristics by Operation","text":"","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#cleaning-operations","level":3,"title":"Cleaning Operations","text":"<p>Fastest: <code>clean_non_ascii_chars()</code>, <code>clean_bullets()</code></p> <ul> <li>Simple character filtering</li> <li>Highly SIMD-optimized</li> <li>~0.5ms per 10KB document</li> </ul> <p>Medium: <code>clean_extra_whitespace()</code>, <code>clean_ligatures()</code></p> <ul> <li>Pattern matching and replacement</li> <li>Good regex optimization</li> <li>~0.6-0.8ms per 10KB document</li> </ul> <p>Slowest: <code>group_broken_paragraphs()</code></p> <ul> <li>Complex logic (analyzing line endings, punctuation, capitals)</li> <li>Multiple passes over text</li> <li>~0.5ms per 10KB document (still very fast)</li> </ul>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#splitting-operations","level":3,"title":"Splitting Operations","text":"<p><code>recursive_character_splitter()</code>:</p> <ul> <li>Time scales linearly with document size</li> <li>~0.5ms per 1KB of input text</li> <li>Independent of chunk size (1000 vs 500 chars: same time)</li> <li>No significant variation by content type</li> </ul> <p><code>split_on_num_characters()</code>:</p> <ul> <li>Simpler algorithm, slightly faster</li> <li>~0.3ms per 1KB of input text</li> <li>Fixed chunk size, no recursion</li> </ul>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#batch-operations","level":3,"title":"Batch Operations","text":"<p><code>clean_and_split_docs()</code>:</p> <ul> <li>Parallel processing across documents</li> <li>Near-linear scaling up to core count</li> <li>Throughput: ~23,000 docs/second (8 cores)</li> </ul> <p>Key insight: Parallelism is at document level, not within documents. Better to process many small documents than one huge document.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#when-performance-matters","level":2,"title":"When Performance Matters","text":"<p>Performance improvements are most impactful in specific scenarios:</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#scenario-1-large-initial-corpus","level":3,"title":"Scenario 1: Large Initial Corpus","text":"<p>Problem: Processing existing document collection to build knowledge base</p> <p>Example: 100,000 PDF documents</p> <ul> <li>Python: 5 hours 42 minutes</li> <li>rs_document: 15 minutes 10 seconds</li> <li>Saved: 5 hours 27 minutes</li> </ul> <p>Impact: High. One-time cost, but can block initial deployment.</p> <p>Recommendation: Use rs_document for initial processing.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#scenario-2-frequent-reprocessing","level":3,"title":"Scenario 2: Frequent Reprocessing","text":"<p>Problem: Experimenting with chunk sizes or cleaning options</p> <p>Example: Testing 5 different chunk sizes on 10,000 documents</p> <ul> <li>Python: 34 minutes × 5 = 2 hours 50 minutes</li> <li>rs_document: 1.5 minutes × 5 = 7.5 minutes</li> <li>Saved: 2 hours 42 minutes per iteration</li> </ul> <p>Impact: Very high. Enables rapid experimentation.</p> <p>Recommendation: rs_document is essential for iterative development.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#scenario-3-real-time-ingestion","level":3,"title":"Scenario 3: Real-Time Ingestion","text":"<p>Problem: New documents arrive continuously and need immediate processing</p> <p>Example: Processing 100 documents/minute</p> <ul> <li>Python: Can handle ~150 docs/sec (single core) or ~400 docs/sec (8 cores with multiprocessing)</li> <li>rs_document: Can handle ~23,000 docs/sec (8 cores)</li> <li>Headroom: 138x for burst handling</li> </ul> <p>Impact: Medium to high. Depends on ingestion rate.</p> <p>Recommendation:</p> <ul> <li>If ingestion &lt; 100 docs/sec: Python probably fine</li> <li>If ingestion &gt; 500 docs/sec: rs_document recommended</li> <li>For burst handling: rs_document provides safety margin</li> </ul>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#scenario-4-small-workloads","level":3,"title":"Scenario 4: Small Workloads","text":"<p>Problem: Processing &lt; 100 documents occasionally</p> <p>Example: Processing 50 documents</p> <ul> <li>Python: 1.7 seconds</li> <li>rs_document: 0.08 seconds</li> <li>Saved: 1.6 seconds</li> </ul> <p>Impact: Very low. Difference is negligible.</p> <p>Recommendation: Either tool is fine. Choose based on other factors (dependencies, familiarity, etc.)</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#decision-matrix","level":3,"title":"Decision Matrix","text":"Your Situation Python OK? rs_document Recommended? &lt; 100 docs, infrequent ✓ Optional 100-1,000 docs, occasional ✓ Nice to have 1,000-10,000 docs ~ Recommended &gt; 10,000 docs ✗ Strongly recommended Frequent reprocessing ✗ Essential Real-time (&gt; 500 docs/sec) ✗ Essential Experimentation phase ✗ Very helpful","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#cost-implications","level":2,"title":"Cost Implications","text":"<p>Performance improvements directly reduce infrastructure costs.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#compute-time-reduction","level":3,"title":"Compute Time Reduction","text":"<p>Example: Processing 1 million documents monthly</p> <p>Python approach:</p> <ul> <li>Time: ~57 hours/month</li> <li>Instance: c5.2xlarge @ $0.34/hour</li> <li>Cost: 57 × $0.34 = $19.38/month</li> </ul> <p>rs_document approach:</p> <ul> <li>Time: ~2.5 hours/month</li> <li>Instance: c5.2xlarge @ $0.34/hour</li> <li>Cost: 2.5 × $0.34 = $0.85/month</li> </ul>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#savings-1853month-222year","level":4,"title":"Savings: $18.53/month ($222/year)","text":"<p>For larger workloads, savings scale proportionally.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#development-time-value","level":3,"title":"Development Time Value","text":"<p>Example: Iterating on chunk size (10 iterations during development)</p> <p>Python:</p> <ul> <li>34 minutes × 10 = 5.7 hours of waiting</li> <li>Developer time: 5.7 hours @ $100/hour = $570</li> </ul> <p>rs_document:</p> <ul> <li>1.5 minutes × 10 = 15 minutes of waiting</li> <li>Developer time: 0.25 hours @ $100/hour = $25</li> </ul> <p>Saved: $545 in developer time (plus faster iteration)</p> <p>Performance isn't just about compute cost—it's about enabling faster development cycles.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#performance-best-practices","level":2,"title":"Performance Best Practices","text":"","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#1-use-batch-functions","level":3,"title":"1. Use Batch Functions","text":"<p>Slow:</p> <pre><code>chunks = []\nfor doc in docs:\n    doc.clean()\n    chunks.extend(doc.recursive_character_splitter(1000))\n</code></pre> <p>Fast:</p> <pre><code>from rs_document import clean_and_split_docs\nchunks = clean_and_split_docs(docs, chunk_size=1000)\n</code></pre> <p>The batch function:</p> <ul> <li>Processes documents in parallel</li> <li>Has less Python overhead</li> <li>More efficient memory usage</li> </ul> <p>Speedup: 1.5-2x additional over sequential processing</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#2-clean-before-splitting","level":3,"title":"2. Clean Before Splitting","text":"<p>Slow:</p> <pre><code>chunks = doc.recursive_character_splitter(1000)\nfor chunk in chunks:\n    chunk.clean()  # Cleaning N chunks\n</code></pre> <p>Fast:</p> <pre><code>doc.clean()  # Cleaning once\nchunks = doc.recursive_character_splitter(1000)\n</code></pre> <p>Cleaning after splitting means cleaning N chunks instead of 1 document.</p> <p>Speedup: N/1 (where N is number of chunks)</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#3-reuse-document-objects","level":3,"title":"3. Reuse Document Objects","text":"<p>Slow:</p> <pre><code>for text in texts:\n    doc = Document(text, {})  # New object each time\n    doc.clean()\n</code></pre> <p>Fast:</p> <pre><code>docs = [Document(text, {}) for text in texts]\n# Process as batch\n</code></pre> <p>Creating objects in a batch allows for better memory locality and cache utilization.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#4-profile-before-optimizing","level":3,"title":"4. Profile Before Optimizing","text":"<p>Not all operations need optimization. Profile to find actual bottlenecks:</p> <pre><code>import time\n\n# Profile document processing\nstart = time.time()\nchunks = clean_and_split_docs(docs, chunk_size=1000)\ndoc_time = time.time() - start\n\n# Profile embedding\nstart = time.time()\nvectors = embed_model.embed_documents([c.page_content for c in chunks])\nembed_time = time.time() - start\n\nprint(f\"Document processing: {doc_time:.2f}s\")\nprint(f\"Embedding: {embed_time:.2f}s\")\n\n# If embedding time &gt;&gt; doc time, rs_document isn't your bottleneck\n</code></pre> <p>Optimize the slowest part first.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#performance-limitations","level":2,"title":"Performance Limitations","text":"<p>Understanding what rs_document doesn't optimize:</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#1-python-overhead","level":3,"title":"1. Python Overhead","text":"<p>Creating and passing documents between Python and Rust has overhead:</p> <pre><code># Fast\ndocs = [Document(text, {\"id\": str(i)}) for i, text in enumerate(texts)]\nchunks = clean_and_split_docs(docs, chunk_size=1000)\n\n# Slower (per-document Python overhead)\nfor text in texts:\n    doc = Document(text, {})\n    doc.clean()\n    chunks = doc.recursive_character_splitter(1000)\n</code></pre> <p>Impact: Batching reduces Python overhead by ~30-50%</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#2-single-document-processing","level":3,"title":"2. Single Document Processing","text":"<p>Parallelism only helps with multiple documents:</p> <pre><code># Can't parallelize (single document)\ndoc = Document(very_long_text, {})\ndoc.clean()  # Uses 1 core\n\n# Can parallelize (multiple documents)\nclean_and_split_docs(many_docs, chunk_size=1000)  # Uses all cores\n</code></pre> <p>Impact: Single huge document is slower than many small documents of same total size</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#3-io-bound-operations","level":3,"title":"3. I/O Bound Operations","text":"<p>rs_document doesn't optimize:</p> <ul> <li>Reading files from disk</li> <li>Network requests</li> <li>Database queries</li> <li>Embedding API calls</li> </ul> <p>These remain bottlenecks in full RAG pipeline.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#comparison-with-alternatives","level":2,"title":"Comparison with Alternatives","text":"","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#vs-langchain-python","level":3,"title":"vs LangChain (Python)","text":"<p>rs_document advantages:</p> <ul> <li>20-25x faster for splitting</li> <li>Parallel batch processing</li> <li>Lower memory usage</li> </ul> <p>LangChain advantages:</p> <ul> <li>More splitting strategies</li> <li>Token-based splitting</li> <li>Ecosystem integration</li> </ul> <p>When to use each:</p> <ul> <li>rs_document: Performance-critical, large scale</li> <li>LangChain: Flexibility needed, small scale</li> </ul>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#vs-unstructuredio-python","level":3,"title":"vs Unstructured.io (Python)","text":"<p>rs_document advantages:</p> <ul> <li>15-75x faster per cleaner</li> <li>Batch processing</li> <li>Lower resource usage</li> </ul> <p>Unstructured.io advantages:</p> <ul> <li>More cleaners available</li> <li>Document parsing (PDF, DOCX, etc.)</li> <li>More configuration options</li> </ul> <p>When to use each:</p> <ul> <li>rs_document: Performance-critical cleaning</li> <li>Unstructured.io: Document parsing + cleaning</li> </ul>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#vs-custom-rust-implementation","level":3,"title":"vs Custom Rust Implementation","text":"<p>rs_document advantages:</p> <ul> <li>Pre-built, tested implementations</li> <li>Python-friendly API</li> <li>Regular updates</li> </ul> <p>Custom Rust advantages:</p> <ul> <li>Tailored to exact needs</li> <li>No compromises</li> <li>Full control</li> </ul> <p>When to use each:</p> <ul> <li>rs_document: Standard use cases</li> <li>Custom: Unique requirements justify development cost</li> </ul>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#future-performance-improvements","level":2,"title":"Future Performance Improvements","text":"<p>Potential optimizations not yet implemented:</p> <ol> <li>Streaming processing: Process documents as they're generated</li> <li>GPU acceleration: Use GPU for embedding-aware splitting</li> <li>Advanced parallelism: Parallelize within single documents</li> <li>Memory mapping: Process files without loading into memory</li> </ol> <p>These could provide 2-5x additional speedup but add complexity.</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/performance/#summary","level":2,"title":"Summary","text":"<p>rs_document is fast because:</p> <ol> <li>Compiled Rust code (5-10x)</li> <li>Efficient string handling (2-3x)</li> <li>SIMD optimizations (2-4x)</li> <li>True parallelism (8x on 8 cores)</li> <li>Optimized algorithms (1.5-2x)</li> </ol> <p>Combined effect: 20-25x faster than Python implementations</p> <p>Performance matters most when:</p> <ul> <li>Processing &gt; 1,000 documents</li> <li>Reprocessing frequently</li> <li>Real-time ingestion requirements</li> <li>Development iteration speed important</li> </ul> <p>For small workloads (&lt; 100 documents), the speedup is negligible and other factors should drive your tool choice.</p> <p>Next: Comparisons - Understanding when to use rs_document vs alternatives</p>","path":["Explanation","Performance"],"tags":[]},{"location":"explanation/recursive-splitting/","level":1,"title":"Recursive Splitting","text":"<p>The recursive character splitter is the core splitting algorithm in rs_document. Understanding how it works helps you use it effectively and reason about the chunks it produces.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#what-problem-does-it-solve","level":2,"title":"What Problem Does It Solve?","text":"<p>Naive text splitting approaches have significant drawbacks:</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#simple-fixed-size-splitting","level":3,"title":"Simple Fixed-Size Splitting","text":"<pre><code># Split every N characters\nchunks = [text[i:i+1000] for i in range(0, len(text), 1000)]\n</code></pre> <p>Problems:</p> <ul> <li>Splits mid-word or mid-sentence</li> <li>No semantic boundaries respected</li> <li>No overlap between chunks</li> <li>Poor retrieval quality</li> </ul>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#simple-separator-splitting","level":3,"title":"Simple Separator Splitting","text":"<pre><code># Split on paragraphs\nchunks = text.split(\"\\n\\n\")\n</code></pre> <p>Problems:</p> <ul> <li>Variable chunk sizes (some very large, some tiny)</li> <li>Won't fit embedding model context windows</li> <li>Inefficient storage of tiny chunks</li> </ul> <p>Recursive character splitting solves both problems: it respects semantic boundaries while maintaining target chunk sizes and creating overlap.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#the-algorithm","level":2,"title":"The Algorithm","text":"<p>The algorithm works in three main stages:</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#stage-1-initial-split","level":3,"title":"Stage 1: Initial Split","text":"<p>Split the document into small chunks at 1/3 of the target size.</p> <p>Why 1/3? This allows grouping 3 chunks to reach the target size while creating ~33% overlap between groups.</p> <p>For a target of 1000 characters:</p> <ul> <li>Small chunk size: 333 characters</li> <li>Three small chunks: ~999 characters</li> </ul>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#stage-2-respect-semantic-boundaries","level":3,"title":"Stage 2: Respect Semantic Boundaries","text":"<p>Use separators in hierarchical order to find good split points:</p> <pre><code>1. \"\\n\\n\" - Paragraph boundaries (strongest semantic boundary)\n2. \"\\n\"   - Line breaks (sentences, list items)\n3. \" \"    - Word boundaries (preserves whole words)\n4. \"\"     - Character boundaries (last resort)\n</code></pre> <p>Try each separator recursively until chunks fit the size constraint.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#stage-3-merge-with-overlap","level":3,"title":"Stage 3: Merge with Overlap","text":"<p>Group every 3 consecutive small chunks together. This creates:</p> <ul> <li>Chunks near the target size</li> <li>Natural overlap where groups share chunks</li> </ul>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#detailed-example","level":2,"title":"Detailed Example","text":"<p>Let's trace through a concrete example to see exactly how this works.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#input","level":3,"title":"Input","text":"<pre><code>Document: \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" (26 chars)\nTarget chunk size: 9 characters\n</code></pre>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#step-1-calculate-small-chunk-size","level":3,"title":"Step 1: Calculate Small Chunk Size","text":"<pre><code>small_chunk_size = target_size / 3 = 9 / 3 = 3 characters\n</code></pre>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#step-2-split-into-small-chunks","level":3,"title":"Step 2: Split into Small Chunks","text":"<p>Split document every 3 characters:</p> <pre><code>[ABC] [DEF] [GHI] [JKL] [MNO] [PQR] [STU] [VWX] [YZ]\n  0     1     2     3     4     5     6     7    8\n</code></pre> <p>We have 9 small chunks.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#step-3-group-by-3-with-sliding-window","level":3,"title":"Step 3: Group by 3 with Sliding Window","text":"<p>Group consecutive chunks with a sliding window:</p> <pre><code>Chunk 0: [ABC DEF GHI] = \"ABCDEFGHI\" (chars 0-9)\nChunk 1: [DEF GHI JKL] = \"DEFGHIJKL\" (chars 3-12)\nChunk 2: [GHI JKL MNO] = \"GHIJKLMNO\" (chars 6-15)\nChunk 3: [JKL MNO PQR] = \"JKLMNOPQR\" (chars 9-18)\nChunk 4: [MNO PQR STU] = \"MNOPQRSTU\" (chars 12-21)\nChunk 5: [PQR STU VWX] = \"PQRSTUVWX\" (chars 15-24)\nChunk 6: [STU VWX YZ]  = \"STUVWXYZ\"  (chars 18-26)\n</code></pre>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#analyzing-the-overlap","level":3,"title":"Analyzing the Overlap","text":"<p>Each chunk shares content with its neighbors:</p> <pre><code>Chunk 0: ABC DEF GHI\nChunk 1:     DEF GHI JKL    (overlaps DEF GHI with chunk 0)\nChunk 2:         GHI JKL MNO (overlaps GHI JKL with chunk 1)\n</code></pre> <p>Overlap percentage: 6 chars overlap / 9 chars total = 66% overlap between adjacent chunks.</p> <p>Overall, each chunk shares ~33% with the previous and ~33% with the next chunk.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#real-text-example","level":2,"title":"Real Text Example","text":"<p>Let's see how this works with actual text:</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#input_1","level":3,"title":"Input","text":"<pre><code>Introduction to Machine Learning\n\nMachine learning is a field of artificial intelligence. It enables\nsystems to learn from data without explicit programming.\n\nDeep learning is a subset of machine learning. It uses neural networks\nwith many layers to model complex patterns.\n\nApplications include image recognition, natural language processing,\nand autonomous vehicles.\n</code></pre> <p>Target: 150 characters per chunk</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#step-1-try-nn-separator","level":3,"title":"Step 1: Try <code>\\n\\n</code> Separator","text":"<p>Split on paragraph breaks:</p> <pre><code>Chunk A: \"Introduction to Machine Learning\" (33 chars)\nChunk B: \"Machine learning is a field...\" (118 chars)\nChunk C: \"Deep learning is a subset...\" (124 chars)\nChunk D: \"Applications include...\" (89 chars)\n</code></pre> <p>Small chunk size = 150 / 3 = 50 chars</p> <p>All chunks fit the 50 char target, so we use this split.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#step-2-group-by-3","level":3,"title":"Step 2: Group by 3","text":"<pre><code>Final Chunk 0: [A + B + C]\n\"Introduction to Machine Learning\n\nMachine learning is a field of artificial intelligence. It enables\nsystems to learn from data without explicit programming.\n\nDeep learning is a subset of machine learning.\"\n(~275 chars, includes 3 paragraphs)\n\nFinal Chunk 1: [B + C + D]\n\"Machine learning is a field of artificial intelligence. It enables\nsystems to learn from data without explicit programming.\n\nDeep learning is a subset of machine learning. It uses neural networks\nwith many layers to model complex patterns.\n\nApplications include image recognition...\"\n(~331 chars, overlaps with Chunk 0)\n</code></pre> <p>Notice how paragraph B and C appear in both chunks, creating context overlap.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#the-recursive-part","level":2,"title":"The Recursive Part","text":"<p>What makes this \"recursive\"? The algorithm recursively tries separators until it finds one that produces small enough chunks.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#pseudocode","level":3,"title":"Pseudocode","text":"<pre><code>def recursive_split(text, target_size, separators):\n    if len(text) &lt;= target_size:\n        return [text]  # Text already small enough\n\n    if not separators:\n        # No more separators, split by character\n        return [text[i:i+target_size] for i in range(0, len(text), target_size)]\n\n    # Try current separator\n    sep = separators[0]\n    parts = text.split(sep)\n\n    # Check if all parts are small enough\n    if all(len(part) &lt;= target_size for part in parts):\n        return parts\n\n    # Some parts too big, recurse with next separator\n    return recursive_split(text, target_size, separators[1:])\n</code></pre>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#example-of-recursion-in-action","level":3,"title":"Example of Recursion in Action","text":"<p>Consider a long paragraph with no <code>\\n\\n</code> breaks:</p> <pre><code>\"This is a very long paragraph with no paragraph breaks. It contains many\nsentences that need to be split somehow to fit the target size. The recursive\nalgorithm will try each separator in turn.\"\n</code></pre> <ol> <li>Try <code>\\n\\n</code>: No splits (paragraph has no <code>\\n\\n</code>)</li> <li>Try <code>\\n</code>: One split (at the line break)</li> <li>Check: Are both parts &lt; target? If no...</li> <li>Try ``: Multiple splits at spaces</li> <li>Check: Are all parts &lt; target? If yes, use these splits</li> </ol> <p>The algorithm automatically falls back to finer-grained separators as needed.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#why-this-algorithm-is-effective","level":2,"title":"Why This Algorithm Is Effective","text":"","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#semantic-boundary-respect","level":3,"title":"Semantic Boundary Respect","text":"<p>By trying <code>\\n\\n</code> first, the algorithm keeps paragraphs together when possible. This preserves:</p> <ul> <li>Complete thoughts</li> <li>Thematic coherence</li> <li>Logical flow</li> </ul> <p>Only when paragraphs are too large does it split at finer boundaries.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#consistent-chunk-sizes","level":3,"title":"Consistent Chunk Sizes","text":"<p>Unlike pure semantic splitting (e.g., split on paragraphs), this algorithm ensures:</p> <ul> <li>Chunks fit embedding model context windows</li> <li>Relatively uniform chunk sizes for efficient storage</li> <li>No tiny chunks that waste database entries</li> </ul>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#overlap-for-context","level":3,"title":"Overlap for Context","text":"<p>The sliding window grouping creates natural overlap:</p> <pre><code>Chunk 0: ====AAAA====BBBB====CCCC====\nChunk 1:          ====BBBB====CCCC====DDDD====\nChunk 2:                   ====CCCC====DDDD====EEEE====\n</code></pre> <p>This overlap is critical for retrieval:</p> <p>Without Overlap Query: \"How does CCCC relate to BBBB?\"</p> <ul> <li>Might miss the connection if they're in different chunks</li> </ul> <p>With Overlap Query: \"How does CCCC relate to BBBB?\"</p> <ul> <li>Chunk 0 contains both BBBB and CCCC</li> <li>Chunk 1 also contains both</li> <li>Higher chance of retrieval</li> </ul>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#computational-efficiency","level":3,"title":"Computational Efficiency","text":"<p>The algorithm is efficient:</p> <ul> <li>Single pass to create small chunks</li> <li>Simple grouping operation to create final chunks</li> <li>No need to compute embeddings or semantic similarity</li> <li>Scales linearly with text length</li> </ul>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#configuration-trade-offs","level":2,"title":"Configuration Trade-offs","text":"<p>The algorithm has fixed parameters in rs_document. Let's understand why these choices were made:</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#why-33-overlap","level":3,"title":"Why 33% Overlap?","text":"<pre><code>10% overlap: =====AAAAA====B=====\n             =====B====CCCCC=====\n             (Minimal context sharing)\n\n33% overlap: =====AAAA====BBBB====CCCC=====\n             ========BBBB====CCCC====DDDD====\n             (Good context continuity)\n\n50% overlap: =====AAA====BBB====CCC=====\n             ====AAA====BBB====CCC====DDD====\n             (Excessive redundancy)\n</code></pre> <p>33% provides strong context continuity without excessive storage cost.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#why-fixed-separators","level":3,"title":"Why Fixed Separators?","text":"<p>The hierarchy <code>[\"\\n\\n\", \"\\n\", \" \", \"\"]</code> works universally:</p> <ul> <li>Prose: Respects paragraphs, sentences, words</li> <li>Code: Respects blank lines, line breaks, tokens</li> <li>Lists: Respects items, lines, words</li> <li>Tables: Respects rows, cells, words</li> </ul> <p>Custom separators would be needed for:</p> <ul> <li>Domain-specific formats (e.g., <code>\"---\"</code> for YAML)</li> <li>Non-English text (different sentence endings)</li> <li>Structured data with special delimiters</li> </ul> <p>For 95% of RAG use cases, the default hierarchy is optimal.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#common-questions","level":2,"title":"Common Questions","text":"","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#why-not-semantic-splitting","level":3,"title":"Why Not Semantic Splitting?","text":"<p>Semantic splitting uses embeddings to find natural break points. Why doesn't rs_document do this?</p> <p>Reasons:</p> <ol> <li>Performance: Computing embeddings is expensive</li> <li>Simplicity: No need for embedding models as dependencies</li> <li>Consistency: Deterministic results, not model-dependent</li> <li>Sufficiency: Character-based splitting works well for most RAG</li> </ol> <p>Semantic splitting is a valid approach but addresses a different point in the performance-accuracy trade-off curve.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#why-not-token-based-splitting","level":3,"title":"Why Not Token-Based Splitting?","text":"<p>Token-based splitting respects model tokenization. Why doesn't rs_document do this?</p> <p>Reasons:</p> <ol> <li>Model Agnostic: Works with any embedding model</li> <li>Performance: Tokenization is slower than character counting</li> <li>Simplicity: No need for tokenizer dependencies</li> <li>Sufficiency: Character approximation works well</li> </ol> <p>If you need exact token limits, post-process with a tokenizer:</p> <pre><code>from rs_document import Document\nfrom tiktoken import get_encoding\n\ndoc = Document(long_text, {})\nchunks = doc.recursive_character_splitter(3000)  # Approximate\n\n# Verify token counts\nenc = get_encoding(\"cl100k_base\")\nfor chunk in chunks:\n    tokens = enc.encode(chunk.page_content)\n    assert len(tokens) &lt;= 1024  # Actual limit\n</code></pre>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#what-about-edge-cases","level":3,"title":"What About Edge Cases?","text":"<p>Very Long Words/URLs: If a word is longer than the chunk size:</p> <pre><code>text = \"A\" * 10000  # 10000 char word\nchunks = doc.recursive_character_splitter(1000)\n# Will split at character boundary: \"AAA...AAA\" (1000 chars each)\n</code></pre> <p>No Natural Boundaries: For text with no separators:</p> <pre><code>text = \"A\" * 10000  # No spaces or newlines\n# Falls back to character splitting automatically\n</code></pre> <p>The algorithm gracefully handles these cases by falling back to character splitting.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#implementation-notes","level":2,"title":"Implementation Notes","text":"<p>The Rust implementation includes optimizations:</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#memory-efficiency","level":3,"title":"Memory Efficiency","text":"<p>Small chunks are stored as string slices (references) into the original text, avoiding copies:</p> <pre><code>// Conceptually\nlet original = String::from(\"long document\");\nlet small_chunks = vec![\n    &amp;original[0..100],    // Slice, not copy\n    &amp;original[100..200],  // Slice, not copy\n    // ...\n];\n</code></pre>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#parallel-processing","level":3,"title":"Parallel Processing","text":"<p>When processing multiple documents with <code>clean_and_split_docs()</code>, the splitting happens in parallel:</p> <pre><code>documents.par_iter()\n    .map(|doc| doc.recursive_character_splitter(size))\n    .flatten()\n    .collect()\n</code></pre> <p>Each document is split independently, utilizing all CPU cores.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#comparison-with-alternatives","level":2,"title":"Comparison with Alternatives","text":"","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#langchain-recursivecharactertextsplitter","level":3,"title":"LangChain RecursiveCharacterTextSplitter","text":"<p>Same:</p> <ul> <li>Recursive separator hierarchy</li> <li>Similar algorithm concept</li> </ul> <p>Different:</p> <ul> <li>LangChain: Configurable overlap (any percentage)</li> <li>rs_document: Fixed 33% overlap</li> <li>LangChain: Configurable separators</li> <li>rs_document: Fixed separators</li> <li>Performance: rs_document ~20x faster</li> </ul>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#simple-fixed-size-splitting_1","level":3,"title":"Simple Fixed-Size Splitting","text":"<p>Advantage of recursive splitting:</p> <ul> <li>Respects semantic boundaries</li> <li>Creates overlap</li> <li>More intelligent split points</li> </ul> <p>Disadvantage:</p> <ul> <li>More complex implementation</li> <li>Slightly slower than naive splitting</li> </ul> <p>The quality improvement far outweighs the complexity cost.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#best-practices","level":2,"title":"Best Practices","text":"","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#choosing-chunk-size","level":3,"title":"Choosing Chunk Size","text":"<p>Target chunk size depends on your embedding model:</p> <pre><code># Common embedding model context windows\nOpenAI text-embedding-ada-002: 8191 tokens (~32k chars)\nCohere embed-english-v3.0: 512 tokens (~2k chars)\nSentence Transformers: 512 tokens (~2k chars)\n</code></pre> <p>Rule of thumb: <code>chunk_size = 0.75 * context_window * 4</code> (chars per token)</p> <p>Example: For 512 token model: <code>chunk_size = 0.75 * 512 * 4 = 1536 chars</code></p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#testing-different-sizes","level":3,"title":"Testing Different Sizes","text":"<p>Experiment with different chunk sizes:</p> <pre><code>for size in [500, 1000, 1500, 2000]:\n    chunks = doc.recursive_character_splitter(size)\n    # Evaluate retrieval quality with each size\n</code></pre> <p>Larger chunks: More context, fewer chunks Smaller chunks: More precise, more chunks</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#handling-already-split-text","level":3,"title":"Handling Already-Split Text","text":"<p>If your text is already structured (e.g., JSON with sections):</p> <pre><code># Split sections separately, not the whole document\nfor section in document_sections:\n    doc = Document(section.text, section.metadata)\n    chunks = doc.recursive_character_splitter(1000)\n</code></pre> <p>This prevents mixing content from different sections.</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/recursive-splitting/#summary","level":2,"title":"Summary","text":"<p>The recursive character splitter is elegant in its simplicity:</p> <ol> <li>Split small, group with overlap</li> <li>Try semantic boundaries first, fall back as needed</li> <li>Produce chunks at target size with ~33% overlap</li> </ol> <p>This algorithm balances semantic coherence, practical chunk sizes, and overlap for retrieval quality—all while being fast and deterministic.</p> <p>Next: Text Cleaning - Understanding why and how documents are cleaned</p>","path":["Explanation","Recursive Splitting"],"tags":[]},{"location":"explanation/text-cleaning/","level":1,"title":"Text Cleaning","text":"<p>Text cleaning is the critical first step in document processing for RAG applications. Understanding why cleaning matters and what each operation does helps you use rs_document effectively.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#why-clean-documents","level":2,"title":"Why Clean Documents?","text":"<p>Documents from various sources contain artifacts that don't carry semantic meaning but hurt embedding quality and retrieval accuracy.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#the-problem-noise-in-embeddings","level":3,"title":"The Problem: Noise in Embeddings","text":"<p>Embedding models encode text into vectors representing semantic meaning. Artifacts create noise:</p> <pre><code># Original text from PDF\ntext1 = \"The ﬁrst step is to ﬁnd the optimal solution.\"\n\n# After cleaning\ntext2 = \"The first step is to find the optimal solution.\"\n\n# Without cleaning\nembedding1 = model.embed(text1)  # Encodes \"ﬁ\" ligature as unique token\n\n# With cleaning\nembedding2 = model.embed(text2)  # Encodes \"fi\" as standard token\n\n# Query: \"first step\"\nquery_embedding = model.embed(\"first step\")\n\n# Similarity (higher is better)\nsimilarity(query_embedding, embedding1) = 0.85  # Lower due to ligature\nsimilarity(query_embedding, embedding2) = 0.95  # Higher, clean match\n</code></pre> <p>The cleaner text produces embeddings that better match queries.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#the-problem-token-inefficiency","level":3,"title":"The Problem: Token Inefficiency","text":"<p>Artifacts waste token budget:</p> <pre><code># With extra whitespace and bullets\ntext = \"●  Key point:   This is   important.  \"\ntokens = 12\n\n# After cleaning\ntext = \"Key point: This is important.\"\ntokens = 6\n</code></pre> <p>Cleaning reduces token count by 50%, allowing more content in each chunk.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#sources-of-artifacts","level":3,"title":"Sources of Artifacts","text":"<p>Different document sources produce different artifacts:</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#pdf-extraction","level":4,"title":"PDF Extraction","text":"<p>PDFs are rendered for display, not text extraction. Common artifacts:</p> <p>Ligatures: Typographic ligatures become special characters</p> <ul> <li><code>fi</code> → <code>ﬁ</code> (single character)</li> <li><code>fl</code> → <code>ﬂ</code></li> <li><code>ae</code> → <code>æ</code></li> <li><code>oe</code> → <code>œ</code></li> </ul> <p>Bullets: Rendered symbols become unicode characters</p> <ul> <li>List bullets: <code>●</code>, <code>■</code>, <code>•</code>, <code>◆</code></li> <li>Checkboxes: <code>☐</code>, <code>☑</code>, <code>☒</code></li> </ul> <p>Extra Whitespace: Table formatting creates multiple spaces</p> <pre><code>Name          Age         City\nJohn          25          NYC\n</code></pre> <p>Broken Paragraphs: Multi-column layouts split paragraphs</p> <pre><code>This is a sentence that was\nincorrectly split across lines\nbecause of column detection.\n</code></pre>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#ocr-output","level":4,"title":"OCR Output","text":"<p>OCR (Optical Character Recognition) produces recognition errors:</p> <p>Non-ASCII Artifacts: Characters misrecognized as symbols</p> <ul> <li><code>l</code> (lowercase L) → <code>|</code> (pipe)</li> <li><code>0</code> (zero) → <code>O</code> (letter O)</li> <li>Accent marks: <code>e</code> → <code>é</code>, <code>n</code> → <code>ñ</code></li> </ul> <p>Whitespace Issues: Inconsistent spacing from layout detection</p> <pre><code>Word   spacing    is     irregular.\n</code></pre> <p>Broken Words: Characters split incorrectly</p> <pre><code>rec-\nognition\n</code></pre>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#web-scraping","level":4,"title":"Web Scraping","text":"<p>HTML conversion creates formatting artifacts:</p> <p>HTML Entities: Special characters encoded</p> <ul> <li><code>&amp;nbsp;</code> → extra spaces</li> <li><code>&amp;mdash;</code> → <code>—</code></li> <li><code>&amp;quot;</code> → <code>\"</code></li> </ul> <p>List Markers: HTML lists become text bullets</p> <pre><code>&lt;ul&gt;\n  &lt;li&gt;Item 1&lt;/li&gt;\n  &lt;li&gt;Item 2&lt;/li&gt;\n&lt;/ul&gt;\n\n→ \"• Item 1 • Item 2\"\n</code></pre> <p>Extra Whitespace: CSS spacing becomes text spaces</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#available-cleaners","level":2,"title":"Available Cleaners","text":"<p>rs_document provides five cleaners, each targeting specific artifacts:</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#clean_ligatures","level":3,"title":"<code>clean_ligatures()</code>","text":"<p>What it does: Converts typographic ligatures to component letters</p> <p>Transformations:</p> <pre><code>\"ﬁ\" → \"fi\"\n\"ﬂ\" → \"fl\"\n\"ﬀ\" → \"ff\"\n\"ﬃ\" → \"ffi\"\n\"ﬄ\" → \"ffl\"\n\"æ\" → \"ae\"\n\"œ\" → \"oe\"\n\"Æ\" → \"AE\"\n\"Œ\" → \"OE\"\n</code></pre> <p>Why it matters: Ligatures are common in professionally typeset PDFs (books, papers, reports). Without cleaning:</p> <pre><code># Query won't match because of ligature\nquery = \"first\"\ntext = \"The ﬁrst step\"  # Contains U+FB01 (ﬁ ligature)\n# String match: False\n# Similarity: Low\n</code></pre> <p>After cleaning:</p> <pre><code>text = \"The first step\"\n# String match: True\n# Similarity: High\n</code></pre> <p>Use case: Essential for any PDF content, especially academic papers, books, and professional reports.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#clean_bullets","level":3,"title":"<code>clean_bullets()</code>","text":"<p>What it does: Removes bullet point characters while preserving list structure</p> <p>Removes:</p> <pre><code>\"●\" → \"\"\n\"■\" → \"\"\n\"•\" → \"\"\n\"◆\" → \"\"\n\"▪\" → \"\"\n\"‣\" → \"\"\n</code></pre> <p>Example:</p> <pre><code># Before\n\"● First item\\n● Second item\\n● Third item\"\n\n# After\n\"First item\\nSecond item\\nThird item\"\n</code></pre> <p>Why it matters: Bullet characters don't carry semantic meaning and waste tokens. The list structure (line breaks) is preserved, which is what matters for understanding.</p> <p>Use case: Documents with lists (presentations, reports, documentation).</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#clean_extra_whitespace","level":3,"title":"<code>clean_extra_whitespace()</code>","text":"<p>What it does: Normalizes all whitespace</p> <p>Operations:</p> <ol> <li>Replace multiple spaces with single space: <code>\"a    b\"</code> → <code>\"a b\"</code></li> <li>Remove leading whitespace: <code>\"  text\"</code> → <code>\"text\"</code></li> <li>Remove trailing whitespace: <code>\"text  \"</code> → <code>\"text\"</code></li> <li>Normalize line endings: <code>\"\\r\\n\"</code> → <code>\"\\n\"</code></li> </ol> <p>Example:</p> <pre><code># Before\n\"  This   has    extra     spaces.  \\n   And   leading   spaces. \"\n\n# After\n\"This has extra spaces.\\nAnd leading spaces.\"\n</code></pre> <p>Why it matters:</p> <ul> <li>Reduces token count (multiple spaces are multiple tokens)</li> <li>Improves embedding consistency</li> <li>Removes visual formatting that doesn't carry meaning</li> </ul> <p>Use case: All documents, but especially PDFs with table formatting and OCR output.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#clean_non_ascii_chars","level":3,"title":"<code>clean_non_ascii_chars()</code>","text":"<p>What it does: Removes all characters outside the ASCII range (0-127)</p> <p>Removes:</p> <ul> <li>Accented characters: <code>é</code>, <code>ñ</code>, <code>ü</code></li> <li>Symbols: <code>™</code>, <code>©</code>, <code>°</code></li> <li>Emoji: 😀, 👍, ❤️</li> <li>Special punctuation: <code>—</code>, <code>…</code>, <code>'</code></li> </ul> <p>Example:</p> <pre><code># Before\n\"Café résumé — très bien! 😀\"\n\n# After\n\"Caf rsum  trs bien! \"\n</code></pre> <p>⚠️ Warning: This cleaner is aggressive and removes useful information in many cases.</p> <p>Use cases:</p> <ul> <li>Legacy systems requiring ASCII-only text</li> <li>Specific embedding models trained only on ASCII</li> <li>English-only content where non-ASCII is truly noise</li> </ul> <p>Avoid for:</p> <ul> <li>Multilingual content (removes non-English characters)</li> <li>Modern systems (most support Unicode)</li> <li>Content with intentional symbols or emoji</li> </ul> <p>Alternative: Only use if you have a specific requirement for ASCII-only text. Otherwise, skip this cleaner.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#group_broken_paragraphs","level":3,"title":"<code>group_broken_paragraphs()</code>","text":"<p>What it does: Rejoins paragraphs incorrectly split by PDF extraction</p> <p>Algorithm:</p> <ol> <li>Find lines that end without punctuation</li> <li>If next line doesn't start with capital letter</li> <li>Merge lines into single paragraph</li> </ol> <p>Example:</p> <pre><code># Before (broken paragraph from 2-column PDF)\n\"This is a sentence that was\\nsplit across lines because\\nof the column layout.\"\n\n# After\n\"This is a sentence that was split across lines because of the column layout.\"\n</code></pre> <p>Why it matters: Broken paragraphs create artificial semantic boundaries. Retrieval systems might treat them as separate thoughts.</p> <p>Use case: PDFs with multi-column layouts, scanned documents with OCR.</p> <p>Note: This cleaner is conservative—it only merges when confident the split was erroneous. It won't merge legitimate line breaks.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#the-clean-method","level":2,"title":"The <code>.clean()</code> Method","text":"<p>The <code>.clean()</code> method runs all cleaners in a specific order:</p> <pre><code>doc = Document(text, metadata)\ndoc.clean()\n# Equivalent to:\n# doc.clean_extra_whitespace()\n# doc.clean_ligatures()\n# doc.clean_bullets()\n# doc.clean_non_ascii_chars()\n# doc.group_broken_paragraphs()\n</code></pre>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#why-this-order","level":3,"title":"Why This Order?","text":"<p>The sequence matters for best results:</p> <p>1. <code>clean_extra_whitespace()</code> first</p> <ul> <li>Normalizes input for other cleaners</li> <li>Ensures consistent spacing for pattern matching</li> <li>Reduces noise before other operations</li> </ul> <p>2. <code>clean_ligatures()</code> second</p> <ul> <li>Converts to standard ASCII letters</li> <li>Ensures subsequent cleaners work with normalized text</li> </ul> <p>3. <code>clean_bullets()</code> third</p> <ul> <li>Removes symbols after ligatures are normalized</li> <li>Operates on clean whitespace</li> </ul> <p>4. <code>clean_non_ascii_chars()</code> fourth</p> <ul> <li>Removes remaining non-ASCII after ligatures converted</li> <li>Operates on text with normalized spacing and bullets removed</li> </ul> <p>5. <code>group_broken_paragraphs()</code> last</p> <ul> <li>Works with fully cleaned text</li> <li>Merges paragraphs after all character-level cleaning</li> <li>Benefits from normalized whitespace</li> </ul> <p>Running in different order could miss artifacts or make incorrect decisions.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#selective-cleaning","level":2,"title":"Selective Cleaning","text":"<p>You can run cleaners individually if you don't need all of them:</p> <pre><code>from rs_document import Document\n\ndoc = Document(text, metadata)\n\n# Only clean whitespace and ligatures\ndoc.clean_extra_whitespace()\ndoc.clean_ligatures()\n\n# Skip bullets, non-ASCII, and paragraph grouping\n</code></pre> <p>Common combinations:</p> <p>Minimal cleaning (fastest, least aggressive):</p> <pre><code>doc.clean_extra_whitespace()\ndoc.clean_ligatures()\n</code></pre> <p>Standard cleaning (good for most PDFs):</p> <pre><code>doc.clean_extra_whitespace()\ndoc.clean_ligatures()\ndoc.clean_bullets()\ndoc.group_broken_paragraphs()\n# Skip clean_non_ascii_chars()\n</code></pre> <p>Aggressive cleaning (ASCII-only systems):</p> <pre><code>doc.clean()  # All cleaners\n</code></pre>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#performance-characteristics","level":2,"title":"Performance Characteristics","text":"<p>Cleaning operations are fast in rs_document:</p> Operation Time (single doc) Speedup vs Python <code>clean_extra_whitespace()</code> &lt; 0.1ms 15-20x <code>clean_ligatures()</code> &lt; 0.1ms 25-30x <code>clean_bullets()</code> &lt; 0.1ms 20-25x <code>clean_non_ascii_chars()</code> &lt; 0.1ms 30-40x <code>group_broken_paragraphs()</code> &lt; 0.5ms 50-75x <code>clean()</code> (all) &lt; 1ms 20-25x <p>For batch processing:</p> <pre><code># 10,000 documents\ndocs = [Document(text, {}) for text in texts]\n\n# Clean all\nfor doc in docs:\n    doc.clean()\n\n# Total time: ~10 seconds\n# Python equivalent: ~4 minutes\n</code></pre>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#best-practices","level":2,"title":"Best Practices","text":"","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#always-clean-before-splitting","level":3,"title":"Always Clean Before Splitting","text":"<p>Clean first, then split:</p> <pre><code># Correct\ndoc.clean()\nchunks = doc.recursive_character_splitter(1000)\n\n# Wrong (splits uncleaned text)\nchunks = doc.recursive_character_splitter(1000)\nfor chunk in chunks:\n    chunk.clean()  # Cleaning each chunk separately\n</code></pre> <p>Cleaning before splitting:</p> <ul> <li>More efficient (clean once vs clean N chunks)</li> <li>Better chunk boundaries (splitting operates on clean text)</li> <li>More consistent results</li> </ul>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#use-clean_and_split_docs-for-batches","level":3,"title":"Use <code>clean_and_split_docs()</code> for Batches","text":"<p>For multiple documents, use the batch function:</p> <pre><code>from rs_document import clean_and_split_docs\n\nchunks = clean_and_split_docs(docs, chunk_size=1000)\n</code></pre> <p>This:</p> <ul> <li>Cleans and splits in one pass</li> <li>Processes documents in parallel</li> <li>Returns all chunks from all documents</li> </ul>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#consider-your-content-type","level":3,"title":"Consider Your Content Type","text":"<p>Adjust cleaning based on source:</p> <p>PDFs from books/papers:</p> <pre><code>doc.clean_extra_whitespace()\ndoc.clean_ligatures()\ndoc.group_broken_paragraphs()\n</code></pre> <p>OCR output:</p> <pre><code>doc.clean()  # All cleaners, OCR is noisy\n</code></pre> <p>Web content:</p> <pre><code>doc.clean_extra_whitespace()\ndoc.clean_bullets()\n# Skip ligatures (rare in HTML) and non-ASCII (preserve content)\n</code></pre> <p>Clean text (already processed):</p> <pre><code># Maybe just whitespace normalization\ndoc.clean_extra_whitespace()\n</code></pre>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#testing-cleaning-impact","level":3,"title":"Testing Cleaning Impact","text":"<p>Evaluate cleaning on sample documents:</p> <pre><code>from rs_document import Document\n\n# Before cleaning\ndoc1 = Document(original_text, {})\nembedding1 = embed_model.embed(doc1.page_content)\n\n# After cleaning\ndoc2 = Document(original_text, {})\ndoc2.clean()\nembedding2 = embed_model.embed(doc2.page_content)\n\n# Compare\nprint(f\"Original: {len(doc1.page_content)} chars\")\nprint(f\"Cleaned: {len(doc2.page_content)} chars\")\nprint(f\"Reduction: {len(doc1.page_content) - len(doc2.page_content)} chars\")\n\n# Test retrieval\nquery = \"your test query\"\nsim1 = similarity(embed_model.embed(query), embedding1)\nsim2 = similarity(embed_model.embed(query), embedding2)\nprint(f\"Similarity improvement: {sim2 - sim1:.3f}\")\n</code></pre>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#limitations","level":2,"title":"Limitations","text":"<p>Understanding what cleaners don't do:</p> <p>No Spelling Correction: OCR errors like \"recieve\" → \"receive\" aren't fixed</p> <p>No Grammar Fix: Broken sentences aren't reconstructed</p> <p>No Language Translation: Non-English text isn't translated</p> <p>No Semantic Cleaning: Meaningless content (lorem ipsum) isn't detected</p> <p>No HTML Removal: HTML tags aren't removed (use an HTML parser first)</p> <p>Cleaners focus on formatting artifacts, not content quality.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#comparison-with-alternatives","level":2,"title":"Comparison with Alternatives","text":"","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#vs-unstructuredio","level":3,"title":"vs Unstructured.io","text":"<p>Similarities:</p> <ul> <li>rs_document implements same core cleaners</li> <li>Logic matches Unstructured.io's behavior</li> </ul> <p>Differences:</p> <ul> <li>rs_document: Faster (15-75x per cleaner)</li> <li>Unstructured.io: More cleaners available (dashes, ordered bullets, etc.)</li> <li>Unstructured.io: More configuration options</li> </ul> <p>When to use each:</p> <ul> <li>rs_document: Speed matters, core cleaners sufficient</li> <li>Unstructured.io: Need specialized cleaners or fine-grained control</li> </ul>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#vs-langchain","level":3,"title":"vs LangChain","text":"<p>LangChain doesn't provide comprehensive text cleaners. Use rs_document for cleaning, LangChain for other pipeline steps.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#vs-custom-regex","level":3,"title":"vs Custom Regex","text":"<p>Writing custom regex cleaners:</p> <p>Advantages of custom:</p> <ul> <li>Tailored to your specific artifacts</li> <li>Full control over behavior</li> </ul> <p>Advantages of rs_document:</p> <ul> <li>Pre-tested, reliable implementations</li> <li>Significantly faster</li> <li>Less code to maintain</li> </ul> <p>Use rs_document cleaners as a baseline, add custom cleaning only for domain-specific artifacts.</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/text-cleaning/#summary","level":2,"title":"Summary","text":"<p>Text cleaning is essential for high-quality RAG:</p> <ol> <li>Why: Removes artifacts that hurt embedding quality and waste tokens</li> <li>What: Five cleaners target different artifact types</li> <li>How: Run <code>.clean()</code> or selective cleaners based on content type</li> <li>When: Always clean before splitting for best results</li> </ol> <p>Proper cleaning improves retrieval accuracy and reduces token costs—making it a high-impact, low-effort optimization.</p> <p>Next: Performance - Understanding what makes rs_document fast</p>","path":["Explanation","Text Cleaning"],"tags":[]},{"location":"explanation/why-rust/","level":1,"title":"Why Rust?","text":"<p>Understanding why rs_document exists requires understanding the performance challenges in RAG (Retrieval Augmented Generation) applications and why Rust provides an ideal solution.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#the-performance-problem","level":2,"title":"The Performance Problem","text":"<p>RAG applications need to process large document collections efficiently. A typical RAG pipeline involves three critical text processing steps:</p> <ol> <li>Cleaning - Removing artifacts from PDFs, OCR errors, formatting issues</li> <li>Splitting - Breaking documents into chunks that fit embedding models</li> <li>Scale - Processing thousands or millions of documents</li> </ol> <p>These operations seem simple, but at scale they become significant bottlenecks.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#real-world-scenarios","level":3,"title":"Real-World Scenarios","text":"<p>Consider these common situations:</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#initial-knowledge-base-creation","level":3,"title":"Initial Knowledge Base Creation","text":"<ul> <li>You have 100,000 PDF documents to process</li> <li>Each needs cleaning (5 operations) and splitting (1 operation)</li> <li>Pure Python: ~3 hours of processing time</li> <li>rs_document: ~8 minutes of processing time</li> </ul>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#experimentation-with-chunk-sizes","level":3,"title":"Experimentation with Chunk Sizes","text":"<ul> <li>Testing different chunk sizes (500, 1000, 1500 characters) for retrieval quality</li> <li>Each experiment requires reprocessing the entire corpus</li> <li>Python: 3 hours × 3 experiments = 9 hours</li> <li>rs_document: 8 minutes × 3 experiments = 24 minutes</li> </ul>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#real-time-document-ingestion","level":3,"title":"Real-Time Document Ingestion","text":"<ul> <li>New documents arrive continuously and need immediate processing</li> <li>Python: Can process ~150 documents/second on 8 cores</li> <li>rs_document: Can process ~23,000 documents/second on 8 cores</li> </ul> <p>The difference between minutes and hours fundamentally changes what's practical to do.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#why-python-is-slow-for-text-processing","level":2,"title":"Why Python Is Slow for Text Processing","text":"<p>Pure Python implementations (like LangChain's text splitters and Unstructured.io's cleaners) work well for small datasets but become bottlenecks at scale. This isn't a criticism of those tools—Python has inherent limitations for text processing:</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#interpreted-execution-overhead","level":3,"title":"Interpreted Execution Overhead","text":"<p>Python code is interpreted at runtime rather than compiled to machine code. Every operation involves:</p> <ul> <li>Looking up methods in object dictionaries</li> <li>Type checking at runtime</li> <li>Reference counting for memory management</li> <li>Frame creation for function calls</li> </ul> <p>For text processing with millions of operations, this overhead accumulates significantly.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#string-immutability","level":3,"title":"String Immutability","text":"<p>Python strings are immutable. Every modification creates a new string:</p> <pre><code>text = \"hello world\"\ntext = text.replace(\"world\", \"python\")  # Creates entirely new string\n</code></pre> <p>For cleaning operations that make multiple passes over text, this means:</p> <ul> <li>Constant memory allocation</li> <li>Copying entire strings repeatedly</li> <li>Memory fragmentation</li> <li>Garbage collection pressure</li> </ul> <p>A document going through 5 cleaning operations creates 5 complete copies in memory.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#global-interpreter-lock-gil","level":3,"title":"Global Interpreter Lock (GIL)","text":"<p>The GIL prevents true parallelism in Python:</p> <ul> <li>Only one thread executes Python bytecode at a time</li> <li>Multi-core CPUs can't be fully utilized for Python-level operations</li> <li>Threading helps with I/O, but not with CPU-bound text processing</li> </ul> <p>Processing 10,000 documents on an 8-core machine uses only 1 core in pure Python (without multiprocessing overhead).</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#regex-performance","level":3,"title":"Regex Performance","text":"<p>Python's <code>re</code> module, while good, can't match the performance of compiled regex engines:</p> <ul> <li>No Just-In-Time (JIT) compilation of patterns</li> <li>Less aggressive optimization</li> <li>Overhead of Python-C boundary for each match</li> </ul> <p>Text cleaning heavily relies on regex for pattern matching and replacement.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#why-rust-is-the-solution","level":2,"title":"Why Rust Is the Solution","text":"<p>Rust provides the perfect combination of performance and safety for this problem domain:</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#compiled-native-code","level":3,"title":"Compiled Native Code","text":"<p>Rust compiles to native machine code with aggressive optimizations:</p> <pre><code>// This gets compiled to direct CPU instructions\npub fn clean_text(text: &amp;mut String) {\n    text.retain(|c| c.is_ascii());\n}\n</code></pre> <ul> <li>No interpretation overhead</li> <li>CPU can execute directly</li> <li>Compiler optimizations (inlining, loop unrolling, etc.)</li> <li>SIMD (Single Instruction Multiple Data) instructions automatically applied</li> </ul> <p>The same operation in Python involves multiple layers of abstraction before reaching the CPU.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#efficient-string-handling","level":3,"title":"Efficient String Handling","text":"<p>Rust's <code>String</code> type allows safe in-place modification:</p> <pre><code>let mut text = String::from(\"hello world\");\ntext.replace_range(6..11, \"rust\");  // Modifies in place\n</code></pre> <p>This means:</p> <ul> <li>One allocation for the lifetime of cleaning operations</li> <li>No intermediate string copies</li> <li>Direct memory manipulation</li> <li>Minimal garbage collection</li> </ul> <p>A document going through 5 cleaning operations uses the same memory buffer throughout.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#true-parallelism-with-rayon","level":3,"title":"True Parallelism with Rayon","text":"<p>Rust has no GIL. Rayon provides data parallelism that actually uses all cores:</p> <pre><code>documents.par_iter()  // Parallel iterator\n    .map(|doc| {\n        doc.clean();\n        doc.split(chunk_size)\n    })\n    .flatten()\n    .collect()\n</code></pre> <p>This code:</p> <ul> <li>Automatically splits work across available CPU cores</li> <li>Processes multiple documents simultaneously</li> <li>Scales linearly with core count</li> <li>Has minimal synchronization overhead</li> </ul> <p>On an 8-core machine, you get close to 8x throughput for document processing.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#high-performance-regex","level":3,"title":"High-Performance Regex","text":"<p>Rust's <code>regex</code> crate provides one of the fastest regex engines available:</p> <ul> <li>DFA (Deterministic Finite Automaton) compilation</li> <li>Literal optimizations for fast prefix/suffix matching</li> <li>SIMD acceleration for character classes</li> <li>Zero-copy operations where possible</li> </ul> <p>Pattern matching and replacement operations are significantly faster than Python's <code>re</code> module.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#the-pyo3-bridge","level":2,"title":"The PyO3 Bridge","text":"<p>rs_document uses PyO3 to make Rust code callable from Python:</p> <pre><code>#[pyclass]\npub struct Document {\n    pub page_content: String,\n    pub metadata: HashMap&lt;String, String&gt;,\n}\n\n#[pymethods]\nimpl Document {\n    #[new]\n    pub fn new(page_content: String, metadata: HashMap&lt;String, String&gt;) -&gt; Self {\n        Self { page_content, metadata }\n    }\n}\n</code></pre> <p>PyO3 generates the Python extension module that:</p> <ul> <li>Exposes Rust structs as Python classes</li> <li>Converts between Python and Rust types</li> <li>Handles reference counting and memory management</li> <li>Minimizes copying across the language boundary</li> </ul> <p>You get Rust performance with Python convenience—no need to rewrite your entire application.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#measured-performance-gains","level":2,"title":"Measured Performance Gains","text":"<p>Typical performance improvements on modern hardware (8-core CPU):</p> Operation Python rs_document Speedup Single document cleaning ~20ms &lt;1ms 20-25x Single document splitting ~100ms &lt;5ms 20-25x Batch processing (1000 docs) ~20s ~0.9s 22x Batch processing (1M docs) ~6 hours ~15 min 24x <p>The speedup is consistent across different dataset sizes—no performance degradation with large batches.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#when-performance-matters","level":2,"title":"When Performance Matters","text":"<p>Not every use case needs this level of performance optimization:</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#small-workloads-100-documents","level":3,"title":"Small Workloads (&lt; 100 documents)","text":"<p>For small datasets, the speedup might not matter:</p> <ul> <li>Python: 2 seconds</li> <li>rs_document: 0.1 seconds</li> <li>Saved: 1.9 seconds</li> </ul> <p>This difference is negligible in most workflows.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#large-workloads-10000-documents","level":3,"title":"Large Workloads (&gt; 10,000 documents)","text":"<p>For large datasets, the speedup is transformative:</p> <ul> <li>Python: 20 minutes</li> <li>rs_document: 50 seconds</li> <li>Saved: 19+ minutes</li> </ul> <p>This changes what's practical:</p> <ul> <li>Enables rapid experimentation</li> <li>Makes real-time processing feasible</li> <li>Reduces infrastructure costs</li> </ul>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#critical-factors","level":3,"title":"Critical Factors","text":"<p>Performance matters when you have:</p> <ol> <li>Large document collections (thousands to millions)</li> <li>Frequent reprocessing (experimentation, updates)</li> <li>Real-time requirements (immediate ingestion)</li> <li>Cost constraints (reducing compute time)</li> </ol> <p>If any of these apply, rs_document's performance advantage is significant.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#design-trade-offs","level":2,"title":"Design Trade-offs","text":"<p>Achieving this performance required specific design choices:</p> <p>Opinionated Defaults - Fixed parameters enable aggressive optimization String-Only Metadata - Simple types avoid complex Python-Rust conversions Limited Customization - Specific behavior allows compiler optimizations Rust Dependency - Building from source requires Rust toolchain</p> <p>These trade-offs are detailed in Design Philosophy.</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"explanation/why-rust/#the-bottom-line","level":2,"title":"The Bottom Line","text":"<p>Python is excellent for many tasks, but CPU-bound text processing at scale isn't one of them. Rust's compiled execution, efficient memory handling, and true parallelism provide dramatic performance improvements while maintaining Python's ease of use through PyO3.</p> <p>rs_document doesn't replace your entire stack—it replaces the specific bottleneck of text cleaning and splitting, letting you keep Python for everything else.</p> <p>Next: Design Philosophy - Understanding the deliberate choices behind the API</p>","path":["Explanation","Why Rust?"],"tags":[]},{"location":"how-to/","level":1,"title":"How-To Guides Overview","text":"<p>This section provides focused, task-based guides for working with rs_document. Each guide covers a specific workflow or use case with practical examples.</p>","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/#getting-started","level":2,"title":"Getting Started","text":"<ul> <li>Loading Documents - Create documents from files and manage metadata</li> <li>Cleaning Tasks - Remove bullets, ligatures, and special characters</li> <li>Splitting Tasks - Split documents with different strategies</li> </ul>","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/#advanced-workflows","level":2,"title":"Advanced Workflows","text":"<ul> <li>Batch Operations - Process multiple documents efficiently</li> <li>Vector DB Preparation - Prepare documents for embedding and retrieval</li> <li>LangChain Integration - Use rs_document with LangChain</li> </ul>","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/#quick-examples","level":2,"title":"Quick Examples","text":"","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/#basic-document-processing","level":3,"title":"Basic Document Processing","text":"<pre><code>from rs_document import Document\n\n# Create, clean, and split a document\ndoc = Document(\n    page_content=\"Your text here...\",\n    metadata={\"source\": \"example.txt\"}\n)\n\ndoc.clean()\nchunks = doc.recursive_character_splitter(1000)\n</code></pre>","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/#batch-processing","level":3,"title":"Batch Processing","text":"<pre><code>from rs_document import clean_and_split_docs, Document\n\n# Process multiple documents at once\ndocuments = [...]  # Your documents\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre>","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/#common-use-cases","level":2,"title":"Common Use Cases","text":"","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/#rag-applications","level":3,"title":"RAG Applications","text":"<ol> <li>Load your documents</li> <li>Clean the text</li> <li>Split into chunks</li> <li>Prepare for vector DB</li> </ol>","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/#text-processing-pipeline","level":3,"title":"Text Processing Pipeline","text":"<ol> <li>Create documents with metadata</li> <li>Apply specific cleaners</li> <li>Split with context overlap</li> <li>Filter and organize chunks</li> </ol>","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/#need-more-help","level":2,"title":"Need More Help?","text":"<ul> <li>See the API Reference for detailed method documentation</li> <li>Check the Tutorial to learn the basics first</li> </ul>","path":["How-To Guides","How-To Guides Overview"],"tags":[]},{"location":"how-to/batch-operations/","level":1,"title":"Batch Operations","text":"<p>Learn how to process multiple documents efficiently using rs_document's batch processing capabilities.</p>","path":["How-To Guides","Batch Operations"],"tags":[]},{"location":"how-to/batch-operations/#process-multiple-documents-efficiently","level":2,"title":"Process Multiple Documents Efficiently","text":"<p>The <code>clean_and_split_docs()</code> function uses parallel processing:</p> <pre><code>from rs_document import clean_and_split_docs, Document\n\n# Create or load multiple documents\ndocuments = [\n    Document(page_content=f\"Document {i} content \" * 100, metadata={\"id\": str(i)})\n    for i in range(1000)\n)\n\n# Process all at once (uses parallel processing)\nall_chunks = clean_and_split_docs(documents, chunk_size=1000)\n\nprint(f\"Processed {len(documents)} docs into {len(all_chunks)} chunks\")\n</code></pre>","path":["How-To Guides","Batch Operations"],"tags":[]},{"location":"how-to/batch-operations/#filter-chunks-after-splitting","level":2,"title":"Filter Chunks After Splitting","text":"<pre><code>from rs_document import clean_and_split_docs, Document\n\ndocuments = [Document(page_content=\"content \" * 100, metadata={\"category\": \"tech\"})]\nchunks = clean_and_split_docs(documents, chunk_size=500)\n\n# Filter out chunks that are too small\nmin_size = 100\nfiltered_chunks = [c for c in chunks if len(c.page_content) &gt;= min_size]\n\nprint(f\"Kept {len(filtered_chunks)} of {len(chunks)} chunks\")\n</code></pre>","path":["How-To Guides","Batch Operations"],"tags":[]},{"location":"how-to/batch-operations/#track-which-document-each-chunk-came-from","level":2,"title":"Track Which Document Each Chunk Came From","text":"<pre><code>from rs_document import Document, clean_and_split_docs\nfrom collections import defaultdict\n\n# Add unique ID to each source document\ndocuments = []\nfor i in range(10):\n    doc = Document(\n        page_content=f\"Content of document {i} \" * 500,\n        metadata={\n            \"source_doc_id\": str(i),\n            \"filename\": f\"doc_{i}.txt\"\n        }\n    )\n    documents.append(doc)\n\n# Process\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\n# Group chunks by source document\nchunks_by_doc = defaultdict(list)\n\nfor chunk in chunks:\n    doc_id = chunk.metadata[\"source_doc_id\"]\n    chunks_by_doc[doc_id].append(chunk)\n\n# See how many chunks each document produced\nfor doc_id, doc_chunks in chunks_by_doc.items():\n    print(f\"Document {doc_id}: {len(doc_chunks)} chunks\")\n</code></pre>","path":["How-To Guides","Batch Operations"],"tags":[]},{"location":"how-to/batch-operations/#complete-rag-processing-pipeline","level":2,"title":"Complete RAG Processing Pipeline","text":"<pre><code>from pathlib import Path\nfrom rs_document import Document, clean_and_split_docs\n\ndef process_documents_for_rag(\n    directory: str,\n    chunk_size: int = 1000\n) -&gt; list[Document]:\n    \"\"\"Complete pipeline for processing documents.\"\"\"\n\n    # 1. Load documents\n    documents = []\n    for file_path in Path(directory).glob(\"**/*.txt\"):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n\n        doc = Document(\n            page_content=content,\n            metadata={\n                \"source\": file_path.name,\n                \"path\": str(file_path),\n                \"category\": file_path.parent.name\n            }\n        )\n        documents.append(doc)\n\n    # 2. Clean and split (parallel processing)\n    chunks = clean_and_split_docs(documents, chunk_size=chunk_size)\n\n    # 3. Filter out very small chunks\n    min_size = chunk_size // 4\n    filtered = [c for c in chunks if len(c.page_content) &gt;= min_size]\n\n    # 4. Add chunk metadata\n    for i, chunk in enumerate(filtered):\n        chunk.metadata[\"chunk_id\"] = str(i)\n\n    return filtered\n\n# Use it\nchunks = process_documents_for_rag(\"./documents\", chunk_size=1000)\nprint(f\"Ready for embedding: {len(chunks)} chunks\")\n</code></pre>","path":["How-To Guides","Batch Operations"],"tags":[]},{"location":"how-to/batch-operations/#benchmark-your-processing","level":2,"title":"Benchmark Your Processing","text":"<pre><code>import time\nfrom rs_document import clean_and_split_docs, Document\n\n# Create test documents\ndocuments = [\n    Document(page_content=\"content \" * 5000, metadata={\"id\": str(i)})\n    for i in range(1000)\n]\n\n# Time the processing\nstart = time.time()\nchunks = clean_and_split_docs(documents, chunk_size=1000)\nelapsed = time.time() - start\n\ndocs_per_second = len(documents) / elapsed\nprint(f\"Processed {len(documents)} documents in {elapsed:.2f}s\")\nprint(f\"Speed: {docs_per_second:.0f} documents/second\")\nprint(f\"Produced {len(chunks)} chunks\")\n</code></pre>","path":["How-To Guides","Batch Operations"],"tags":[]},{"location":"how-to/batch-operations/#process-by-category","level":2,"title":"Process by Category","text":"<pre><code>from rs_document import Document, clean_and_split_docs\n\n# Documents with categories\ndocuments = [\n    Document(page_content=\"tech content\", metadata={\"category\": \"tech\"}),\n    Document(page_content=\"business content\", metadata={\"category\": \"business\"}),\n    Document(page_content=\"more tech\", metadata={\"category\": \"tech\"}),\n]\n\n# Process all\nchunks = clean_and_split_docs(documents, chunk_size=500)\n\n# Group by category\ntech_chunks = [c for c in chunks if c.metadata[\"category\"] == \"tech\"]\nbusiness_chunks = [c for c in chunks if c.metadata[\"category\"] == \"business\"]\n\nprint(f\"Tech chunks: {len(tech_chunks)}\")\nprint(f\"Business chunks: {len(business_chunks)}\")\n</code></pre>","path":["How-To Guides","Batch Operations"],"tags":[]},{"location":"how-to/batch-operations/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>Prepare chunks for vector databases</li> <li>Integrate with LangChain</li> <li>See Loading Documents for input strategies</li> </ul>","path":["How-To Guides","Batch Operations"],"tags":[]},{"location":"how-to/cleaning-tasks/","level":1,"title":"Cleaning Tasks","text":"<p>Learn how to clean text in documents using various cleaning methods available in rs_document.</p>","path":["How-To Guides","Cleaning Tasks"],"tags":[]},{"location":"how-to/cleaning-tasks/#clean-all-issues-at-once","level":2,"title":"Clean All Issues at Once","text":"<p>The <code>clean()</code> method applies all cleaners in sequence:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"Smart quotes: \"hello\" and 'world'\\nEm dash: —\\nBullets: ● Item\",\n    metadata={}\n)\n\n# Clean everything\ndoc.clean()\nprint(doc.page_content)\n</code></pre>","path":["How-To Guides","Cleaning Tasks"],"tags":[]},{"location":"how-to/cleaning-tasks/#remove-specific-types-of-issues","level":2,"title":"Remove Specific Types of Issues","text":"<p>Apply individual cleaners for targeted cleaning:</p> <pre><code>from rs_document import Document\n\n# Document with various issues\ndoc = Document(\n    page_content=\"●  Item 1\\n● Item 2\\n\\næ special chars\",\n    metadata={}\n)\n\n# Remove just bullets\ndoc.clean_bullets()\n\n# Remove just ligatures\ndoc.clean_ligatures()\n\n# Chain multiple cleaners\ndoc.clean_extra_whitespace()\n</code></pre>","path":["How-To Guides","Cleaning Tasks"],"tags":[]},{"location":"how-to/cleaning-tasks/#available-cleaners","level":2,"title":"Available Cleaners","text":"<p>Each cleaner targets specific text issues:</p> <ul> <li><code>clean_bullets()</code> - Removes bullet points (●, •, ○, etc.)</li> <li><code>clean_ligatures()</code> - Converts ligatures (æ, œ) to ASCII equivalents</li> <li><code>clean_dashes()</code> - Normalizes em/en dashes to hyphens</li> <li><code>clean_extra_whitespace()</code> - Removes excess whitespace</li> <li><code>clean_headers()</code> - Cleans markdown-style headers</li> <li><code>clean_non_ascii()</code> - Removes non-ASCII characters</li> <li><code>clean()</code> - Applies all cleaners in order</li> </ul>","path":["How-To Guides","Cleaning Tasks"],"tags":[]},{"location":"how-to/cleaning-tasks/#clean-text-without-modifying-original","level":2,"title":"Clean Text Without Modifying Original","text":"<p>Create a copy if you need to preserve the original:</p> <pre><code>from rs_document import Document\n\n# Original document\noriginal_doc = Document(\n    page_content=\"Original text with ● bullets\",\n    metadata={\"id\": \"1\"}\n)\n\n# Create a copy\ncleaned_doc = Document(\n    page_content=original_doc.page_content,\n    metadata=original_doc.metadata.copy()\n)\n\n# Clean the copy\ncleaned_doc.clean()\n\n# Now you have both versions\nprint(\"Original:\", original_doc.page_content)\nprint(\"Cleaned:\", cleaned_doc.page_content)\n</code></pre>","path":["How-To Guides","Cleaning Tasks"],"tags":[]},{"location":"how-to/cleaning-tasks/#handle-documents-with-special-characters","level":2,"title":"Handle Documents with Special Characters","text":"<pre><code>from rs_document import Document\n\n# Document with unicode and special characters\ndoc = Document(\n    page_content=\"Smart quotes: \"hello\" and 'world'\\nEm dash: —\\nLigatures: æ œ\",\n    metadata={}\n)\n\n# Clean everything\ndoc.clean()\n\n# Result will have ASCII-safe versions\nprint(doc.page_content)\n</code></pre>","path":["How-To Guides","Cleaning Tasks"],"tags":[]},{"location":"how-to/cleaning-tasks/#chain-cleaning-operations","level":2,"title":"Chain Cleaning Operations","text":"<pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"●  Extra   spaces\\n\\n\\n● More bullets—dashes\",\n    metadata={}\n)\n\n# Apply cleaners in specific order\ndoc.clean_bullets()\ndoc.clean_dashes()\ndoc.clean_extra_whitespace()\n</code></pre>","path":["How-To Guides","Cleaning Tasks"],"tags":[]},{"location":"how-to/cleaning-tasks/#batch-cleaning","level":2,"title":"Batch Cleaning","text":"<p>For multiple documents, use <code>clean_and_split_docs()</code>:</p> <pre><code>from rs_document import clean_and_split_docs, Document\n\ndocuments = [\n    Document(page_content=\"● Bullet text\", metadata={\"id\": \"1\"}),\n    Document(page_content=\"Smart \"quotes\"\", metadata={\"id\": \"2\"})\n]\n\n# Clean and split in one operation\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre> <p>See Batch Operations for more batch processing patterns.</p>","path":["How-To Guides","Cleaning Tasks"],"tags":[]},{"location":"how-to/cleaning-tasks/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>Split cleaned documents into chunks</li> <li>Process multiple documents at once</li> <li>Prepare for vector databases</li> </ul>","path":["How-To Guides","Cleaning Tasks"],"tags":[]},{"location":"how-to/langchain-integration/","level":1,"title":"LangChain Integration","text":"<p>rs_document's <code>Document</code> class is compatible with LangChain's Document model. Learn how to integrate rs_document into your LangChain workflows.</p>","path":["How-To Guides","LangChain Integration"],"tags":[]},{"location":"how-to/langchain-integration/#convert-from-langchain-documents","level":2,"title":"Convert from LangChain Documents","text":"<pre><code>from langchain_core.documents import Document as LCDocument\nfrom rs_document import Document as RSDocument\n\n# LangChain document\nlc_doc = LCDocument(\n    page_content=\"Some text\",\n    metadata={\"source\": \"file.txt\"}\n)\n\n# Convert to rs_document (metadata must be strings)\nrs_doc = RSDocument(\n    page_content=lc_doc.page_content,\n    metadata={k: str(v) for k, v in lc_doc.metadata.items()}\n)\n\n# Process with rs_document\nrs_doc.clean()\nchunks = rs_doc.recursive_character_splitter(1000)\n</code></pre>","path":["How-To Guides","LangChain Integration"],"tags":[]},{"location":"how-to/langchain-integration/#use-with-langchain-document-loaders","level":2,"title":"Use with LangChain Document Loaders","text":"<pre><code>from langchain_community.document_loaders import TextLoader\nfrom rs_document import Document, clean_and_split_docs\n\n# Load with LangChain\nloader = TextLoader(\"document.txt\")\nlc_documents = loader.load()\n\n# Convert to rs_document\nrs_documents = [\n    Document(\n        page_content=doc.page_content,\n        metadata={k: str(v) for k, v in doc.metadata.items()}\n    )\n    for doc in lc_documents\n]\n\n# Process with rs_document's fast cleaning and splitting\nchunks = clean_and_split_docs(rs_documents, chunk_size=1000)\n</code></pre>","path":["How-To Guides","LangChain Integration"],"tags":[]},{"location":"how-to/langchain-integration/#convert-back-to-langchain-format","level":2,"title":"Convert Back to LangChain Format","text":"<pre><code>from rs_document import Document as RSDocument\nfrom langchain_core.documents import Document as LCDocument\n\n# Process with rs_document\nrs_doc = RSDocument(page_content=\"text\", metadata={\"source\": \"file.txt\"})\nrs_doc.clean()\nchunks = rs_doc.recursive_character_splitter(1000)\n\n# Convert back to LangChain\nlc_chunks = [\n    LCDocument(\n        page_content=chunk.page_content,\n        metadata=chunk.metadata\n    )\n    for chunk in chunks\n]\n</code></pre>","path":["How-To Guides","LangChain Integration"],"tags":[]},{"location":"how-to/langchain-integration/#use-with-directory-loaders","level":2,"title":"Use with Directory Loaders","text":"<pre><code>from langchain_community.document_loaders import DirectoryLoader\nfrom rs_document import Document, clean_and_split_docs\n\n# Load directory with LangChain\nloader = DirectoryLoader(\"./documents\", glob=\"**/*.txt\")\nlc_documents = loader.load()\n\n# Convert and process\nrs_documents = [\n    Document(\n        page_content=doc.page_content,\n        metadata={k: str(v) for k, v in doc.metadata.items()}\n    )\n    for doc in lc_documents\n]\n\nchunks = clean_and_split_docs(rs_documents, chunk_size=1000)\n</code></pre>","path":["How-To Guides","LangChain Integration"],"tags":[]},{"location":"how-to/langchain-integration/#integration-with-langchain-embeddings","level":2,"title":"Integration with LangChain Embeddings","text":"<pre><code>from langchain_community.document_loaders import TextLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom rs_document import Document, clean_and_split_docs\n\n# Load with LangChain\nloader = TextLoader(\"document.txt\")\nlc_documents = loader.load()\n\n# Convert and process with rs_document\nrs_documents = [\n    Document(\n        page_content=doc.page_content,\n        metadata={k: str(v) for k, v in doc.metadata.items()}\n    )\n    for doc in lc_documents\n]\n\nchunks = clean_and_split_docs(rs_documents, chunk_size=1000)\n\n# Use with LangChain embeddings\nembeddings = OpenAIEmbeddings()\ntexts = [chunk.page_content for chunk in chunks]\nvectors = embeddings.embed_documents(texts)\n</code></pre>","path":["How-To Guides","LangChain Integration"],"tags":[]},{"location":"how-to/langchain-integration/#complete-langchain-rs_document-pipeline","level":2,"title":"Complete LangChain + rs_document Pipeline","text":"<pre><code>from langchain_community.document_loaders import DirectoryLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom rs_document import Document, clean_and_split_docs\n\n# 1. Load with LangChain\nloader = DirectoryLoader(\"./documents\", glob=\"**/*.txt\")\nlc_documents = loader.load()\n\n# 2. Convert to rs_document\nrs_documents = [\n    Document(\n        page_content=doc.page_content,\n        metadata={k: str(v) for k, v in doc.metadata.items()}\n    )\n    for doc in lc_documents\n]\n\n# 3. Clean and split with rs_document (fast!)\nchunks = clean_and_split_docs(rs_documents, chunk_size=1000)\n\n# 4. Convert back for vector store\nfrom langchain_core.documents import Document as LCDocument\nlc_chunks = [\n    LCDocument(page_content=c.page_content, metadata=c.metadata)\n    for c in chunks\n]\n\n# 5. Create vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(lc_chunks, embeddings)\n</code></pre>","path":["How-To Guides","LangChain Integration"],"tags":[]},{"location":"how-to/langchain-integration/#why-use-rs_document-with-langchain","level":2,"title":"Why Use rs_document with LangChain?","text":"<ul> <li>Performance: rs_document uses Rust for faster text cleaning and splitting</li> <li>Parallel Processing: <code>clean_and_split_docs()</code> processes multiple documents in parallel</li> <li>Specialized Cleaners: More cleaning options than LangChain's default text splitters</li> <li>Compatible: Works seamlessly with LangChain's ecosystem</li> </ul>","path":["How-To Guides","LangChain Integration"],"tags":[]},{"location":"how-to/langchain-integration/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>See Batch Operations for performance optimization</li> <li>Check Vector DB Preparation for embedding workflows</li> <li>Review Splitting Tasks for chunking strategies</li> </ul>","path":["How-To Guides","LangChain Integration"],"tags":[]},{"location":"how-to/loading-documents/","level":1,"title":"Loading Documents","text":"<p>Learn how to create <code>Document</code> objects from various sources and manage their metadata effectively.</p>","path":["How-To Guides","Loading Documents"],"tags":[]},{"location":"how-to/loading-documents/#create-a-document-from-a-file","level":2,"title":"Create a Document from a File","text":"<pre><code>from rs_document import Document\n\n# Read file content\nwith open(\"document.txt\", \"r\", encoding=\"utf-8\") as f:\n    content = f.read()\n\n# Create document with metadata\ndoc = Document(\n    page_content=content,\n    metadata={\n        \"source\": \"document.txt\",\n        \"created_at\": \"2024-01-01\"\n    }\n)\n</code></pre>","path":["How-To Guides","Loading Documents"],"tags":[]},{"location":"how-to/loading-documents/#create-documents-from-multiple-files","level":2,"title":"Create Documents from Multiple Files","text":"<pre><code>from pathlib import Path\nfrom rs_document import Document\n\ndef load_documents(directory: str) -&gt; list[Document]:\n    \"\"\"Load all text files from a directory.\"\"\"\n    docs = []\n    for file_path in Path(directory).glob(\"*.txt\"):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n\n        doc = Document(\n            page_content=content,\n            metadata={\n                \"source\": file_path.name,\n                \"path\": str(file_path)\n            }\n        )\n        docs.append(doc)\n\n    return docs\n\n# Use it\ndocuments = load_documents(\"./my_documents\")\n</code></pre>","path":["How-To Guides","Loading Documents"],"tags":[]},{"location":"how-to/loading-documents/#load-documents-recursively","level":2,"title":"Load Documents Recursively","text":"<pre><code>from pathlib import Path\nfrom rs_document import Document\n\ndef load_documents_recursive(directory: str) -&gt; list[Document]:\n    \"\"\"Load all text files from directory and subdirectories.\"\"\"\n    docs = []\n    for file_path in Path(directory).glob(\"**/*.txt\"):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n\n        doc = Document(\n            page_content=content,\n            metadata={\n                \"source\": file_path.name,\n                \"path\": str(file_path),\n                \"category\": file_path.parent.name\n            }\n        )\n        docs.append(doc)\n\n    return docs\n</code></pre>","path":["How-To Guides","Loading Documents"],"tags":[]},{"location":"how-to/loading-documents/#preserve-metadata-through-processing","level":2,"title":"Preserve Metadata Through Processing","text":"<p>Metadata is automatically preserved through all cleaning and splitting operations:</p> <pre><code>from rs_document import Document\n\n# Create document with important metadata\ndoc = Document(\n    page_content=\"Some text content\",\n    metadata={\n        \"doc_id\": \"12345\",\n        \"author\": \"Jane Doe\",\n        \"category\": \"technical\"\n    }\n)\n\n# Metadata is preserved through cleaning\ndoc.clean()\nprint(doc.metadata)  # All metadata still there\n\n# And through splitting\nchunks = doc.recursive_character_splitter(1000)\nfor chunk in chunks:\n    print(chunk.metadata)  # Same metadata on every chunk\n</code></pre>","path":["How-To Guides","Loading Documents"],"tags":[]},{"location":"how-to/loading-documents/#validate-metadata-types","level":2,"title":"Validate Metadata Types","text":"<p>Metadata values must be strings:</p> <pre><code>from rs_document import Document\n\n# Convert to strings to ensure compatibility\ndoc = Document(\n    page_content=\"text\",\n    metadata={\n        \"id\": str(123),\n        \"page\": str(5),\n        \"score\": str(0.95)\n    }\n)\n</code></pre>","path":["How-To Guides","Loading Documents"],"tags":[]},{"location":"how-to/loading-documents/#handle-empty-documents","level":2,"title":"Handle Empty Documents","text":"<pre><code>from rs_document import Document\n\n# Empty document\ndoc = Document(page_content=\"\", metadata={\"id\": \"1\"})\n\n# Cleaning works fine\ndoc.clean()\n\n# Splitting returns empty list\nchunks = doc.recursive_character_splitter(1000)\nassert len(chunks) == 0\n</code></pre>","path":["How-To Guides","Loading Documents"],"tags":[]},{"location":"how-to/loading-documents/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>Clean your documents to remove unwanted characters</li> <li>Split documents into manageable chunks</li> <li>Process multiple documents efficiently</li> </ul>","path":["How-To Guides","Loading Documents"],"tags":[]},{"location":"how-to/splitting-tasks/","level":1,"title":"Splitting Tasks","text":"<p>Learn how to split documents into chunks using rs_document's splitting methods.</p>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#split-with-specific-chunk-size","level":2,"title":"Split with Specific Chunk Size","text":"<p>The <code>recursive_character_splitter()</code> method is the primary way to split documents:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"Your long document text here...\",\n    metadata={\"source\": \"doc.txt\"}\n)\n\n# Split into 500-character chunks\nsmall_chunks = doc.recursive_character_splitter(500)\n\n# Split into 2000-character chunks\nlarge_chunks = doc.recursive_character_splitter(2000)\n\nprint(f\"500-char chunks: {len(small_chunks)}\")\nprint(f\"2000-char chunks: {len(large_chunks)}\")\n</code></pre>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#split-and-maintain-context","level":2,"title":"Split and Maintain Context","text":"<p>The recursive character splitter automatically creates ~33% overlap between chunks:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"Paragraph one.\\n\\nParagraph two.\\n\\nParagraph three.\",\n    metadata={}\n)\n\nchunks = doc.recursive_character_splitter(50)\n\n# Adjacent chunks will have overlapping content\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i}: {chunk.page_content}\")\n</code></pre>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#split-on-exact-boundaries","level":2,"title":"Split on Exact Boundaries","text":"<p>Use <code>split_on_num_characters()</code> for exact splits without overlap:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"ABCDEFGHIJKLMNOP\",\n    metadata={}\n)\n\n# Split exactly every 5 characters (no overlap)\nchunks = doc.split_on_num_characters(5)\n\n# Result: [\"ABCDE\", \"FGHIJ\", \"KLMNO\", \"P\"]\n</code></pre>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#how-separator-selection-works","level":2,"title":"How Separator Selection Works","text":"<p>The recursive splitter tries separators in this order:</p> <ol> <li>Double newline (<code>\\n\\n</code>) - paragraph breaks</li> <li>Single newline (<code>\\n</code>) - line breaks</li> <li>Space (``) - word boundaries</li> <li>Empty string (<code>\"\"</code>) - character-by-character</li> </ol> <pre><code>from rs_document import Document\n\n# Document with no paragraph breaks, only lines\ndoc = Document(\n    page_content=\"Line 1\\nLine 2\\nLine 3\\nLine 4\",\n    metadata={}\n)\n\n# Will split on line breaks when possible\nchunks = doc.recursive_character_splitter(20)\n</code></pre>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#handle-different-document-structures","level":2,"title":"Handle Different Document Structures","text":"","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#well-structured-text","level":3,"title":"Well-Structured Text","text":"<pre><code>from rs_document import Document\n\n# Document with clear paragraph breaks\ndoc = Document(\n    page_content=\"First paragraph.\\n\\nSecond paragraph.\\n\\nThird paragraph.\",\n    metadata={}\n)\n\n# Will split on paragraph boundaries\nchunks = doc.recursive_character_splitter(100)\n</code></pre>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#line-based-text","level":3,"title":"Line-Based Text","text":"<pre><code>from rs_document import Document\n\n# Document with single lines\ndoc = Document(\n    page_content=\"Line 1\\nLine 2\\nLine 3\\nLine 4\\nLine 5\",\n    metadata={}\n)\n\n# Will split on line breaks\nchunks = doc.recursive_character_splitter(30)\n</code></pre>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#dense-text","level":3,"title":"Dense Text","text":"<pre><code>from rs_document import Document\n\n# Document with minimal structure\ndoc = Document(\n    page_content=\"This is a long continuous paragraph with no breaks just spaces between words\",\n    metadata={}\n)\n\n# Will split on word boundaries\nchunks = doc.recursive_character_splitter(25)\n</code></pre>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#choose-appropriate-chunk-sizes","level":2,"title":"Choose Appropriate Chunk Sizes","text":"<p>Consider your use case when selecting chunk size:</p> <pre><code>from rs_document import Document\n\ndoc = Document(page_content=\"test \" * 10000, metadata={})\n\n# Smaller chunks: more chunks, more context overlap\nsmall = doc.recursive_character_splitter(500)\nprint(f\"500 chars: {len(small)} chunks\")\n\n# Larger chunks: fewer chunks, less overlap\nlarge = doc.recursive_character_splitter(2000)\nprint(f\"2000 chars: {len(large)} chunks\")\n\n# Choose based on:\n# - Your embedding model's context window\n# - Retrieval granularity needs\n# - Memory constraints\n</code></pre>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/splitting-tasks/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>Process multiple documents efficiently</li> <li>Add chunk metadata for tracking</li> <li>Prepare for vector databases</li> </ul>","path":["How-To Guides","Splitting Tasks"],"tags":[]},{"location":"how-to/vector-db-prep/","level":1,"title":"Vector Database Preparation","text":"<p>Learn how to prepare documents for embedding models and vector databases in RAG applications.</p>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#prepare-documents-for-embedding","level":2,"title":"Prepare Documents for Embedding","text":"<pre><code>from rs_document import clean_and_split_docs, Document\n\n# Load your documents\ndocuments = [...]  # Your documents here\n\n# Clean and split for embeddings\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\n# Extract text and metadata for your vector DB\ntexts = [chunk.page_content for chunk in chunks]\nmetadatas = [chunk.metadata for chunk in chunks]\n\n# Use with your embedding model\n# embeddings = your_embedding_model.embed(texts)\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#add-chunk-index-to-metadata","level":2,"title":"Add Chunk Index to Metadata","text":"<p>Track chunk positions within the original document:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"Long content \" * 1000,\n    metadata={\"source\": \"doc.txt\"}\n)\n\n# Split and add chunk index\nchunks = doc.recursive_character_splitter(1000)\n\n# Add chunk position info\nfor i, chunk in enumerate(chunks):\n    chunk.metadata[\"chunk_index\"] = str(i)\n    chunk.metadata[\"total_chunks\"] = str(len(chunks))\n\n# Now each chunk knows its position\nprint(chunks[0].metadata)\n# {\"source\": \"doc.txt\", \"chunk_index\": \"0\", \"total_chunks\": \"5\"}\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#choose-chunk-size-for-your-embedding-model","level":2,"title":"Choose Chunk Size for Your Embedding Model","text":"<p>Match chunk size to your embedding model's context window:</p> <pre><code>from rs_document import Document\n\ndoc = Document(page_content=\"test \" * 10000, metadata={})\n\n# For OpenAI text-embedding-ada-002 (8191 tokens ~= 32,000 chars)\nopenai_chunks = doc.recursive_character_splitter(2000)\n\n# For smaller context windows (512 tokens ~= 2000 chars)\nsmall_chunks = doc.recursive_character_splitter(1000)\n\n# For larger context windows\nlarge_chunks = doc.recursive_character_splitter(4000)\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#filter-chunks-for-quality","level":2,"title":"Filter Chunks for Quality","text":"<p>Remove chunks that are too small or don't meet quality criteria:</p> <pre><code>from rs_document import clean_and_split_docs, Document\n\ndocuments = [...]  # Your documents\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\n# Filter out very small chunks\nmin_size = 100\nquality_chunks = [\n    c for c in chunks\n    if len(c.page_content) &gt;= min_size\n]\n\nprint(f\"Kept {len(quality_chunks)} of {len(chunks)} chunks\")\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#prepare-with-openai-embeddings","level":2,"title":"Prepare with OpenAI Embeddings","text":"<pre><code>from rs_document import clean_and_split_docs, Document\nimport openai\n\n# Process documents\ndocuments = [...]  # Your documents\nchunks = clean_and_split_docs(documents, chunk_size=1500)\n\n# Get texts and metadata\ntexts = [chunk.page_content for chunk in chunks]\nmetadatas = [chunk.metadata for chunk in chunks]\n\n# Create embeddings\nclient = openai.OpenAI()\nembeddings = []\nfor text in texts:\n    response = client.embeddings.create(\n        input=text,\n        model=\"text-embedding-ada-002\"\n    )\n    embeddings.append(response.data[0].embedding)\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#prepare-with-huggingface-embeddings","level":2,"title":"Prepare with HuggingFace Embeddings","text":"<pre><code>from rs_document import clean_and_split_docs, Document\nfrom sentence_transformers import SentenceTransformer\n\n# Process documents\ndocuments = [...]  # Your documents\nchunks = clean_and_split_docs(documents, chunk_size=512)\n\n# Get texts\ntexts = [chunk.page_content for chunk in chunks]\n\n# Create embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(texts)\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#prepare-for-pinecone","level":2,"title":"Prepare for Pinecone","text":"<pre><code>from rs_document import clean_and_split_docs, Document\nimport pinecone\n\n# Process documents\ndocuments = [...]  # Your documents\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\n# Prepare vectors for Pinecone\nvectors = []\nfor i, chunk in enumerate(chunks):\n    vectors.append({\n        \"id\": f\"chunk-{i}\",\n        \"values\": embedding[i],  # Your embedding\n        \"metadata\": chunk.metadata\n    })\n\n# Upsert to Pinecone\n# index.upsert(vectors=vectors)\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#prepare-for-qdrant","level":2,"title":"Prepare for Qdrant","text":"<pre><code>from rs_document import clean_and_split_docs, Document\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import PointStruct\n\n# Process documents\ndocuments = [...]  # Your documents\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\n# Prepare points for Qdrant\npoints = []\nfor i, chunk in enumerate(chunks):\n    points.append(\n        PointStruct(\n            id=i,\n            vector=embeddings[i],  # Your embedding\n            payload={\n                \"text\": chunk.page_content,\n                **chunk.metadata\n            }\n        )\n    )\n\n# Upsert to Qdrant\n# client.upsert(collection_name=\"documents\", points=points)\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#prepare-for-chromadb","level":2,"title":"Prepare for ChromaDB","text":"<pre><code>from rs_document import clean_and_split_docs, Document\nimport chromadb\n\n# Process documents\ndocuments = [...]  # Your documents\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\n# Prepare for ChromaDB\nids = [f\"chunk-{i}\" for i in range(len(chunks))]\ndocuments_text = [chunk.page_content for chunk in chunks]\nmetadatas = [chunk.metadata for chunk in chunks]\n\n# Add to ChromaDB\n# collection.add(\n#     ids=ids,\n#     documents=documents_text,\n#     metadatas=metadatas\n# )\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#complete-rag-preparation-pipeline","level":2,"title":"Complete RAG Preparation Pipeline","text":"<pre><code>from pathlib import Path\nfrom rs_document import Document, clean_and_split_docs\n\ndef prepare_for_rag(\n    directory: str,\n    chunk_size: int = 1000,\n    min_chunk_size: int = 100\n) -&gt; tuple[list[str], list[dict]]:\n    \"\"\"Complete pipeline for RAG preparation.\"\"\"\n\n    # 1. Load documents\n    documents = []\n    for file_path in Path(directory).glob(\"**/*.txt\"):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n\n        doc = Document(\n            page_content=content,\n            metadata={\n                \"source\": file_path.name,\n                \"path\": str(file_path),\n            }\n        )\n        documents.append(doc)\n\n    # 2. Clean and split\n    chunks = clean_and_split_docs(documents, chunk_size=chunk_size)\n\n    # 3. Filter quality\n    quality_chunks = [\n        c for c in chunks\n        if len(c.page_content) &gt;= min_chunk_size\n    ]\n\n    # 4. Add chunk metadata\n    for i, chunk in enumerate(quality_chunks):\n        chunk.metadata[\"chunk_id\"] = str(i)\n\n    # 5. Extract texts and metadata\n    texts = [chunk.page_content for chunk in quality_chunks]\n    metadatas = [chunk.metadata for chunk in quality_chunks]\n\n    return texts, metadatas\n\n# Use it\ntexts, metadatas = prepare_for_rag(\"./documents\")\nprint(f\"Ready: {len(texts)} chunks\")\n</code></pre>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"how-to/vector-db-prep/#next-steps","level":2,"title":"Next Steps","text":"<ul> <li>See Batch Operations for performance tips</li> <li>Check LangChain Integration for LangChain workflows</li> <li>Review Splitting Tasks for chunking strategies</li> </ul>","path":["How-To Guides","Vector Database Preparation"],"tags":[]},{"location":"reference/","level":1,"title":"API Reference","text":"<p>Complete reference documentation for all public APIs in rs_document.</p>","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/#quick-navigation","level":2,"title":"Quick Navigation","text":"","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/#core-components","level":3,"title":"Core Components","text":"<ul> <li>Document Class - Document constructor and attributes<ul> <li>Constructor and initialization</li> <li><code>page_content</code> attribute</li> <li><code>metadata</code> attribute</li> </ul> </li> </ul>","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/#document-processing","level":3,"title":"Document Processing","text":"<ul> <li> <p>Cleaning Methods - Text cleaning and normalization</p> <ul> <li><code>clean()</code> - Run all cleaners</li> <li><code>clean_extra_whitespace()</code> - Normalize whitespace</li> <li><code>clean_ligatures()</code> - Convert typographic ligatures</li> <li><code>clean_bullets()</code> - Remove bullet characters</li> <li><code>clean_non_ascii_chars()</code> - Remove non-ASCII characters</li> <li><code>group_broken_paragraphs()</code> - Join split paragraphs</li> </ul> </li> <li> <p>Splitting Methods - Document chunking strategies</p> <ul> <li><code>recursive_character_splitter()</code> - Smart splitting with overlap</li> <li><code>split_on_num_characters()</code> - Fixed-size splitting</li> </ul> </li> </ul>","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/#utilities","level":3,"title":"Utilities","text":"<ul> <li>Utility Functions - Batch processing and helpers<ul> <li><code>clean_and_split_docs()</code> - Parallel processing for multiple documents</li> </ul> </li> </ul>","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/#reference-information","level":3,"title":"Reference Information","text":"<ul> <li>Types and Constants - Type hints, defaults, and error handling<ul> <li>Type signatures</li> <li>Default values and constants</li> <li>Error handling patterns</li> <li>Compatibility notes</li> </ul> </li> </ul>","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/#overview","level":2,"title":"Overview","text":"<p>rs_document is a high-performance Python library for document processing, built with Rust for speed. The library provides:</p> <ul> <li>Document representation: Simple, LangChain-compatible document structure</li> <li>Text cleaning: Normalize whitespace, remove artifacts, fix ligatures</li> <li>Document splitting: Split large documents into chunks for RAG applications</li> <li>Parallel processing: Process thousands of documents efficiently</li> </ul>","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/#basic-usage","level":2,"title":"Basic Usage","text":"<pre><code>from rs_document import Document, clean_and_split_docs\n\n# Create a document\ndoc = Document(\n    page_content=\"Your text content here\",\n    metadata={\"source\": \"example.txt\", \"page\": \"1\"}\n)\n\n# Clean and split\ndoc.clean()\nchunks = doc.recursive_character_splitter(chunk_size=1000)\n\n# Or process multiple documents in parallel\ndocuments = [doc1, doc2, doc3]\nall_chunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre>","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/#performance","level":2,"title":"Performance","text":"<ul> <li>Fast: 20-25x faster than pure Python implementations</li> <li>Parallel: Automatically uses all CPU cores for batch processing</li> <li>Scalable: Process ~23,000 documents per second on typical hardware</li> </ul>","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/#requirements","level":2,"title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>Pre-built wheels available for most platforms (Linux, macOS, Windows)</li> </ul>","path":["Reference","API Reference Overview"],"tags":[]},{"location":"reference/cleaning-methods/","level":1,"title":"Cleaning Methods","text":"<p>All cleaning methods modify the document in-place and return <code>None</code>. They are designed to clean and normalize text extracted from PDFs, web pages, and other sources.</p>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#overview","level":2,"title":"Overview","text":"<p>Cleaning methods are called on <code>Document</code> instances:</p> <pre><code>from rs_document import Document\n\ndoc = Document(page_content=\"Raw text\", metadata={})\ndoc.clean()  # Modifies doc.page_content in-place\n</code></pre> <p>Important: All cleaning methods modify the document's <code>page_content</code> directly. The original content is not preserved. If you need to keep the original, create a copy first.</p>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#methods","level":2,"title":"Methods","text":"","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#clean","level":3,"title":"<code>clean()</code>","text":"<p>Run all available cleaners in sequence.</p> <p>Signature:</p> <pre><code>doc.clean() -&gt; None\n</code></pre> <p>Description:</p> <p>Applies all cleaning operations in a specific order. This is the most convenient method for general-purpose cleaning.</p> <p>Execution Order:</p> <ol> <li><code>clean_extra_whitespace()</code> - Normalize whitespace</li> <li><code>clean_ligatures()</code> - Convert typographic ligatures</li> <li><code>clean_bullets()</code> - Remove bullet characters</li> <li><code>clean_non_ascii_chars()</code> - Remove non-ASCII characters</li> <li><code>group_broken_paragraphs()</code> - Join split paragraphs</li> </ol> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p><code>None</code> - Modifies the document in-place.</p> <p>Example:</p> <pre><code>doc = Document(\n    page_content=\"●  Text with   spaces and æ ligatures\\x88\",\n    metadata={}\n)\n\ndoc.clean()\n\n# All cleaning operations applied:\n# - Extra whitespace normalized\n# - Ligature 'æ' converted to 'ae'\n# - Bullet '●' removed\n# - Non-ASCII character '\\x88' removed\nprint(doc.page_content)  # \"Text with spaces and ae ligatures\"\n</code></pre> <p>Use Cases:</p> <ul> <li>General-purpose document cleaning</li> <li>Preparing text for analysis or embedding</li> <li>Sanitizing text from unknown sources</li> </ul> <p>Note: The order is fixed and cannot be customized. If you need a different order or subset of operations, call individual cleaning methods directly.</p>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#clean_extra_whitespace","level":3,"title":"<code>clean_extra_whitespace()</code>","text":"<p>Normalize whitespace in the document.</p> <p>Signature:</p> <pre><code>doc.clean_extra_whitespace() -&gt; None\n</code></pre> <p>Description:</p> <p>Normalizes all whitespace in the document by:</p> <ul> <li>Replacing multiple consecutive spaces with a single space</li> <li>Removing leading and trailing whitespace from lines</li> <li>Preserving newlines and paragraph structure</li> </ul> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p><code>None</code> - Modifies the document in-place.</p> <p>Example:</p> <pre><code>doc = Document(\n    page_content=\"ITEM 1.     BUSINESS \",\n    metadata={}\n)\n\ndoc.clean_extra_whitespace()\n\nprint(doc.page_content)  # \"ITEM 1. BUSINESS\"\nprint(repr(doc.page_content))  # Shows no trailing or extra spaces\n</code></pre> <p>Complex Example:</p> <pre><code>doc = Document(\n    page_content=\"Line 1    with    spaces\\n   Line 2   \\n\\nLine 3  \",\n    metadata={}\n)\n\ndoc.clean_extra_whitespace()\n\nprint(doc.page_content)\n# \"Line 1 with spaces\\nLine 2\\n\\nLine 3\"\n# - Multiple spaces reduced to one\n# - Leading/trailing spaces removed from each line\n# - Newlines preserved\n</code></pre> <p>What is Preserved:</p> <ul> <li>Single newlines (<code>\\n</code>)</li> <li>Paragraph breaks (double newlines <code>\\n\\n</code>)</li> <li>Document structure</li> </ul> <p>What is Removed:</p> <ul> <li>Multiple consecutive spaces</li> <li>Leading spaces on lines</li> <li>Trailing spaces on lines</li> <li>Tabs are converted to spaces</li> </ul> <p>Use Cases:</p> <ul> <li>Cleaning up OCR output</li> <li>Normalizing text from PDFs with poor formatting</li> <li>Removing formatting artifacts from copied text</li> <li>Preparing text for tokenization or analysis</li> </ul>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#clean_ligatures","level":3,"title":"<code>clean_ligatures()</code>","text":"<p>Convert typographic ligatures to their component characters.</p> <p>Signature:</p> <pre><code>doc.clean_ligatures() -&gt; None\n</code></pre> <p>Description:</p> <p>Replaces typographic ligatures with their expanded character sequences. Ligatures are single characters representing multiple letters, commonly found in typeset documents and PDFs.</p> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p><code>None</code> - Modifies the document in-place.</p> <p>Ligature Conversions:</p> Ligature Converted To Unicode <code>æ</code> <code>ae</code> U+00E6 <code>Æ</code> <code>AE</code> U+00C6 <code>œ</code> <code>oe</code> U+0153 <code>Œ</code> <code>OE</code> U+0152 <code>ﬁ</code> <code>fi</code> U+FB01 <code>ﬂ</code> <code>fl</code> U+FB02 <code>ﬀ</code> <code>ff</code> U+FB00 <code>ﬃ</code> <code>ffi</code> U+FB03 <code>ﬄ</code> <code>ffl</code> U+FB04 <code>ﬅ</code> <code>ft</code> U+FB05 <code>ﬆ</code> <code>st</code> U+FB06 <p>Example:</p> <pre><code>doc = Document(\n    page_content=\"The encyclopædia has œnology section\",\n    metadata={}\n)\n\ndoc.clean_ligatures()\n\nprint(doc.page_content)  # \"The encyclopaedia has oenology section\"\n</code></pre> <p>PDF Example:</p> <pre><code># Text extracted from a PDF may contain ligatures\ndoc = Document(\n    page_content=\"The ofﬁce ﬂoor has ﬁne ﬁnishes\",\n    metadata={\"source\": \"document.pdf\"}\n)\n\ndoc.clean_ligatures()\n\nprint(doc.page_content)  # \"The office floor has fine finishes\"\n</code></pre> <p>Use Cases:</p> <ul> <li>Normalizing text from PDFs</li> <li>Cleaning text from typeset documents</li> <li>Improving search and matching (ligatures may not match regular searches)</li> <li>Preparing text for NLP or embedding models</li> </ul> <p>Note: This only affects actual ligature characters. Regular letter combinations (like \"ae\" or \"fi\") are not modified.</p>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#clean_bullets","level":3,"title":"<code>clean_bullets()</code>","text":"<p>Remove bullet point characters from the text.</p> <p>Signature:</p> <pre><code>doc.clean_bullets() -&gt; None\n</code></pre> <p>Description:</p> <p>Removes common bullet point characters used in lists. The bullet characters are deleted completely (not replaced with anything).</p> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p><code>None</code> - Modifies the document in-place.</p> <p>Removed Characters:</p> <ul> <li><code>●</code> (U+25CF - Black Circle)</li> <li><code>○</code> (U+25CB - White Circle)</li> <li><code>■</code> (U+25A0 - Black Square)</li> <li><code>□</code> (U+25A1 - White Square)</li> <li><code>•</code> (U+2022 - Bullet)</li> <li><code>◦</code> (U+25E6 - White Bullet)</li> <li><code>▪</code> (U+25AA - Black Small Square)</li> <li><code>▫</code> (U+25AB - White Small Square)</li> </ul> <p>Example:</p> <pre><code>doc = Document(\n    page_content=\"● First item\\n● Second item\\n● Third item\",\n    metadata={}\n)\n\ndoc.clean_bullets()\n\nprint(doc.page_content)\n# \"First item\\nSecond item\\nThird item\"\n</code></pre> <p>PDF List Example:</p> <pre><code># Common in PDF extractions\ndoc = Document(\n    page_content=\"Key Points:\\n■ Point one\\n■ Point two\\n□ Sub-point\",\n    metadata={}\n)\n\ndoc.clean_bullets()\n\nprint(doc.page_content)\n# \"Key Points:\\nPoint one\\nPoint two\\nSub-point\"\n</code></pre> <p>Combined with Whitespace Cleaning:</p> <pre><code>doc = Document(\n    page_content=\"●  Item with extra spaces\",\n    metadata={}\n)\n\ndoc.clean_bullets()\ndoc.clean_extra_whitespace()\n\nprint(doc.page_content)  # \"Item with extra spaces\"\n</code></pre> <p>Use Cases:</p> <ul> <li>Cleaning bulleted lists from PDFs</li> <li>Removing formatting artifacts from web content</li> <li>Preparing text for analysis where bullets are not needed</li> <li>Normalizing list formatting</li> </ul> <p>Note: This only removes the bullet characters themselves. List structure (newlines and indentation) is preserved.</p>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#clean_non_ascii_chars","level":3,"title":"<code>clean_non_ascii_chars()</code>","text":"<p>Remove all non-ASCII characters from the document.</p> <p>Signature:</p> <pre><code>doc.clean_non_ascii_chars() -&gt; None\n</code></pre> <p>Description:</p> <p>Removes any character with an ASCII value greater than 127. This includes:</p> <ul> <li>Extended Unicode characters</li> <li>Accented letters (é, ñ, ü, etc.)</li> <li>Special symbols and emoji</li> <li>Control characters beyond basic ASCII</li> </ul> <p>What is Kept:</p> <p>Standard ASCII characters (0-127):</p> <ul> <li>Letters: <code>a-z</code>, <code>A-Z</code></li> <li>Numbers: <code>0-9</code></li> <li>Punctuation: <code>. , ! ? ; : ' \" -</code> etc.</li> <li>Whitespace: space, tab, newline</li> <li>Basic symbols: <code>@ # $ % &amp; * ( ) [ ] { } &lt; &gt; / \\</code> etc.</li> </ul> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p><code>None</code> - Modifies the document in-place.</p> <p>Example:</p> <pre><code>doc = Document(\n    page_content=\"Hello\\x88World\\x89\",\n    metadata={}\n)\n\ndoc.clean_non_ascii_chars()\n\nprint(doc.page_content)  # \"HelloWorld\"\n</code></pre> <p>Complex Example:</p> <pre><code>doc = Document(\n    page_content=\"Café résumé 中文 emoji😀 special™ characters©\",\n    metadata={}\n)\n\ndoc.clean_non_ascii_chars()\n\nprint(doc.page_content)\n# \"Caf rsum  emoji special characters\"\n# All non-ASCII characters removed\n</code></pre> <p>PDF Artifacts Example:</p> <pre><code># PDFs often contain non-ASCII control characters\ndoc = Document(\n    page_content=\"Text\\x00with\\x01hidden\\x02control\\x88chars\",\n    metadata={}\n)\n\ndoc.clean_non_ascii_chars()\n\nprint(doc.page_content)  # \"Textwith\\x01hidden\\x02controlchars\"\n# Note: Only chars &gt; 127 removed; ASCII control chars (0-31) remain\n</code></pre> <p>Use Cases:</p> <ul> <li>Sanitizing text for ASCII-only systems</li> <li>Removing PDF extraction artifacts</li> <li>Cleaning text for systems with poor Unicode support</li> <li>Removing special characters before processing</li> </ul> <p>Warning: This is a destructive operation that removes all accented characters and non-English text. Use with caution:</p> <pre><code>doc = Document(page_content=\"Résumé: José García\", metadata={})\ndoc.clean_non_ascii_chars()\nprint(doc.page_content)  # \"Rsum: Jos Garca\" - information lost!\n</code></pre> <p>Alternative: Consider <code>clean_ligatures()</code> for preserving character information while normalizing ligatures specifically.</p>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#group_broken_paragraphs","level":3,"title":"<code>group_broken_paragraphs()</code>","text":"<p>Join paragraphs that were incorrectly split across multiple lines.</p> <p>Signature:</p> <pre><code>doc.group_broken_paragraphs() -&gt; None\n</code></pre> <p>Description:</p> <p>Identifies and joins lines that should be part of the same paragraph. This is especially useful for text extracted from PDFs, where line breaks often don't correspond to semantic paragraph boundaries.</p> <p>What it Does:</p> <ul> <li>Identifies lines that are part of the same paragraph</li> <li>Joins them with appropriate spacing</li> <li>Preserves intentional paragraph breaks (double newlines)</li> <li>Maintains document structure</li> </ul> <p>Parameters:</p> <p>None.</p> <p>Returns:</p> <p><code>None</code> - Modifies the document in-place.</p> <p>Example:</p> <pre><code>doc = Document(\n    page_content=\"This is a sentence\\nthat was split\\nacross lines.\\n\\nNew paragraph.\",\n    metadata={}\n)\n\ndoc.group_broken_paragraphs()\n\n# The first paragraph is joined, but the paragraph break is preserved\nprint(doc.page_content)\n# \"This is a sentence that was split across lines.\\n\\nNew paragraph.\"\n</code></pre> <p>PDF Extraction Example:</p> <pre><code># Common issue with PDF extraction\ndoc = Document(\n    page_content=\"\"\"The quick brown fox jumps\nover the lazy dog. This is\na continuous paragraph that\nwas split by page width.\n\nThis is a new paragraph after\na blank line.\"\"\",\n    metadata={\"source\": \"document.pdf\"}\n)\n\ndoc.group_broken_paragraphs()\n\nprint(doc.page_content)\n# \"The quick brown fox jumps over the lazy dog. This is a continuous\n# paragraph that was split by page width.\n#\n# This is a new paragraph after a blank line.\"\n</code></pre> <p>What is Preserved:</p> <ul> <li>Paragraph breaks (double newlines or blank lines)</li> <li>Document structure and sections</li> <li>Intentional formatting</li> </ul> <p>What is Modified:</p> <ul> <li>Single newlines within paragraphs are converted to spaces</li> <li>Lines are joined when they appear to be part of the same thought</li> <li>Broken sentences are reassembled</li> </ul> <p>Use Cases:</p> <ul> <li>Fixing text extracted from PDFs where line breaks don't match semantic structure</li> <li>Cleaning documents with arbitrary line wrapping</li> <li>Improving readability of extracted text</li> <li>Preparing documents for semantic analysis or embedding</li> </ul> <p>Combined Usage:</p> <pre><code># Typical cleaning sequence for PDF text\ndoc = Document(page_content=pdf_text, metadata={\"source\": \"doc.pdf\"})\n\ndoc.clean_extra_whitespace()    # First, normalize spacing\ndoc.group_broken_paragraphs()   # Then, join broken paragraphs\ndoc.clean_ligatures()            # Finally, convert ligatures\n\n# Or just use clean() for all operations\ndoc.clean()\n</code></pre>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#method-comparison","level":2,"title":"Method Comparison","text":"Method Purpose Preserves Structure Typical Use <code>clean()</code> Apply all cleaners Mostly General cleaning <code>clean_extra_whitespace()</code> Normalize spaces Yes OCR cleanup <code>clean_ligatures()</code> Expand ligatures Yes PDF normalization <code>clean_bullets()</code> Remove bullets Yes List cleaning <code>clean_non_ascii_chars()</code> ASCII only Yes Sanitization <code>group_broken_paragraphs()</code> Join paragraphs Partial PDF paragraph fixing","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#common-patterns","level":2,"title":"Common Patterns","text":"","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#full-cleaning-pipeline","level":3,"title":"Full Cleaning Pipeline","text":"<pre><code>from rs_document import Document\n\ndoc = Document(page_content=raw_text, metadata={\"source\": \"file.pdf\"})\ndoc.clean()  # Run all cleaners in optimal order\n</code></pre>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#selective-cleaning","level":3,"title":"Selective Cleaning","text":"<pre><code># Only specific operations needed\ndoc = Document(page_content=text, metadata={})\ndoc.clean_extra_whitespace()\ndoc.clean_ligatures()\n# Skip bullet/ASCII cleaning\n</code></pre>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#preserve-original","level":3,"title":"Preserve Original","text":"<pre><code># Keep original content\noriginal_content = doc.page_content\ndoc.clean()\n# Can still access original_content variable\n</code></pre>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#custom-order","level":3,"title":"Custom Order","text":"<pre><code># Different order than clean() method\ndoc = Document(page_content=text, metadata={})\ndoc.group_broken_paragraphs()  # First, fix structure\ndoc.clean_extra_whitespace()   # Then, normalize\ndoc.clean_bullets()             # Then, remove bullets\n# Skip ligatures and non-ASCII\n</code></pre>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/cleaning-methods/#see-also","level":2,"title":"See Also","text":"<ul> <li>Document Class - Document creation and attributes</li> <li>Splitting Methods - Split documents after cleaning</li> <li>Utility Functions - Batch processing with cleaning</li> </ul>","path":["Reference","Cleaning Methods"],"tags":[]},{"location":"reference/document-class/","level":1,"title":"Document Class","text":"<p>The main class for working with text documents in rs_document.</p>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#constructor","level":2,"title":"Constructor","text":"","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#documentpage_content-metadata","level":3,"title":"<code>Document(page_content, metadata)</code>","text":"<p>Create a new Document instance.</p> <p>Signature:</p> <pre><code>Document(page_content: str, metadata: dict[str, str]) -&gt; Document\n</code></pre> <p>Parameters:</p> <ul> <li><code>page_content</code> (<code>str</code>): The text content of the document</li> <li><code>metadata</code> (<code>dict[str, str]</code>): Dictionary of string key-value pairs containing document metadata</li> </ul> <p>Returns:</p> <p>A new <code>Document</code> instance.</p> <p>Example:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"Hello, world!\",\n    metadata={\"source\": \"example.txt\", \"page\": \"1\"}\n)\n</code></pre> <p>Important Notes:</p> <ul> <li>Metadata keys and values must be strings</li> <li>Non-string values must be converted to strings before creating the document</li> <li>Empty strings are valid for both <code>page_content</code> and metadata values</li> </ul> <p>Example with Type Conversion:</p> <pre><code># Convert non-string metadata values\nraw_metadata = {\n    \"id\": 123,\n    \"page\": 5,\n    \"active\": True,\n    \"score\": 98.6\n}\n\ndoc = Document(\n    page_content=\"Document text\",\n    metadata={k: str(v) for k, v in raw_metadata.items()}\n)\n# metadata is now {\"id\": \"123\", \"page\": \"5\", \"active\": \"True\", \"score\": \"98.6\"}\n</code></pre>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#attributes","level":2,"title":"Attributes","text":"","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#page_content","level":3,"title":"<code>page_content</code>","text":"<p>The text content of the document.</p> <p>Type: <code>str</code></p> <p>Access: Read and write</p> <p>Description:</p> <p>The <code>page_content</code> attribute contains the document's text. It can be read or modified directly. Changes to this attribute affect all subsequent operations on the document.</p> <p>Example:</p> <pre><code>doc = Document(page_content=\"Hello\", metadata={})\n\n# Read the content\nprint(doc.page_content)  # \"Hello\"\n\n# Modify the content\ndoc.page_content = \"Goodbye\"\nprint(doc.page_content)  # \"Goodbye\"\n\n# Content is updated\ndoc.clean()  # Operates on \"Goodbye\", not \"Hello\"\n</code></pre> <p>Use Cases:</p> <ul> <li>Accessing document text for display or analysis</li> <li>Modifying content programmatically</li> <li>Checking content length or properties before processing</li> </ul>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#metadata","level":3,"title":"<code>metadata</code>","text":"<p>Dictionary containing document metadata.</p> <p>Type: <code>dict[str, str]</code></p> <p>Access: Read and write</p> <p>Description:</p> <p>The <code>metadata</code> attribute stores key-value pairs about the document. All keys and values must be strings. The metadata dictionary can be read, modified, or updated like a standard Python dictionary.</p> <p>Example:</p> <pre><code>doc = Document(\n    page_content=\"text\",\n    metadata={\"author\": \"Jane\", \"date\": \"2024-01-01\"}\n)\n\n# Read metadata\nprint(doc.metadata[\"author\"])  # \"Jane\"\n\n# Add new metadata\ndoc.metadata[\"category\"] = \"tech\"\n\n# Update existing metadata\ndoc.metadata[\"date\"] = \"2024-01-02\"\n\n# Iterate over metadata\nfor key, value in doc.metadata.items():\n    print(f\"{key}: {value}\")\n</code></pre> <p>Metadata Preservation:</p> <p>When documents are split using splitting methods, the metadata dictionary is copied to all resulting chunks:</p> <pre><code>doc = Document(\n    page_content=\"A\" * 5000,\n    metadata={\"source\": \"original.txt\", \"section\": \"introduction\"}\n)\n\nchunks = doc.recursive_character_splitter(1000)\n\n# All chunks have the same metadata\nfor chunk in chunks:\n    print(chunk.metadata)  # {\"source\": \"original.txt\", \"section\": \"introduction\"}\n</code></pre> <p>Important Notes:</p> <ul> <li>Only string keys and values are supported</li> <li>Attempting to store non-string values may cause errors</li> <li>Metadata is shallow-copied during splitting operations</li> <li>Empty dictionaries are valid metadata</li> </ul>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#instance-creation-patterns","level":2,"title":"Instance Creation Patterns","text":"","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#basic-creation","level":3,"title":"Basic Creation","text":"<pre><code>doc = Document(\n    page_content=\"Simple text\",\n    metadata={\"id\": \"1\"}\n)\n</code></pre>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#empty-document","level":3,"title":"Empty Document","text":"<pre><code># Valid - empty document\ndoc = Document(page_content=\"\", metadata={})\n</code></pre>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#from-file","level":3,"title":"From File","text":"<pre><code>with open(\"document.txt\", \"r\") as f:\n    content = f.read()\n\ndoc = Document(\n    page_content=content,\n    metadata={\"source\": \"document.txt\", \"type\": \"text\"}\n)\n</code></pre>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#from-multiple-sources","level":3,"title":"From Multiple Sources","text":"<pre><code>documents = []\n\nfor filename in [\"doc1.txt\", \"doc2.txt\", \"doc3.txt\"]:\n    with open(filename, \"r\") as f:\n        doc = Document(\n            page_content=f.read(),\n            metadata={\"source\": filename}\n        )\n        documents.append(doc)\n</code></pre>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#langchain-compatibility","level":2,"title":"LangChain Compatibility","text":"<p>The <code>Document</code> class is designed to be compatible with LangChain's Document model.</p> <p>Similarities:</p> <ul> <li>Same attribute names (<code>page_content</code>, <code>metadata</code>)</li> <li>Similar API design and usage patterns</li> <li>Compatible structure for most operations</li> </ul> <p>Differences:</p> <ul> <li>rs_document: Metadata values must be strings</li> <li>LangChain: Metadata can contain any Python object</li> </ul> <p>Converting from LangChain:</p> <pre><code>from langchain_core.documents import Document as LCDocument\nfrom rs_document import Document as RSDocument\n\n# LangChain document\nlc_doc = LCDocument(\n    page_content=\"text\",\n    metadata={\"key\": \"value\", \"count\": 42, \"active\": True}\n)\n\n# Convert to rs_document (stringify metadata)\nrs_doc = RSDocument(\n    page_content=lc_doc.page_content,\n    metadata={k: str(v) for k, v in lc_doc.metadata.items()}\n)\n</code></pre> <p>Converting to LangChain:</p> <pre><code>from rs_document import Document as RSDocument\nfrom langchain_core.documents import Document as LCDocument\n\n# rs_document document\nrs_doc = RSDocument(\n    page_content=\"text\",\n    metadata={\"id\": \"123\", \"page\": \"5\"}\n)\n\n# Convert to LangChain (direct copy)\nlc_doc = LCDocument(\n    page_content=rs_doc.page_content,\n    metadata=rs_doc.metadata\n)\n</code></pre>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/document-class/#see-also","level":2,"title":"See Also","text":"<ul> <li>Cleaning Methods - Methods for cleaning document content</li> <li>Splitting Methods - Methods for splitting documents into chunks</li> <li>Types and Constants - Type signatures and error handling</li> </ul>","path":["Reference","Document Class"],"tags":[]},{"location":"reference/splitting-methods/","level":1,"title":"Splitting Methods","text":"<p>Methods for splitting documents into smaller chunks. All splitting methods return a list of new <code>Document</code> instances, leaving the original document unchanged.</p>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#overview","level":2,"title":"Overview","text":"<p>Splitting methods create new documents from the original:</p> <pre><code>from rs_document import Document\n\ndoc = Document(page_content=\"Long text...\", metadata={\"source\": \"file.txt\"})\nchunks = doc.recursive_character_splitter(1000)\n\n# Original document is unchanged\nprint(doc.page_content)  # Still \"Long text...\"\n\n# New documents created\nprint(len(chunks))  # Number of chunks\nprint(chunks[0].metadata)  # {\"source\": \"file.txt\"} - metadata copied\n</code></pre>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#methods","level":2,"title":"Methods","text":"","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#recursive_character_splitter","level":3,"title":"<code>recursive_character_splitter()</code>","text":"<p>Split document into chunks using recursive strategy with natural language boundaries.</p> <p>Signature:</p> <pre><code>doc.recursive_character_splitter(chunk_size: int) -&gt; list[Document]\n</code></pre> <p>Description:</p> <p>Splits a document into chunks of approximately <code>chunk_size</code> characters, attempting to split on natural language boundaries. Uses a recursive approach, trying multiple separators in order of preference.</p> <p>Parameters:</p> <ul> <li><code>chunk_size</code> (<code>int</code>): Target size for each chunk in characters. Chunks will not exceed this size.</li> </ul> <p>Returns:</p> <p><code>list[Document]</code> - List of Document instances, each with:</p> <ul> <li><code>page_content</code>: A chunk of the original text (≤ <code>chunk_size</code> characters)</li> <li><code>metadata</code>: Copy of the original document's metadata</li> </ul> <p>Splitting Strategy:</p> <p>The method tries separators in this order of preference:</p> <ol> <li>Paragraph breaks (<code>\\n\\n</code>) - Preferred for maintaining semantic coherence</li> <li>Line breaks (<code>\\n</code>) - If paragraphs are too large</li> <li>Word boundaries (spaces) - If lines are too large</li> <li>Character boundaries - Last resort if words are too large</li> </ol> <p>Chunk Overlap:</p> <p>Creates approximately 33% overlap between consecutive chunks. This overlap is hardcoded and ensures context is preserved across chunk boundaries.</p> <p>Example:</p> <pre><code>doc = Document(\n    page_content=\"A\" * 5000,\n    metadata={\"source\": \"file.txt\"}\n)\n\nchunks = doc.recursive_character_splitter(1000)\n\nprint(len(chunks))  # Number of chunks created (approximately 5-7 due to overlap)\nprint(len(chunks[0].page_content))  # ~1000 or less\nprint(len(chunks[1].page_content))  # ~1000 or less\nprint(chunks[0].metadata)  # {\"source\": \"file.txt\"}\nprint(chunks[1].metadata)  # {\"source\": \"file.txt\"}\n</code></pre> <p>Paragraph Splitting Example:</p> <pre><code>doc = Document(\n    page_content=\"\"\"First paragraph with some content.\n\nSecond paragraph with more content.\n\nThird paragraph with even more content.\"\"\",\n    metadata={\"doc_id\": \"123\"}\n)\n\nchunks = doc.recursive_character_splitter(50)\n\n# Splits on paragraph breaks when possible\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i}: {chunk.page_content[:30]}...\")\n    print(f\"Length: {len(chunk.page_content)}\")\n</code></pre> <p>Overlap Demonstration:</p> <pre><code>doc = Document(\n    page_content=\"Word \" * 1000,  # 5000 characters\n    metadata={}\n)\n\nchunks = doc.recursive_character_splitter(100)\n\n# Check overlap between consecutive chunks\nchunk1_end = chunks[0].page_content[-20:]\nchunk2_start = chunks[1].page_content[:20]\n\nprint(f\"End of chunk 1: '{chunk1_end}'\")\nprint(f\"Start of chunk 2: '{chunk2_start}'\")\n# Likely to see overlapping content\n</code></pre> <p>Edge Cases:</p> <pre><code># Empty document\ndoc = Document(page_content=\"\", metadata={})\nchunks = doc.recursive_character_splitter(1000)\nprint(chunks)  # []\n\n# Short document (smaller than chunk_size)\ndoc = Document(page_content=\"Short\", metadata={})\nchunks = doc.recursive_character_splitter(1000)\nprint(len(chunks))  # 1\nprint(chunks[0].page_content)  # \"Short\"\n\n# Very long single word (no spaces)\ndoc = Document(page_content=\"A\" * 5000, metadata={})\nchunks = doc.recursive_character_splitter(1000)\n# Will split by characters as last resort\n</code></pre> <p>Characteristics:</p> <ul> <li>Respects boundaries: Prefers paragraph, then line, then word boundaries</li> <li>Overlap: ~33% overlap between chunks (hardcoded)</li> <li>Metadata preservation: All chunks receive copy of original metadata</li> <li>Size guarantee: No chunk exceeds <code>chunk_size</code></li> <li>Context preservation: Overlap ensures semantic context across boundaries</li> </ul> <p>Use Cases:</p> <ul> <li>RAG applications: When context is important for retrieval</li> <li>Semantic search: Maintaining paragraph coherence</li> <li>Question answering: Overlapping chunks help answer questions at boundaries</li> <li>Document analysis: Preserving document structure</li> </ul> <p>Performance:</p> <pre><code>import time\nfrom rs_document import Document\n\ndoc = Document(page_content=\"A\" * 1_000_000, metadata={})\n\nstart = time.time()\nchunks = doc.recursive_character_splitter(1000)\nelapsed = time.time() - start\n\nprint(f\"Split into {len(chunks)} chunks in {elapsed:.3f} seconds\")\n# Fast even for large documents (Rust implementation)\n</code></pre> <p>Comparison with <code>split_on_num_characters()</code>:</p> Feature <code>recursive_character_splitter()</code> <code>split_on_num_characters()</code> Boundary respect Yes (paragraph → line → word → char) No (exact character positions) Overlap Yes (~33%) No Chunk size Target (may be smaller) Exact (except last chunk) Use case RAG, semantic applications Uniform processing","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#split_on_num_characters","level":3,"title":"<code>split_on_num_characters()</code>","text":"<p>Split document into chunks of exactly the specified size with no overlap.</p> <p>Signature:</p> <pre><code>doc.split_on_num_characters(num_chars: int) -&gt; list[Document]\n</code></pre> <p>Description:</p> <p>Splits a document into fixed-size chunks at exact character boundaries. Does not consider word, line, or paragraph boundaries. Creates no overlap between chunks.</p> <p>Parameters:</p> <ul> <li><code>num_chars</code> (<code>int</code>): Number of characters per chunk</li> </ul> <p>Returns:</p> <p><code>list[Document]</code> - List of Document instances, each with:</p> <ul> <li><code>page_content</code>: Exactly <code>num_chars</code> characters (except possibly the last chunk)</li> <li><code>metadata</code>: Copy of the original document's metadata</li> </ul> <p>Example:</p> <pre><code>doc = Document(\n    page_content=\"ABCDEFGHIJ\",\n    metadata={\"id\": \"123\"}\n)\n\nchunks = doc.split_on_num_characters(3)\n\nprint(len(chunks))  # 4\nprint([c.page_content for c in chunks])  # [\"ABC\", \"DEF\", \"GHI\", \"J\"]\nprint(chunks[0].metadata)  # {\"id\": \"123\"}\nprint(chunks[1].metadata)  # {\"id\": \"123\"}\n</code></pre> <p>Longer Example:</p> <pre><code>doc = Document(\n    page_content=\"The quick brown fox jumps over the lazy dog\",\n    metadata={\"source\": \"example\"}\n)\n\nchunks = doc.split_on_num_characters(10)\n\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i}: '{chunk.page_content}'\")\n\n# Output:\n# Chunk 0: 'The quick '\n# Chunk 1: 'brown fox '\n# Chunk 2: 'jumps over'\n# Chunk 3: ' the lazy '\n# Chunk 4: 'dog'\n</code></pre> <p>Edge Cases:</p> <pre><code># Empty document\ndoc = Document(page_content=\"\", metadata={})\nchunks = doc.split_on_num_characters(10)\nprint(chunks)  # []\n\n# Document smaller than chunk size\ndoc = Document(page_content=\"Hello\", metadata={})\nchunks = doc.split_on_num_characters(100)\nprint(len(chunks))  # 1\nprint(chunks[0].page_content)  # \"Hello\"\n\n# Exact multiple\ndoc = Document(page_content=\"ABCDEFGHIJKL\", metadata={})\nchunks = doc.split_on_num_characters(4)\nprint([c.page_content for c in chunks])  # [\"ABCD\", \"EFGH\", \"IJKL\"]\n\n# Last chunk smaller\ndoc = Document(page_content=\"ABCDEFGHIJ\", metadata={})\nchunks = doc.split_on_num_characters(4)\nprint([c.page_content for c in chunks])  # [\"ABCD\", \"EFGH\", \"IJ\"]\n</code></pre> <p>Word Splitting Demonstration:</p> <pre><code># This method WILL split words mid-character\ndoc = Document(\n    page_content=\"Supercalifragilisticexpialidocious\",\n    metadata={}\n)\n\nchunks = doc.split_on_num_characters(10)\n\nfor chunk in chunks:\n    print(chunk.page_content)\n\n# Output:\n# Supercalif\n# ragilistic\n# expialidoc\n# ious\n# Note: Words are split without regard for boundaries\n</code></pre> <p>Characteristics:</p> <ul> <li>Fixed size: All chunks exactly <code>num_chars</code> characters (except last)</li> <li>No overlap: Chunks are consecutive with no shared content</li> <li>No boundary respect: Splits at exact character positions</li> <li>Simple: Predictable, straightforward splitting</li> <li>Metadata preservation: All chunks receive copy of original metadata</li> </ul> <p>Use Cases:</p> <ul> <li>Fixed-size processing: When exact chunk sizes are required</li> <li>Token limit compliance: Ensuring chunks fit within strict limits</li> <li>Uniform analysis: When all chunks should have same size</li> <li>Simple splitting: When semantic boundaries don't matter</li> </ul> <p>Performance:</p> <pre><code>import time\nfrom rs_document import Document\n\ndoc = Document(page_content=\"X\" * 1_000_000, metadata={})\n\nstart = time.time()\nchunks = doc.split_on_num_characters(1000)\nelapsed = time.time() - start\n\nprint(f\"Split into {len(chunks)} chunks in {elapsed:.3f} seconds\")\n# Very fast - simpler algorithm than recursive splitter\n</code></pre> <p>When to Use:</p> <p>Choose <code>split_on_num_characters()</code> when:</p> <ul> <li>Exact chunk sizes are required</li> <li>Semantic boundaries are not important</li> <li>You need predictable, uniform chunks</li> <li>You're processing text that doesn't have natural structure</li> </ul> <p>Choose <code>recursive_character_splitter()</code> when:</p> <ul> <li>Semantic coherence matters</li> <li>You need context across chunks (overlap)</li> <li>Natural language boundaries should be preserved</li> <li>You're building RAG or search applications</li> </ul>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#method-comparison","level":2,"title":"Method Comparison","text":"","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#feature-comparison-table","level":3,"title":"Feature Comparison Table","text":"Feature <code>recursive_character_splitter()</code> <code>split_on_num_characters()</code> Chunk Size Target (may be smaller) Exact (except last chunk) Overlap Yes (~33% hardcoded) No Boundary Respect Yes (paragraph → line → word → char) No Speed Fast Very fast Predictability Chunk sizes vary Chunk sizes fixed Context Preservation Good (overlap) None Use Case RAG, semantic search, QA Token limits, uniform processing Best For Natural language text Any text, fixed requirements","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#visual-comparison","level":3,"title":"Visual Comparison","text":"<p><code>recursive_character_splitter(chunk_size=20)</code>:</p> <pre><code>Original: \"The quick brown fox jumps over the lazy dog\"\n\nChunk 1: \"The quick brown\"       (15 chars)\nChunk 2: \"brown fox jumps\"       (15 chars) - overlap: \"brown\"\nChunk 3: \"jumps over the\"        (14 chars) - overlap: \"jumps\"\nChunk 4: \"the lazy dog\"          (12 chars) - overlap: \"the\"\n</code></pre> <p><code>split_on_num_characters(num_chars=20)</code>:</p> <pre><code>Original: \"The quick brown fox jumps over the lazy dog\"\n\nChunk 1: \"The quick brown fox \"  (20 chars)\nChunk 2: \"jumps over the lazy \"  (20 chars)\nChunk 3: \"dog\"                   (3 chars)\n</code></pre>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#common-patterns","level":2,"title":"Common Patterns","text":"","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#basic-splitting","level":3,"title":"Basic Splitting","text":"<pre><code>from rs_document import Document\n\n# Create document\ndoc = Document(page_content=long_text, metadata={\"source\": \"doc.txt\"})\n\n# Split for RAG\nchunks = doc.recursive_character_splitter(1000)\n\n# Or split uniformly\nchunks = doc.split_on_num_characters(1000)\n</code></pre>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#clean-then-split","level":3,"title":"Clean Then Split","text":"<pre><code>doc = Document(page_content=pdf_text, metadata={\"source\": \"doc.pdf\"})\n\n# Clean first\ndoc.clean()\n\n# Then split\nchunks = doc.recursive_character_splitter(1000)\n</code></pre>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#preserving-original","level":3,"title":"Preserving Original","text":"<pre><code># Original document is not modified by splitting\ndoc = Document(page_content=\"Original text\", metadata={})\nchunks = doc.recursive_character_splitter(100)\n\nprint(doc.page_content)  # Still \"Original text\"\nprint(len(chunks))  # 1\n</code></pre>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#different-chunk-sizes","level":3,"title":"Different Chunk Sizes","text":"<pre><code>doc = Document(page_content=long_text, metadata={})\n\n# Try different sizes\nsmall_chunks = doc.recursive_character_splitter(500)   # More chunks\nmedium_chunks = doc.recursive_character_splitter(1000)  # Balanced\nlarge_chunks = doc.recursive_character_splitter(2000)  # Fewer chunks\n\nprint(f\"500: {len(small_chunks)} chunks\")\nprint(f\"1000: {len(medium_chunks)} chunks\")\nprint(f\"2000: {len(large_chunks)} chunks\")\n</code></pre>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#metadata-tracking","level":3,"title":"Metadata Tracking","text":"<pre><code>doc = Document(\n    page_content=long_text,\n    metadata={\"source\": \"doc.txt\", \"page\": \"5\", \"section\": \"intro\"}\n)\n\nchunks = doc.recursive_character_splitter(1000)\n\n# All chunks have same metadata\nfor i, chunk in enumerate(chunks):\n    # Could add chunk index to metadata\n    chunk.metadata[\"chunk_index\"] = str(i)\n    chunk.metadata[\"total_chunks\"] = str(len(chunks))\n\n    print(chunk.metadata)\n    # {\"source\": \"doc.txt\", \"page\": \"5\", \"section\": \"intro\",\n    #  \"chunk_index\": \"0\", \"total_chunks\": \"8\"}\n</code></pre>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#multiple-document-splitting","level":3,"title":"Multiple Document Splitting","text":"<pre><code>documents = [\n    Document(page_content=text1, metadata={\"id\": \"1\"}),\n    Document(page_content=text2, metadata={\"id\": \"2\"}),\n    Document(page_content=text3, metadata={\"id\": \"3\"}),\n]\n\nall_chunks = []\nfor doc in documents:\n    chunks = doc.recursive_character_splitter(1000)\n    all_chunks.extend(chunks)\n\nprint(f\"Split {len(documents)} documents into {len(all_chunks)} chunks\")\n</code></pre> <p>Better approach: Use <code>clean_and_split_docs()</code> for parallel processing:</p> <pre><code>from rs_document import clean_and_split_docs\n\nall_chunks = clean_and_split_docs(documents, chunk_size=1000)\n# Faster - processes in parallel\n</code></pre>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/splitting-methods/#see-also","level":2,"title":"See Also","text":"<ul> <li>Document Class - Creating documents</li> <li>Cleaning Methods - Clean before splitting</li> <li>Utility Functions - Batch splitting with <code>clean_and_split_docs()</code></li> <li>Types and Constants - Splitter defaults and constants</li> </ul>","path":["Reference","Splitting Methods"],"tags":[]},{"location":"reference/types-and-constants/","level":1,"title":"Types and Constants","text":"<p>Type information, constants, default values, and error handling for rs_document.</p>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#type-signatures","level":2,"title":"Type Signatures","text":"<p>Complete type signatures for all public APIs. These are provided for type checkers (mypy, pyright) and IDE autocompletion.</p>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#document-class","level":3,"title":"Document Class","text":"<pre><code>from rs_document import Document\n\nclass Document:\n    \"\"\"A document with content and metadata.\"\"\"\n\n    def __init__(\n        self,\n        page_content: str,\n        metadata: dict[str, str]\n    ) -&gt; None:\n        \"\"\"\n        Create a new Document.\n\n        Args:\n            page_content: The text content of the document\n            metadata: Dictionary of string key-value pairs\n        \"\"\"\n        ...\n\n    # Attributes\n    page_content: str\n    metadata: dict[str, str]\n\n    # Cleaning methods\n    def clean(self) -&gt; None:\n        \"\"\"Run all cleaning operations.\"\"\"\n        ...\n\n    def clean_non_ascii_chars(self) -&gt; None:\n        \"\"\"Remove all non-ASCII characters.\"\"\"\n        ...\n\n    def clean_bullets(self) -&gt; None:\n        \"\"\"Remove bullet point characters.\"\"\"\n        ...\n\n    def clean_ligatures(self) -&gt; None:\n        \"\"\"Convert typographic ligatures to component characters.\"\"\"\n        ...\n\n    def clean_extra_whitespace(self) -&gt; None:\n        \"\"\"Normalize whitespace.\"\"\"\n        ...\n\n    def group_broken_paragraphs(self) -&gt; None:\n        \"\"\"Join paragraphs incorrectly split across lines.\"\"\"\n        ...\n\n    # Splitting methods\n    def recursive_character_splitter(\n        self,\n        chunk_size: int\n    ) -&gt; list[Document]:\n        \"\"\"\n        Split document with recursive strategy.\n\n        Args:\n            chunk_size: Target size for each chunk in characters\n\n        Returns:\n            List of Document instances (chunks)\n        \"\"\"\n        ...\n\n    def split_on_num_characters(\n        self,\n        num_chars: int\n    ) -&gt; list[Document]:\n        \"\"\"\n        Split document into fixed-size chunks.\n\n        Args:\n            num_chars: Number of characters per chunk\n\n        Returns:\n            List of Document instances (chunks)\n        \"\"\"\n        ...\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#utility-functions","level":3,"title":"Utility Functions","text":"<pre><code>from rs_document import clean_and_split_docs\n\ndef clean_and_split_docs(\n    documents: list[Document],\n    chunk_size: int\n) -&gt; list[Document]:\n    \"\"\"\n    Process multiple documents in parallel: clean and split.\n\n    Args:\n        documents: List of documents to process\n        chunk_size: Target size for chunks in characters\n\n    Returns:\n        Flattened list of all chunks from all documents\n    \"\"\"\n    ...\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#type-checking","level":2,"title":"Type Checking","text":"","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#using-with-mypy","level":3,"title":"Using with mypy","text":"<pre><code>from rs_document import Document, clean_and_split_docs\n\n# Type checker knows the types\ndoc: Document = Document(\n    page_content=\"text\",\n    metadata={\"key\": \"value\"}\n)\n\n# Type checker validates method calls\ndoc.clean()  # OK - returns None\nchunks: list[Document] = doc.recursive_character_splitter(1000)  # OK\n\n# Type checker catches errors\ndoc.clean_non_ascii_chars(123)  # Error: takes no arguments\nbad_doc = Document(123, {})  # Error: page_content must be str\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#using-with-pyrightpylance","level":3,"title":"Using with pyright/pylance","text":"<pre><code>from rs_document import Document\n\ndef process_document(doc: Document) -&gt; list[Document]:\n    \"\"\"Type-safe document processing.\"\"\"\n    doc.clean()\n    return doc.recursive_character_splitter(1000)\n\n# IDE provides autocomplete and type checking\nresult = process_document(my_doc)\nreveal_type(result)  # list[Document]\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#constants","level":2,"title":"Constants","text":"","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#recursive-splitter-separators","level":3,"title":"Recursive Splitter Separators","text":"<p>The <code>recursive_character_splitter()</code> method uses these separators in order of preference:</p> Order Separator Description Unicode 1 <code>\"\\n\\n\"</code> Paragraph breaks (double newline) U+000A U+000A 2 <code>\"\\n\"</code> Line breaks (single newline) U+000A 3 <code>\" \"</code> Word boundaries (space) U+0020 4 <code>\"\"</code> Character-by-character (fallback) - <p>Note: These separators are hardcoded and cannot be customized.</p> <p>Example:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"Paragraph 1\\n\\nParagraph 2\\n\\nParagraph 3\",\n    metadata={}\n)\n\n# Will try to split on \"\\n\\n\" first\nchunks = doc.recursive_character_splitter(50)\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#chunk-overlap","level":3,"title":"Chunk Overlap","text":"<p>The <code>recursive_character_splitter()</code> uses approximately 33% overlap between consecutive chunks.</p> <p>Calculation:</p> <pre><code>overlap_size = chunk_size / 3  (integer division)\n</code></pre> <p>Example:</p> <pre><code># chunk_size = 1000\n# overlap_size = 1000 / 3 = 333 characters\n\ndoc = Document(page_content=\"A\" * 3000, metadata={})\nchunks = doc.recursive_character_splitter(1000)\n\n# Chunk 0: characters 0-1000\n# Chunk 1: characters 667-1667 (333 char overlap with chunk 0)\n# Chunk 2: characters 1334-2334 (333 char overlap with chunk 1)\n# etc.\n</code></pre> <p>Note: The overlap percentage is hardcoded and cannot be customized.</p>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#cleaned-characters","level":3,"title":"Cleaned Characters","text":"","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#bullet-characters","level":4,"title":"Bullet Characters","text":"<p>Characters removed by <code>clean_bullets()</code>:</p> Character Description Unicode <code>●</code> Black Circle U+25CF <code>○</code> White Circle U+25CB <code>■</code> Black Square U+25A0 <code>□</code> White Square U+25A1 <code>•</code> Bullet U+2022 <code>◦</code> White Bullet U+25E6 <code>▪</code> Black Small Square U+25AA <code>▫</code> White Small Square U+25AB <code>‣</code> Triangular Bullet U+2023 <code>⁃</code> Hyphen Bullet U+2043","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#ligature-conversions","level":4,"title":"Ligature Conversions","text":"<p>Ligatures converted by <code>clean_ligatures()</code>:</p> Ligature Converts To Unicode <code>æ</code> <code>ae</code> U+00E6 <code>Æ</code> <code>AE</code> U+00C6 <code>œ</code> <code>oe</code> U+0153 <code>Œ</code> <code>OE</code> U+0152 <code>ﬁ</code> <code>fi</code> U+FB01 <code>ﬂ</code> <code>fl</code> U+FB02 <code>ﬀ</code> <code>ff</code> U+FB00 <code>ﬃ</code> <code>ffi</code> U+FB03 <code>ﬄ</code> <code>ffl</code> U+FB04 <code>ﬅ</code> <code>ft</code> U+FB05 <code>ﬆ</code> <code>st</code> U+FB06","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#error-handling","level":2,"title":"Error Handling","text":"","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#empty-documents","level":3,"title":"Empty Documents","text":"<p>All methods handle empty documents gracefully without raising errors:</p> <pre><code>from rs_document import Document\n\ndoc = Document(page_content=\"\", metadata={})\n\n# Cleaning methods - no error\ndoc.clean()  # No-op, returns None\ndoc.clean_non_ascii_chars()  # No-op, returns None\ndoc.clean_bullets()  # No-op, returns None\ndoc.clean_ligatures()  # No-op, returns None\ndoc.clean_extra_whitespace()  # No-op, returns None\ndoc.group_broken_paragraphs()  # No-op, returns None\n\n# Splitting methods - return empty list\nchunks = doc.recursive_character_splitter(1000)\nprint(chunks)  # []\n\nchunks = doc.split_on_num_characters(100)\nprint(chunks)  # []\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#invalid-metadata-types","level":3,"title":"Invalid Metadata Types","text":"<p>Metadata must contain only string keys and string values. Non-string values may cause errors.</p> <p>Correct Usage:</p> <pre><code># All strings - correct\ndoc = Document(\n    page_content=\"text\",\n    metadata={\"id\": \"123\", \"count\": \"456\", \"active\": \"True\"}\n)\n</code></pre> <p>Incorrect Usage:</p> <pre><code># Non-string values - may cause errors\ndoc = Document(\n    page_content=\"text\",\n    metadata={\"id\": 123, \"active\": True}  # ERROR: int and bool\n)\n</code></pre> <p>Solution - Convert to Strings:</p> <pre><code>raw_metadata = {\n    \"id\": 123,\n    \"page\": 5,\n    \"active\": True,\n    \"score\": 98.6,\n    \"tags\": [\"a\", \"b\"],\n}\n\n# Convert all values to strings\ndoc = Document(\n    page_content=\"text\",\n    metadata={k: str(v) for k, v in raw_metadata.items()}\n)\n\nprint(doc.metadata)\n# {\"id\": \"123\", \"page\": \"5\", \"active\": \"True\",\n#  \"score\": \"98.6\", \"tags\": \"['a', 'b']\"}\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#invalid-parameters","level":3,"title":"Invalid Parameters","text":"<p>Methods validate parameters and raise exceptions for invalid inputs:</p> <pre><code>from rs_document import Document\n\ndoc = Document(page_content=\"text\", metadata={})\n\n# Invalid chunk_size - negative\ntry:\n    chunks = doc.recursive_character_splitter(-100)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n\n# Invalid chunk_size - zero\ntry:\n    chunks = doc.split_on_num_characters(0)\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n\n# Wrong type for chunk_size\ntry:\n    chunks = doc.recursive_character_splitter(\"1000\")  # string instead of int\nexcept TypeError as e:\n    print(f\"Error: {e}\")\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#empty-lists","level":3,"title":"Empty Lists","text":"<p><code>clean_and_split_docs()</code> handles empty lists:</p> <pre><code>from rs_document import clean_and_split_docs\n\n# Empty list - returns empty list\nchunks = clean_and_split_docs([], chunk_size=1000)\nprint(chunks)  # []\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#default-values","level":2,"title":"Default Values","text":"","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#method-defaults","level":3,"title":"Method Defaults","text":"<p>All cleaning and splitting methods have no optional parameters. All parameters shown are required:</p> <pre><code># Cleaning methods - no parameters\ndoc.clean()\ndoc.clean_non_ascii_chars()\ndoc.clean_bullets()\ndoc.clean_ligatures()\ndoc.clean_extra_whitespace()\ndoc.group_broken_paragraphs()\n\n# Splitting methods - required parameters\nchunks = doc.recursive_character_splitter(chunk_size=1000)  # chunk_size required\nchunks = doc.split_on_num_characters(num_chars=500)  # num_chars required\n\n# Utility function - required parameters\nfrom rs_document import clean_and_split_docs\nchunks = clean_and_split_docs(documents, chunk_size=1000)  # both required\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#hardcoded-values","level":3,"title":"Hardcoded Values","text":"<p>Values that cannot be customized:</p> Feature Value Location Recursive splitter separators <code>[\"\\n\\n\", \"\\n\", \" \", \"\"]</code> <code>recursive_character_splitter()</code> Chunk overlap ~33% (chunk_size / 3) <code>recursive_character_splitter()</code> Cleaning order See <code>clean()</code> method <code>clean()</code> Bullet characters See table above <code>clean_bullets()</code> Ligatures See table above <code>clean_ligatures()</code>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#platform-support","level":2,"title":"Platform Support","text":"","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#python-version-requirements","level":3,"title":"Python Version Requirements","text":"<ul> <li>Minimum: Python 3.10</li> <li>Recommended: Python 3.11 or higher for best performance</li> <li>Type hints: Use modern type hint syntax (PEP 604 union types)</li> </ul> <p>Python Version Type Hints:</p> <pre><code># Python 3.10+ syntax (used in rs_document)\ndef process(docs: list[Document]) -&gt; dict[str, str]:\n    ...\n\n# Older syntax (also supported)\nfrom typing import List, Dict\ndef process(docs: List[Document]) -&gt; Dict[str, str]:\n    ...\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#pre-built-wheels","level":3,"title":"Pre-built Wheels","text":"<p>Pre-built binary wheels are available for:</p> <p>Linux:</p> <ul> <li>x86_64 (64-bit Intel/AMD)</li> <li>aarch64 (64-bit ARM)</li> <li>armv7 (32-bit ARM)</li> <li>i686 (32-bit Intel)</li> <li>s390x (IBM Z)</li> <li>ppc64le (PowerPC)</li> </ul> <p>macOS:</p> <ul> <li>x86_64 (Intel Macs)</li> <li>aarch64 (Apple Silicon M1/M2/M3)</li> </ul> <p>Windows:</p> <ul> <li>x64 (64-bit)</li> <li>x86 (32-bit)</li> </ul> <p>Installation:</p> <pre><code># Most platforms - uses pre-built wheel\npip install rs-document\n\n# If wheel not available - compiles from source (requires Rust)\npip install rs-document\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#langchain-compatibility","level":2,"title":"LangChain Compatibility","text":"","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#similarities","level":3,"title":"Similarities","text":"<p>rs_document's <code>Document</code> class is designed to be compatible with LangChain:</p> <pre><code># Both use same attribute names\nfrom rs_document import Document as RSDocument\nfrom langchain_core.documents import Document as LCDocument\n\nrs_doc = RSDocument(page_content=\"text\", metadata={\"key\": \"value\"})\nlc_doc = LCDocument(page_content=\"text\", metadata={\"key\": \"value\"})\n\n# Both have same attributes\nprint(rs_doc.page_content)  # \"text\"\nprint(lc_doc.page_content)  # \"text\"\n\nprint(rs_doc.metadata)  # {\"key\": \"value\"}\nprint(lc_doc.metadata)  # {\"key\": \"value\"}\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#differences","level":3,"title":"Differences","text":"Feature rs_document LangChain Metadata values Must be strings Any Python object Metadata keys Must be strings Any hashable object Performance High (Rust) Standard (Python) Methods Cleaning + splitting Minimal (constructor only)","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#conversion","level":3,"title":"Conversion","text":"<p>From LangChain to rs_document:</p> <pre><code>from langchain_core.documents import Document as LCDocument\nfrom rs_document import Document as RSDocument\n\nlc_doc = LCDocument(\n    page_content=\"text\",\n    metadata={\"id\": 123, \"active\": True, \"tags\": [\"a\", \"b\"]}\n)\n\n# Convert - stringify all metadata\nrs_doc = RSDocument(\n    page_content=lc_doc.page_content,\n    metadata={k: str(v) for k, v in lc_doc.metadata.items()}\n)\n</code></pre> <p>From rs_document to LangChain:</p> <pre><code>from rs_document import Document as RSDocument\nfrom langchain_core.documents import Document as LCDocument\n\nrs_doc = RSDocument(\n    page_content=\"text\",\n    metadata={\"id\": \"123\", \"page\": \"5\"}\n)\n\n# Convert - direct copy (already strings)\nlc_doc = LCDocument(\n    page_content=rs_doc.page_content,\n    metadata=rs_doc.metadata\n)\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#integration-example","level":3,"title":"Integration Example","text":"<pre><code>from langchain_core.documents import Document as LCDocument\nfrom rs_document import Document as RSDocument, clean_and_split_docs\n\n# Start with LangChain documents\nlc_documents = [\n    LCDocument(page_content=text, metadata={\"source\": f\"doc{i}.txt\"})\n    for i, text in enumerate(texts)\n]\n\n# Convert to rs_document for processing\nrs_documents = [\n    RSDocument(\n        page_content=doc.page_content,\n        metadata={k: str(v) for k, v in doc.metadata.items()}\n    )\n    for doc in lc_documents\n]\n\n# Process with rs_document (fast)\nchunks = clean_and_split_docs(rs_documents, chunk_size=1000)\n\n# Convert back to LangChain\nlc_chunks = [\n    LCDocument(\n        page_content=chunk.page_content,\n        metadata=chunk.metadata\n    )\n    for chunk in chunks\n]\n\n# Now use with LangChain tools\n# vectorstore.add_documents(lc_chunks)\n</code></pre>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/types-and-constants/#see-also","level":2,"title":"See Also","text":"<ul> <li>Document Class - Document class reference</li> <li>Cleaning Methods - Cleaning method details</li> <li>Splitting Methods - Splitting method details</li> <li>Utility Functions - Batch processing utilities</li> </ul>","path":["Reference","Types and Constants"],"tags":[]},{"location":"reference/utility-functions/","level":1,"title":"Utility Functions","text":"<p>Utility functions for processing multiple documents efficiently.</p>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#overview","level":2,"title":"Overview","text":"<p>The <code>clean_and_split_docs()</code> function provides high-performance batch processing for multiple documents using parallel execution.</p> <pre><code>from rs_document import Document, clean_and_split_docs\n\n# Create many documents\ndocuments = [\n    Document(page_content=text, metadata={\"id\": str(i)})\n    for i, text in enumerate(text_list)\n]\n\n# Process all at once\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#functions","level":2,"title":"Functions","text":"","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#clean_and_split_docs","level":3,"title":"<code>clean_and_split_docs()</code>","text":"<p>Process multiple documents in parallel: clean and split all at once.</p> <p>Signature:</p> <pre><code>clean_and_split_docs(\n    documents: list[Document],\n    chunk_size: int\n) -&gt; list[Document]\n</code></pre> <p>Description:</p> <p>Processes a list of documents by cleaning and splitting each one, using parallel execution across all available CPU cores. Returns a flattened list of all resulting chunks.</p> <p>Parameters:</p> <ul> <li><code>documents</code> (<code>list[Document]</code>): List of Document instances to process</li> <li><code>chunk_size</code> (<code>int</code>): Target size for chunks in characters (used for <code>recursive_character_splitter()</code>)</li> </ul> <p>Returns:</p> <p><code>list[Document]</code> - Flattened list containing all chunks from all input documents</p> <p>Processing Steps:</p> <p>For each document in the input list:</p> <ol> <li>Runs <code>document.clean()</code> (applies all cleaning operations)</li> <li>Splits with <code>document.recursive_character_splitter(chunk_size)</code></li> <li>Processes documents in parallel using all CPU cores</li> <li>Returns flattened list of all resulting chunks</li> </ol> <p>Example:</p> <pre><code>from rs_document import Document, clean_and_split_docs\n\n# Create multiple documents\ndocuments = [\n    Document(\n        page_content=f\"Document {i} content \" * 1000,\n        metadata={\"doc_id\": str(i), \"source\": f\"file_{i}.txt\"}\n    )\n    for i in range(100)\n]\n\n# Process all documents in parallel\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\nprint(f\"Processed {len(documents)} documents\")\nprint(f\"Created {len(chunks)} total chunks\")\n\n# Chunks are flattened - all mixed together\nfor chunk in chunks[:5]:  # First 5 chunks\n    print(f\"Source: {chunk.metadata['source']}, Length: {len(chunk.page_content)}\")\n</code></pre> <p>Large Batch Processing:</p> <pre><code>import time\nfrom rs_document import Document, clean_and_split_docs\n\n# Simulate loading many documents\ndocuments = []\nfor i in range(10_000):\n    doc = Document(\n        page_content=\"Sample text \" * 500,  # ~6000 characters\n        metadata={\"id\": str(i), \"batch\": \"1\"}\n    )\n    documents.append(doc)\n\n# Process with timing\nstart = time.time()\nchunks = clean_and_split_docs(documents, chunk_size=1000)\nelapsed = time.time() - start\n\nprint(f\"Processed {len(documents):,} documents\")\nprint(f\"Created {len(chunks):,} chunks\")\nprint(f\"Time: {elapsed:.2f} seconds\")\nprint(f\"Throughput: {len(documents) / elapsed:,.0f} docs/sec\")\n\n# Expected output on typical hardware:\n# Processed 10,000 documents\n# Created 60,000 chunks\n# Time: 0.43 seconds\n# Throughput: 23,000 docs/sec\n</code></pre> <p>Edge Cases:</p> <pre><code># Empty list\nchunks = clean_and_split_docs([], chunk_size=1000)\nprint(chunks)  # []\n\n# Single document\ndoc = Document(page_content=\"Single doc\", metadata={})\nchunks = clean_and_split_docs([doc], chunk_size=1000)\nprint(len(chunks))  # 1\n\n# Documents with varying sizes\ndocuments = [\n    Document(page_content=\"Short\", metadata={\"id\": \"1\"}),\n    Document(page_content=\"A\" * 10000, metadata={\"id\": \"2\"}),\n    Document(page_content=\"\", metadata={\"id\": \"3\"}),  # Empty\n]\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n# Returns varying number of chunks per document\n</code></pre> <p>Metadata Preservation:</p> <pre><code>documents = [\n    Document(\n        page_content=\"Long text \" * 1000,\n        metadata={\"source\": \"doc1.txt\", \"author\": \"Alice\", \"date\": \"2024-01-01\"}\n    ),\n    Document(\n        page_content=\"More text \" * 1000,\n        metadata={\"source\": \"doc2.txt\", \"author\": \"Bob\", \"date\": \"2024-01-02\"}\n    ),\n]\n\nchunks = clean_and_split_docs(documents, chunk_size=500)\n\n# Each chunk retains its parent document's metadata\nfor chunk in chunks:\n    print(f\"Source: {chunk.metadata['source']}, Author: {chunk.metadata['author']}\")\n</code></pre> <p>Comparison with Sequential Processing:</p> <pre><code>import time\nfrom rs_document import Document, clean_and_split_docs\n\n# Create test documents\ndocuments = [\n    Document(page_content=\"Text \" * 1000, metadata={\"id\": str(i)})\n    for i in range(1000)\n]\n\n# Method 1: Sequential (manual loop)\nstart = time.time()\nsequential_chunks = []\nfor doc in documents:\n    doc.clean()\n    chunks = doc.recursive_character_splitter(1000)\n    sequential_chunks.extend(chunks)\nsequential_time = time.time() - start\n\n# Method 2: Parallel (clean_and_split_docs)\nstart = time.time()\nparallel_chunks = clean_and_split_docs(documents, chunk_size=1000)\nparallel_time = time.time() - start\n\nprint(f\"Sequential: {sequential_time:.2f}s\")\nprint(f\"Parallel: {parallel_time:.2f}s\")\nprint(f\"Speedup: {sequential_time / parallel_time:.1f}x\")\n\n# Expected results:\n# Sequential: 0.50s\n# Parallel: 0.02s\n# Speedup: 25.0x\n</code></pre>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#performance-characteristics","level":2,"title":"Performance Characteristics","text":"","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#parallel-processing","level":3,"title":"Parallel Processing","text":"<p><code>clean_and_split_docs()</code> uses Rayon (Rust's parallelism library) to process documents in parallel:</p> <ul> <li>CPU cores: Automatically uses all available CPU cores</li> <li>Work stealing: Efficient load balancing across threads</li> <li>No GIL: True parallelism (not limited by Python's Global Interpreter Lock)</li> </ul>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#benchmarks","level":3,"title":"Benchmarks","text":"<p>Typical performance on modern hardware (8-core CPU):</p> Documents Avg Size Chunk Size Time Throughput 100 5 KB 1000 0.004s 25,000 docs/s 1,000 5 KB 1000 0.04s 25,000 docs/s 10,000 5 KB 1000 0.43s 23,000 docs/s 100,000 5 KB 1000 4.3s 23,000 docs/s <p>Key observations:</p> <ul> <li>Near-linear scaling with document count</li> <li>20-25x faster than pure Python implementations</li> <li>Minimal overhead for small batches</li> <li>Consistent throughput across batch sizes</li> </ul>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#memory-usage","level":3,"title":"Memory Usage","text":"<p>Memory consumption is proportional to:</p> <ul> <li>Total size of input documents</li> <li>Number of resulting chunks</li> <li>Chunk overlap (~33%)</li> </ul> <p>Example:</p> <pre><code># Estimate memory for large batch\nnum_docs = 100_000\navg_doc_size = 5_000  # bytes\nchunk_size = 1_000\nchunks_per_doc = 7  # Approximate with overlap\n\ninput_memory = num_docs * avg_doc_size  # 500 MB\noutput_memory = num_docs * chunks_per_doc * chunk_size  # 700 MB\ntotal_memory = input_memory + output_memory  # ~1.2 GB\n\nprint(f\"Estimated memory: {total_memory / 1e9:.1f} GB\")\n</code></pre> <p>Memory optimization tips:</p> <ul> <li>Process in batches if total memory is a concern</li> <li>Clear input documents after processing if not needed</li> <li>Consider chunk size to control output size</li> </ul>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#optimization-tips","level":3,"title":"Optimization Tips","text":"<p>Batch Size:</p> <pre><code># For very large datasets, process in batches\ndef process_large_dataset(all_documents, batch_size=10_000):\n    all_chunks = []\n\n    for i in range(0, len(all_documents), batch_size):\n        batch = all_documents[i:i + batch_size]\n        chunks = clean_and_split_docs(batch, chunk_size=1000)\n        all_chunks.extend(chunks)\n\n        print(f\"Processed batch {i // batch_size + 1}\")\n\n    return all_chunks\n\n# Process 100,000 documents in batches of 10,000\nchunks = process_large_dataset(huge_document_list)\n</code></pre> <p>Chunk Size Selection:</p> <pre><code># Smaller chunks = more total chunks = more memory\nchunks_small = clean_and_split_docs(docs, chunk_size=500)   # More chunks\n\n# Larger chunks = fewer total chunks = less memory\nchunks_large = clean_and_split_docs(docs, chunk_size=2000)  # Fewer chunks\n\nprint(f\"Small chunks: {len(chunks_small)}\")\nprint(f\"Large chunks: {len(chunks_large)}\")\n</code></pre> <p>Metadata Optimization:</p> <pre><code># Keep metadata minimal to reduce memory\ndocuments = [\n    Document(\n        page_content=text,\n        metadata={\"id\": str(i)}  # Only essential metadata\n    )\n    for i, text in enumerate(texts)\n]\n\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#common-patterns","level":2,"title":"Common Patterns","text":"","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#basic-batch-processing","level":3,"title":"Basic Batch Processing","text":"<pre><code>from rs_document import Document, clean_and_split_docs\n\n# Load documents\ndocuments = [\n    Document(page_content=load_file(f), metadata={\"source\": f})\n    for f in file_list\n]\n\n# Process all at once\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#processing-with-progress-tracking","level":3,"title":"Processing with Progress Tracking","text":"<pre><code>from rs_document import Document, clean_and_split_docs\n\ndef process_with_progress(documents, chunk_size, batch_size=1000):\n    \"\"\"Process documents in batches with progress tracking.\"\"\"\n    all_chunks = []\n    total = len(documents)\n\n    for i in range(0, total, batch_size):\n        batch = documents[i:i + batch_size]\n        chunks = clean_and_split_docs(batch, chunk_size)\n        all_chunks.extend(chunks)\n\n        processed = min(i + batch_size, total)\n        print(f\"Progress: {processed}/{total} documents ({100*processed/total:.1f}%)\")\n\n    return all_chunks\n\n# Use it\nchunks = process_with_progress(huge_list, chunk_size=1000, batch_size=5000)\n</code></pre>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#from-files-to-chunks","level":3,"title":"From Files to Chunks","text":"<pre><code>import os\nfrom rs_document import Document, clean_and_split_docs\n\ndef process_directory(directory, chunk_size=1000):\n    \"\"\"Load all text files from directory and process them.\"\"\"\n    documents = []\n\n    for filename in os.listdir(directory):\n        if filename.endswith('.txt'):\n            path = os.path.join(directory, filename)\n            with open(path, 'r', encoding='utf-8') as f:\n                content = f.read()\n\n            doc = Document(\n                page_content=content,\n                metadata={\"source\": filename, \"path\": path}\n            )\n            documents.append(doc)\n\n    print(f\"Loaded {len(documents)} documents\")\n    chunks = clean_and_split_docs(documents, chunk_size)\n    print(f\"Created {len(chunks)} chunks\")\n\n    return chunks\n\n# Process entire directory\nchunks = process_directory('/path/to/documents')\n</code></pre>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#integration-with-vector-stores","level":3,"title":"Integration with Vector Stores","text":"<pre><code>from rs_document import Document, clean_and_split_docs\n\ndef prepare_for_embedding(documents, chunk_size=1000):\n    \"\"\"Prepare documents for embedding in vector store.\"\"\"\n    # Clean and split\n    chunks = clean_and_split_docs(documents, chunk_size)\n\n    # Add chunk identifiers\n    for i, chunk in enumerate(chunks):\n        chunk.metadata[\"chunk_id\"] = str(i)\n        chunk.metadata[\"char_count\"] = str(len(chunk.page_content))\n\n    return chunks\n\n# Prepare for vector database\ndocuments = [...]  # Your documents\nchunks = prepare_for_embedding(documents)\n\n# Now ready for embedding\n# embeddings = embed_model.embed_documents([c.page_content for c in chunks])\n# vector_store.add_documents(chunks, embeddings)\n</code></pre>","path":["Reference","Utility Functions"],"tags":[]},{"location":"reference/utility-functions/#see-also","level":2,"title":"See Also","text":"<ul> <li>Document Class - Creating Document instances</li> <li>Cleaning Methods - Individual cleaning operations</li> <li>Splitting Methods - Document splitting details</li> <li>Types and Constants - Type signatures and defaults</li> </ul>","path":["Reference","Utility Functions"],"tags":[]},{"location":"tutorial/","level":1,"title":"Tutorial: Getting Started with RS Document","text":"<p>Welcome! This tutorial will teach you how to use rs_document to clean and split text documents for RAG applications.</p>","path":["Tutorial"],"tags":[]},{"location":"tutorial/#what-youll-learn","level":2,"title":"What You'll Learn","text":"<p>By the end of this tutorial, you'll know how to:</p> <ul> <li>Install and set up rs_document</li> <li>Create and work with documents</li> <li>Clean messy text from various sources</li> <li>Split documents into chunks for embeddings</li> <li>Process multiple documents efficiently</li> </ul>","path":["Tutorial"],"tags":[]},{"location":"tutorial/#prerequisites","level":2,"title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>Basic familiarity with Python</li> <li>pip or uv package manager</li> </ul>","path":["Tutorial"],"tags":[]},{"location":"tutorial/#tutorial-structure","level":2,"title":"Tutorial Structure","text":"<p>Follow these pages in order:</p> <ol> <li>Installation - Set up rs_document</li> <li>First Document - Create and understand documents</li> <li>Cleaning Text - Remove artifacts and normalize text</li> <li>Splitting Documents - Break documents into chunks</li> <li>Batch Processing - Process multiple documents efficiently</li> </ol> <p>Each page builds on the previous one, so we recommend following them in sequence if you're new to rs_document.</p>","path":["Tutorial"],"tags":[]},{"location":"tutorial/#quick-start","level":2,"title":"Quick Start","text":"<p>If you want to jump right in:</p> <pre><code>pip install rs-document\n</code></pre> <pre><code>from rs_document import Document, clean_and_split_docs\n\n# Create a document\ndoc = Document(\n    page_content=\"Your text here...\",\n    metadata={\"source\": \"example.txt\"}\n)\n\n# Clean and split\ndoc.clean()\nchunks = doc.recursive_character_splitter(1000)\n</code></pre> <p>Now continue to Installation to get started!</p>","path":["Tutorial"],"tags":[]},{"location":"tutorial/batch-processing/","level":1,"title":"Batch Processing","text":"<p>Learn how to process many documents efficiently with parallel processing.</p>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#the-clean_and_split_docs-function","level":2,"title":"The clean_and_split_docs Function","text":"<p>Process multiple documents with one function call:</p> <pre><code>from rs_document import Document, clean_and_split_docs\n\n# Create multiple documents\ndocuments = [\n    Document(\n        page_content=f\"Document {i} content \" * 100,\n        metadata={\"doc_id\": str(i)}\n    )\n    for i in range(100)\n]\n\n# Clean and split all at once\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\nprint(f\"Processed {len(documents)} documents\")\nprint(f\"Created {len(chunks)} chunks\")\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#what-it-does","level":3,"title":"What It Does","text":"<p><code>clean_and_split_docs</code> performs these steps in parallel:</p> <ol> <li>Runs <code>.clean()</code> on each document</li> <li>Splits each with <code>.recursive_character_splitter(chunk_size)</code></li> <li>Returns a flat list of all chunks</li> </ol>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#why-use-batch-processing","level":2,"title":"Why Use Batch Processing?","text":"","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#performance","level":3,"title":"Performance","text":"<p>Processing documents individually:</p> <pre><code># Slow - processes sequentially\nall_chunks = []\nfor doc in documents:\n    doc.clean()\n    chunks = doc.recursive_character_splitter(1000)\n    all_chunks.extend(chunks)\n</code></pre> <p>Using batch processing:</p> <pre><code># Fast - processes in parallel\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre> <p>Result: 20-25x faster, processing ~23,000 documents/second on typical hardware.</p>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#simplicity","level":3,"title":"Simplicity","text":"<p>One function call instead of a loop:</p> <pre><code># Before\nall_chunks = []\nfor doc in documents:\n    doc.clean()\n    chunks = doc.recursive_character_splitter(1000)\n    all_chunks.extend(chunks)\n\n# After\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#complete-example","level":2,"title":"Complete Example","text":"<p>Process a directory of text files:</p> <pre><code>from pathlib import Path\nfrom rs_document import Document, clean_and_split_docs\n\n# Load all text files\ndocuments = []\nfor file_path in Path(\"./documents\").glob(\"*.txt\"):\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n\n    doc = Document(\n        page_content=content,\n        metadata={\n            \"source\": file_path.name,\n            \"path\": str(file_path)\n        }\n    )\n    documents.append(doc)\n\nprint(f\"Loaded {len(documents)} documents\")\n\n# Process all at once\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\nprint(f\"Created {len(chunks)} chunks\")\nprint(f\"Average chunks per doc: {len(chunks) / len(documents):.1f}\")\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#tracking-progress","level":2,"title":"Tracking Progress","text":"<p>For large batches, show progress:</p> <pre><code>from rs_document import Document, clean_and_split_docs\nimport time\n\ndocuments = [...]  # Your documents\n\nprint(f\"Processing {len(documents)} documents...\")\nstart_time = time.time()\n\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\nelapsed = time.time() - start_time\ndocs_per_sec = len(documents) / elapsed\n\nprint(f\"Done in {elapsed:.2f}s ({docs_per_sec:.0f} docs/sec)\")\nprint(f\"Created {len(chunks)} chunks\")\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#working-with-results","level":2,"title":"Working with Results","text":"","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#group-chunks-by-source","level":3,"title":"Group Chunks by Source","text":"<p>Track which chunks came from which document:</p> <pre><code>from collections import defaultdict\n\n# Add unique IDs before processing\nfor i, doc in enumerate(documents):\n    doc.metadata[\"doc_id\"] = str(i)\n\n# Process\nchunks = clean_and_split_docs(documents, chunk_size=1000)\n\n# Group by source\nchunks_by_doc = defaultdict(list)\nfor chunk in chunks:\n    doc_id = chunk.metadata[\"doc_id\"]\n    chunks_by_doc[doc_id].append(chunk)\n\n# See distribution\nfor doc_id, doc_chunks in chunks_by_doc.items():\n    print(f\"Document {doc_id}: {len(doc_chunks)} chunks\")\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#filter-small-chunks","level":3,"title":"Filter Small Chunks","text":"<p>Remove chunks below a minimum size:</p> <pre><code>chunks = clean_and_split_docs(documents, chunk_size=1000)\n\n# Keep only chunks &gt;= 200 characters\nmin_size = 200\nfiltered = [c for c in chunks if len(c.page_content) &gt;= min_size]\n\nprint(f\"Kept {len(filtered)} of {len(chunks)} chunks\")\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#add-global-metadata","level":3,"title":"Add Global Metadata","text":"<p>Add metadata to all chunks:</p> <pre><code>chunks = clean_and_split_docs(documents, chunk_size=1000)\n\n# Add processing info to all chunks\nbatch_id = \"batch_001\"\nfor chunk in chunks:\n    chunk.metadata[\"batch_id\"] = batch_id\n    chunk.metadata[\"processed_at\"] = \"2024-01-01\"\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#performance-considerations","level":2,"title":"Performance Considerations","text":"","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#optimal-batch-size","level":3,"title":"Optimal Batch Size","text":"<p>Process documents in batches for best performance:</p> <pre><code>from rs_document import clean_and_split_docs\n\ndef process_in_batches(documents, chunk_size=1000, batch_size=1000):\n    \"\"\"Process documents in batches.\"\"\"\n    all_chunks = []\n\n    for i in range(0, len(documents), batch_size):\n        batch = documents[i:i + batch_size]\n        chunks = clean_and_split_docs(batch, chunk_size=chunk_size)\n        all_chunks.extend(chunks)\n\n        print(f\"Processed batch {i//batch_size + 1}: {len(chunks)} chunks\")\n\n    return all_chunks\n\n# Process 10,000 documents in batches of 1,000\ndocuments = [...]  # Your 10,000 documents\nchunks = process_in_batches(documents, chunk_size=1000, batch_size=1000)\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#memory-management","level":3,"title":"Memory Management","text":"<p>For very large datasets:</p> <pre><code>def process_and_save(documents, output_file, chunk_size=1000):\n    \"\"\"Process documents and save chunks incrementally.\"\"\"\n    import json\n\n    with open(output_file, \"w\") as f:\n        batch_size = 1000\n\n        for i in range(0, len(documents), batch_size):\n            batch = documents[i:i + batch_size]\n            chunks = clean_and_split_docs(batch, chunk_size=chunk_size)\n\n            # Save chunks to file\n            for chunk in chunks:\n                data = {\n                    \"content\": chunk.page_content,\n                    \"metadata\": chunk.metadata\n                }\n                f.write(json.dumps(data) + \"\\n\")\n\n            print(f\"Saved {len(chunks)} chunks from batch {i//batch_size + 1}\")\n\n# Use it\ndocuments = [...]  # Large list\nprocess_and_save(documents, \"chunks.jsonl\", chunk_size=1000)\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#integration-example","level":2,"title":"Integration Example","text":"<p>Complete pipeline for RAG:</p> <pre><code>from pathlib import Path\nfrom rs_document import Document, clean_and_split_docs\n\ndef prepare_documents_for_rag(directory, chunk_size=1000):\n    \"\"\"Load, clean, split, and prepare documents for RAG.\"\"\"\n\n    # 1. Load documents\n    print(\"Loading documents...\")\n    documents = []\n    for file_path in Path(directory).glob(\"**/*.txt\"):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n\n        doc = Document(\n            page_content=content,\n            metadata={\n                \"source\": file_path.name,\n                \"path\": str(file_path),\n                \"category\": file_path.parent.name\n            }\n        )\n        documents.append(doc)\n\n    print(f\"Loaded {len(documents)} documents\")\n\n    # 2. Clean and split\n    print(\"Processing...\")\n    chunks = clean_and_split_docs(documents, chunk_size=chunk_size)\n\n    # 3. Filter small chunks\n    min_size = chunk_size // 4\n    filtered = [c for c in chunks if len(c.page_content) &gt;= min_size]\n\n    # 4. Add chunk IDs\n    for i, chunk in enumerate(filtered):\n        chunk.metadata[\"chunk_id\"] = str(i)\n\n    print(f\"Created {len(filtered)} chunks\")\n    return filtered\n\n# Use it\nchunks = prepare_documents_for_rag(\"./my_documents\", chunk_size=1000)\n\n# Ready for embedding\ntexts = [c.page_content for c in chunks]\nmetadatas = [c.metadata for c in chunks]\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#benchmarking","level":2,"title":"Benchmarking","text":"<p>Measure your processing speed:</p> <pre><code>import time\nfrom rs_document import Document, clean_and_split_docs\n\n# Create test documents\nnum_docs = 1000\ndocuments = [\n    Document(\n        page_content=\"test content \" * 500,\n        metadata={\"id\": str(i)}\n    )\n    for i in range(num_docs)\n]\n\n# Benchmark\nstart = time.time()\nchunks = clean_and_split_docs(documents, chunk_size=1000)\nelapsed = time.time() - start\n\n# Results\ndocs_per_sec = num_docs / elapsed\nprint(f\"Processed {num_docs} documents in {elapsed:.2f}s\")\nprint(f\"Speed: {docs_per_sec:.0f} documents/second\")\nprint(f\"Created {len(chunks)} chunks\")\n</code></pre>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#summary","level":2,"title":"Summary","text":"<p>You've learned:</p> <ul> <li>✅ How to use <code>clean_and_split_docs</code> for batch processing</li> <li>✅ Why batch processing is faster (parallel processing)</li> <li>✅ How to track and group chunks by source</li> <li>✅ Performance optimization techniques</li> <li>✅ Complete RAG pipeline example</li> </ul>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/batch-processing/#next-steps","level":2,"title":"Next Steps","text":"<p>You've completed the tutorial! Here's what to explore next:</p> <ul> <li>How-To Guides - Solve specific problems</li> <li>API Reference - Look up exact method signatures</li> <li>Explanation - Understand design decisions</li> </ul> <p>Happy document processing! 🚀</p>","path":["Tutorial","Batch Processing"],"tags":[]},{"location":"tutorial/cleaning/","level":1,"title":"Cleaning Text","text":"<p>Learn how to clean messy text from PDFs, OCR, and web scraping.</p>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#why-clean-documents","level":2,"title":"Why Clean Documents?","text":"<p>Documents from various sources contain artifacts that hurt embedding quality:</p> <ul> <li>PDFs: Ligatures (æ, ﬁ), bullet symbols, extra spaces</li> <li>OCR: Non-ASCII artifacts, broken paragraphs</li> <li>Web: HTML entities, inconsistent whitespace</li> </ul> <p>Cleaning normalizes text so embeddings focus on semantic content.</p>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#clean-everything-at-once","level":2,"title":"Clean Everything at Once","text":"<p>The easiest way to clean a document:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"●  Text with bullets, æ ligatures, and  extra   spaces!\",\n    metadata={\"source\": \"messy.pdf\"}\n)\n\n# Run all cleaners\ndoc.clean()\n\nprint(doc.page_content)\n# Output: \"Text with bullets, ae ligatures, and extra spaces!\"\n</code></pre> <p>The <code>.clean()</code> method runs all cleaners in the right order.</p>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#individual-cleaners","level":2,"title":"Individual Cleaners","text":"<p>You can also use specific cleaners:</p>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#remove-extra-whitespace","level":3,"title":"Remove Extra Whitespace","text":"<p>Normalize spaces and remove trailing whitespace:</p> <pre><code>doc = Document(\n    page_content=\"ITEM 1.     BUSINESS \",\n    metadata={}\n)\n\ndoc.clean_extra_whitespace()\n\nprint(doc.page_content)  # \"ITEM 1. BUSINESS\"\n</code></pre>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#convert-ligatures","level":3,"title":"Convert Ligatures","text":"<p>Turn typographic ligatures into regular letters:</p> <pre><code>doc = Document(\n    page_content=\"The encyclopædia has œnology and ﬁsh sections\",\n    metadata={}\n)\n\ndoc.clean_ligatures()\n\nprint(doc.page_content)\n# \"The encyclopaedia has oenology and fish sections\"\n</code></pre> <p>Common conversions:</p> <ul> <li>æ → ae</li> <li>œ → oe</li> <li>ﬁ → fi</li> <li>ﬂ → fl</li> </ul>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#remove-bullets","level":3,"title":"Remove Bullets","text":"<p>Clean up bullet symbols from lists:</p> <pre><code>doc = Document(\n    page_content=\"●  First item\\n●  Second item\\n●  Third item\",\n    metadata={}\n)\n\ndoc.clean_bullets()\n\nprint(doc.page_content)\n# \"First item\\nSecond item\\nThird item\"\n</code></pre> <p>Removes: ●, ○, ■, □, •, and other bullet symbols.</p>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#remove-non-ascii-characters","level":3,"title":"Remove Non-ASCII Characters","text":"<p>Strip out non-ASCII characters:</p> <pre><code>doc = Document(\n    page_content=\"Hello\\x88World\\x89\",\n    metadata={}\n)\n\ndoc.clean_non_ascii_chars()\n\nprint(doc.page_content)  # \"HelloWorld\"\n</code></pre> <p>Use carefully: This also removes accented characters and emoji!</p>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#group-broken-paragraphs","level":3,"title":"Group Broken Paragraphs","text":"<p>Fix paragraphs incorrectly split across lines:</p> <pre><code>doc = Document(\n    page_content=\"This is a sentence\\nthat was split\\nacross lines.\\n\\nNew paragraph here.\",\n    metadata={}\n)\n\ndoc.group_broken_paragraphs()\n\n# Joins the broken paragraph while preserving the paragraph break\nprint(doc.page_content)\n# \"This is a sentence that was split across lines.\\n\\nNew paragraph here.\"\n</code></pre>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#real-world-example","level":2,"title":"Real-World Example","text":"<p>Cleaning a PDF document:</p> <pre><code>from rs_document import Document\n\n# Load PDF text (using your PDF library)\npdf_text = extract_text_from_pdf(\"document.pdf\")\n\n# Create document\ndoc = Document(\n    page_content=pdf_text,\n    metadata={\"source\": \"document.pdf\"}\n)\n\n# Clean all issues\ndoc.clean()\n\n# Now the text is ready for embedding\nprint(f\"Cleaned {len(doc.page_content)} characters\")\n</code></pre>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#cleaning-multiple-documents","level":2,"title":"Cleaning Multiple Documents","text":"<p>Process a batch of documents:</p> <pre><code>from rs_document import Document\n\ndocuments = [\n    Document(page_content=\"messy text 1\", metadata={\"id\": \"1\"}),\n    Document(page_content=\"messy text 2\", metadata={\"id\": \"2\"}),\n    # ... more documents\n]\n\n# Clean each document\nfor doc in documents:\n    doc.clean()\n\nprint(f\"Cleaned {len(documents)} documents\")\n</code></pre>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#understanding-cleaner-order","level":2,"title":"Understanding Cleaner Order","text":"<p>When you call <code>.clean()</code>, cleaners run in this order:</p> <ol> <li><code>clean_extra_whitespace</code> - Normalize spacing first</li> <li><code>clean_ligatures</code> - Convert to standard characters</li> <li><code>clean_bullets</code> - Remove list markers</li> <li><code>clean_non_ascii_chars</code> - Remove remaining non-ASCII</li> <li><code>group_broken_paragraphs</code> - Fix structure last</li> </ol> <p>This order ensures each cleaner works with normalized input.</p>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#important-notes","level":2,"title":"Important Notes","text":"","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#cleaning-modifies-in-place","level":3,"title":"Cleaning Modifies In-Place","text":"<p>Cleaning changes the document directly:</p> <pre><code>doc = Document(page_content=\"original text\", metadata={})\n\n# This modifies doc\ndoc.clean()\n\n# Original content is gone\n</code></pre> <p>If you need the original, create a copy first:</p> <pre><code>from rs_document import Document\n\noriginal = Document(page_content=\"text\", metadata={\"id\": \"1\"})\n\n# Make a copy\ncleaned = Document(\n    page_content=original.page_content,\n    metadata=original.metadata.copy()\n)\n\n# Clean the copy\ncleaned.clean()\n\n# Now you have both versions\n</code></pre>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#metadata-is-preserved","level":3,"title":"Metadata is Preserved","text":"<p>Cleaning never touches metadata:</p> <pre><code>doc = Document(\n    page_content=\"●  messy text\",\n    metadata={\"source\": \"file.txt\", \"page\": \"5\"}\n)\n\ndoc.clean()\n\n# Metadata unchanged\nassert doc.metadata == {\"source\": \"file.txt\", \"page\": \"5\"}\n</code></pre>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/cleaning/#next-steps","level":2,"title":"Next Steps","text":"<p>Now that your text is clean, let's learn how to split documents into chunks for embeddings!</p>","path":["Tutorial","Cleaning Text"],"tags":[]},{"location":"tutorial/first-document/","level":1,"title":"Your First Document","text":"<p>Learn how to create and work with Document objects in rs_document.</p>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#creating-a-document","level":2,"title":"Creating a Document","text":"<p>A <code>Document</code> has two parts:</p> <ol> <li>page_content: The text content</li> <li>metadata: String key-value pairs for tracking information</li> </ol> <p>Let's create one:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"This is my first document with some text content.\",\n    metadata={\"source\": \"tutorial.txt\", \"page\": \"1\"}\n)\n\nprint(doc)\n</code></pre> <p>Output:</p> <pre><code>Document(page_content=\"This is my first document...\", metadata={\"source\": \"tutorial.txt\", \"page\": \"1\"})\n</code></pre>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#understanding-document-components","level":2,"title":"Understanding Document Components","text":"","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#page-content","level":3,"title":"Page Content","text":"<p>The text content of your document:</p> <pre><code>doc = Document(\n    page_content=\"Hello, world!\",\n    metadata={}\n)\n\n# Access the content\nprint(doc.page_content)  # \"Hello, world!\"\n\n# Modify it\ndoc.page_content = \"Goodbye, world!\"\nprint(doc.page_content)  # \"Goodbye, world!\"\n</code></pre>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#metadata","level":3,"title":"Metadata","text":"<p>Metadata stores information about the document:</p> <pre><code>doc = Document(\n    page_content=\"Document text\",\n    metadata={\n        \"source\": \"article.txt\",\n        \"author\": \"Jane Doe\",\n        \"date\": \"2024-01-01\",\n        \"category\": \"tutorial\"\n    }\n)\n\n# Access metadata\nprint(doc.metadata[\"source\"])  # \"article.txt\"\n\n# Add more metadata\ndoc.metadata[\"page\"] = \"5\"\n\n# View all metadata\nprint(doc.metadata)\n</code></pre> <p>Important: Metadata values must be strings. Convert other types:</p> <pre><code># Wrong - will cause errors\nmetadata = {\"page\": 5, \"score\": 0.95}\n\n# Correct - convert to strings\nmetadata = {\"page\": \"5\", \"score\": \"0.95\"}\n\ndoc = Document(page_content=\"text\", metadata=metadata)\n</code></pre>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#creating-documents-from-files","level":2,"title":"Creating Documents from Files","text":"<p>Load content from a text file:</p> <pre><code>from rs_document import Document\n\n# Read file\nwith open(\"document.txt\", \"r\", encoding=\"utf-8\") as f:\n    content = f.read()\n\n# Create document\ndoc = Document(\n    page_content=content,\n    metadata={\n        \"source\": \"document.txt\",\n        \"path\": \"/path/to/document.txt\"\n    }\n)\n</code></pre>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#working-with-multiple-documents","level":2,"title":"Working with Multiple Documents","text":"<p>Create a list of documents:</p> <pre><code>from rs_document import Document\n\ndocuments = []\n\nfor i in range(5):\n    doc = Document(\n        page_content=f\"Content of document {i}\",\n        metadata={\"doc_id\": str(i)}\n    )\n    documents.append(doc)\n\nprint(f\"Created {len(documents)} documents\")\n</code></pre>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#viewing-documents","level":2,"title":"Viewing Documents","text":"<p>Documents have a helpful string representation:</p> <pre><code>doc = Document(\n    page_content=\"Short content\",\n    metadata={\"id\": \"123\"}\n)\n\n# Print the document\nprint(doc)\n\n# Convert to string\ndoc_string = str(doc)\n</code></pre>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#common-patterns","level":2,"title":"Common Patterns","text":"","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#document-from-user-input","level":3,"title":"Document from User Input","text":"<pre><code>user_text = input(\"Enter your text: \")\n\ndoc = Document(\n    page_content=user_text,\n    metadata={\"source\": \"user_input\"}\n)\n</code></pre>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#document-with-timestamps","level":3,"title":"Document with Timestamps","text":"<pre><code>from datetime import datetime\n\ndoc = Document(\n    page_content=\"Current content\",\n    metadata={\n        \"created_at\": datetime.now().isoformat(),\n        \"source\": \"app\"\n    }\n)\n</code></pre>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#empty-document","level":3,"title":"Empty Document","text":"<pre><code># Empty document (useful as placeholder)\ndoc = Document(page_content=\"\", metadata={})\n\n# Check if empty\nif not doc.page_content:\n    print(\"Document is empty\")\n</code></pre>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/first-document/#next-steps","level":2,"title":"Next Steps","text":"<p>Now that you know how to create documents, let's learn how to clean text to remove artifacts and normalize formatting!</p>","path":["Tutorial","Your First Document"],"tags":[]},{"location":"tutorial/installation/","level":1,"title":"Installation","text":"<p>Learn how to install rs_document in your Python environment.</p>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#using-pip","level":2,"title":"Using pip","text":"<p>The simplest way to install rs_document:</p> <pre><code>pip install rs-document\n</code></pre>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#using-uv","level":2,"title":"Using uv","text":"<p>If you're using uv for faster package management:</p> <pre><code>uv pip install rs-document\n</code></pre>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#verify-installation","level":2,"title":"Verify Installation","text":"<p>Check that rs_document is installed correctly:</p> <pre><code>import rs_document\n\n# Check version\nprint(rs_document.__version__)\n\n# Create a test document\ndoc = rs_document.Document(\n    page_content=\"Hello, rs_document!\",\n    metadata={\"test\": \"true\"}\n)\nprint(doc)\n</code></pre> <p>You should see output like:</p> <pre><code>Document(page_content=\"Hello, rs_document!\", metadata={\"test\": \"true\"})\n</code></pre>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#requirements","level":2,"title":"Requirements","text":"<ul> <li>Python: 3.10 or higher</li> <li>Platforms: Linux, macOS, Windows (x86_64 and ARM)</li> </ul>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#pre-built-wheels","level":2,"title":"Pre-built Wheels","text":"<p>rs_document provides pre-built wheels for most platforms:</p> <ul> <li>Linux: x86_64, aarch64, armv7, i686, s390x, ppc64le</li> <li>macOS: x86_64 (Intel), aarch64 (Apple Silicon)</li> <li>Windows: x64, x86</li> </ul> <p>If a wheel isn't available for your platform, pip will automatically compile from source (requires Rust toolchain).</p>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#installing-from-source","level":2,"title":"Installing from Source","text":"<p>If you need to build from source:</p>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#prerequisites","level":3,"title":"Prerequisites","text":"<ol> <li>Install Rust: https://rustup.rs</li> <li>Install Python development headers</li> </ol>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#build","level":3,"title":"Build","text":"<pre><code>pip install maturin\ngit clone https://github.com/cam-barts/rs_document.git\ncd rs_document\nmaturin develop --release\n</code></pre>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#troubleshooting","level":2,"title":"Troubleshooting","text":"","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#no-matching-distribution-found","level":3,"title":"\"No matching distribution found\"","text":"<p>Make sure you're using Python 3.10 or higher:</p> <pre><code>python --version\n</code></pre>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#build-errors","level":3,"title":"Build Errors","text":"<p>If building from source fails:</p> <ol> <li>Ensure Rust is installed: <code>rustc --version</code></li> <li>Update Rust: <code>rustup update</code></li> <li>Check Python headers are installed (python3-dev on Ubuntu)</li> </ol>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/installation/#next-steps","level":2,"title":"Next Steps","text":"<p>Now that rs_document is installed, let's create your first document!</p>","path":["Tutorial","Installation"],"tags":[]},{"location":"tutorial/splitting/","level":1,"title":"Splitting Documents","text":"<p>Learn how to split large documents into smaller chunks for embedding models.</p>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#why-split-documents","level":2,"title":"Why Split Documents?","text":"<p>Embedding models have token limits (e.g., 512 or 8192 tokens). Large documents must be split into chunks that:</p> <ul> <li>Fit within the model's context window</li> <li>Maintain semantic coherence</li> <li>Provide enough context for retrieval</li> </ul>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#recursive-character-splitting","level":2,"title":"Recursive Character Splitting","text":"<p>The recommended way to split documents:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"A\" * 5000,  # 5000 characters\n    metadata={\"source\": \"large.txt\"}\n)\n\n# Split into ~1000 character chunks\nchunks = doc.recursive_character_splitter(1000)\n\nprint(f\"Created {len(chunks)} chunks\")\nprint(f\"First chunk: {len(chunks[0].page_content)} chars\")\nprint(f\"Metadata preserved: {chunks[0].metadata}\")\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#how-it-works","level":3,"title":"How It Works","text":"<p>The splitter tries to split at natural boundaries:</p> <ol> <li>Paragraph breaks (<code>\\n\\n</code>) - first choice</li> <li>Line breaks (<code>\\n</code>) - if paragraphs too large</li> <li>Word boundaries (``) - if lines too large</li> <li>Character-by-character - last resort</li> </ol> <p>This respects document structure and keeps semantic units together.</p>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#chunk-overlap","level":3,"title":"Chunk Overlap","text":"<p>The splitter automatically creates ~33% overlap between chunks:</p> <pre><code>doc = Document(\n    page_content=\"Paragraph one.\\n\\nParagraph two.\\n\\nParagraph three.\",\n    metadata={}\n)\n\nchunks = doc.recursive_character_splitter(50)\n\n# Adjacent chunks share content for better context\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i}: {chunk.page_content}\")\n</code></pre> <p>This overlap ensures:</p> <ul> <li>Concepts spanning chunk boundaries appear complete in at least one chunk</li> <li>Better retrieval for queries near chunk edges</li> </ul>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#choosing-chunk-size","level":3,"title":"Choosing Chunk Size","text":"<p>Pick a chunk size based on your embedding model:</p> <pre><code># Small chunks (good for precise retrieval)\nchunks = doc.recursive_character_splitter(500)\n\n# Medium chunks (balanced)\nchunks = doc.recursive_character_splitter(1000)\n\n# Large chunks (more context)\nchunks = doc.recursive_character_splitter(2000)\n</code></pre> <p>Guidelines:</p> <ul> <li>Check your embedding model's token limit</li> <li>~4 characters ≈ 1 token (rough estimate)</li> <li>Leave room for the ~33% overlap</li> <li>Experiment to find what works for your use case</li> </ul>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#simple-character-splitting","level":2,"title":"Simple Character Splitting","text":"<p>For exact chunk sizes without overlap:</p> <pre><code>from rs_document import Document\n\ndoc = Document(\n    page_content=\"ABCDEFGHIJ\",\n    metadata={\"id\": \"123\"}\n)\n\n# Split exactly every 3 characters\nchunks = doc.split_on_num_characters(3)\n\nprint([c.page_content for c in chunks])\n# Output: [\"ABC\", \"DEF\", \"GHI\", \"J\"]\n</code></pre> <p>This splits at character boundaries, ignoring words or sentences.</p> <p>When to use:</p> <ul> <li>You need uniform chunk sizes</li> <li>Document structure doesn't matter</li> <li>Testing or debugging</li> </ul> <p>When not to use:</p> <ul> <li>For natural language (use recursive splitter instead)</li> <li>When context matters (no overlap)</li> </ul>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#complete-example","level":2,"title":"Complete Example","text":"<p>Clean and split a document:</p> <pre><code>from rs_document import Document\n\n# Load document\nwith open(\"article.txt\", \"r\") as f:\n    content = f.read()\n\n# Create document\ndoc = Document(\n    page_content=content,\n    metadata={\n        \"source\": \"article.txt\",\n        \"category\": \"tech\"\n    }\n)\n\n# Clean the text\ndoc.clean()\n\n# Split into chunks\nchunks = doc.recursive_character_splitter(1000)\n\n# Inspect results\nprint(f\"Original: {len(doc.page_content)} chars\")\nprint(f\"Chunks: {len(chunks)}\")\nprint(f\"\\nFirst chunk:\")\nprint(f\"  Length: {len(chunks[0].page_content)}\")\nprint(f\"  Metadata: {chunks[0].metadata}\")\nprint(f\"  Preview: {chunks[0].page_content[:100]}...\")\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#working-with-chunks","level":2,"title":"Working with Chunks","text":"","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#all-chunks-have-same-metadata","level":3,"title":"All Chunks Have Same Metadata","text":"<p>Every chunk gets a copy of the original metadata:</p> <pre><code>doc = Document(\n    page_content=\"text \" * 1000,\n    metadata={\"source\": \"doc.txt\", \"author\": \"Jane\"}\n)\n\nchunks = doc.recursive_character_splitter(500)\n\n# All chunks have the same metadata\nfor chunk in chunks:\n    assert chunk.metadata == {\"source\": \"doc.txt\", \"author\": \"Jane\"}\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#add-chunk-specific-metadata","level":3,"title":"Add Chunk-Specific Metadata","text":"<p>Track chunk position:</p> <pre><code>chunks = doc.recursive_character_splitter(1000)\n\n# Add chunk index to each\nfor i, chunk in enumerate(chunks):\n    chunk.metadata[\"chunk_index\"] = str(i)\n    chunk.metadata[\"total_chunks\"] = str(len(chunks))\n\nprint(chunks[0].metadata)\n# {\"source\": \"doc.txt\", \"chunk_index\": \"0\", \"total_chunks\": \"5\"}\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#filter-chunks","level":3,"title":"Filter Chunks","text":"<p>Remove chunks that are too small:</p> <pre><code>chunks = doc.recursive_character_splitter(1000)\n\n# Keep only chunks &gt;= 200 chars\nmin_size = 200\nfiltered = [c for c in chunks if len(c.page_content) &gt;= min_size]\n\nprint(f\"Kept {len(filtered)} of {len(chunks)} chunks\")\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#common-patterns","level":2,"title":"Common Patterns","text":"","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#split-multiple-documents","level":3,"title":"Split Multiple Documents","text":"<pre><code>documents = [doc1, doc2, doc3]  # Your documents\n\nall_chunks = []\nfor doc in documents:\n    doc.clean()\n    chunks = doc.recursive_character_splitter(1000)\n    all_chunks.extend(chunks)\n\nprint(f\"Total chunks: {len(all_chunks)}\")\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#track-source-document","level":3,"title":"Track Source Document","text":"<pre><code>documents = [...]  # Your documents\n\n# Add unique ID to each source doc\nfor i, doc in enumerate(documents):\n    doc.metadata[\"doc_id\"] = str(i)\n\n# Split and chunks inherit doc_id\nall_chunks = []\nfor doc in documents:\n    chunks = doc.recursive_character_splitter(1000)\n    all_chunks.extend(chunks)\n\n# Group chunks by source\nfrom collections import defaultdict\nchunks_by_doc = defaultdict(list)\n\nfor chunk in all_chunks:\n    doc_id = chunk.metadata[\"doc_id\"]\n    chunks_by_doc[doc_id].append(chunk)\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#important-notes","level":2,"title":"Important Notes","text":"","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#splitting-returns-new-documents","level":3,"title":"Splitting Returns New Documents","text":"<p>Unlike cleaning, splitting creates new documents:</p> <pre><code>doc = Document(page_content=\"text\", metadata={})\n\n# Creates NEW documents, doesn't modify doc\nchunks = doc.recursive_character_splitter(100)\n\n# Original document unchanged\nassert doc.page_content == \"text\"\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#empty-documents","level":3,"title":"Empty Documents","text":"<p>Empty documents return empty list:</p> <pre><code>doc = Document(page_content=\"\", metadata={})\n\nchunks = doc.recursive_character_splitter(1000)\n\nassert len(chunks) == 0\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#maximum-chunk-size","level":3,"title":"Maximum Chunk Size","text":"<p>Chunks never exceed the requested size:</p> <pre><code>chunks = doc.recursive_character_splitter(1000)\n\n# All chunks &lt;= 1000 characters\nfor chunk in chunks:\n    assert len(chunk.page_content) &lt;= 1000\n</code></pre>","path":["Tutorial","Splitting Documents"],"tags":[]},{"location":"tutorial/splitting/#next-steps","level":2,"title":"Next Steps","text":"<p>Now you know how to split documents! Let's learn how to process multiple documents efficiently with parallel processing.</p>","path":["Tutorial","Splitting Documents"],"tags":[]}]}